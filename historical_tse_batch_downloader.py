#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Historical Batch Downloader - å°è‚¡æ­·å²è³‡æ–™æ‰¹é‡ä¸‹è¼‰å™¨
åŸºæ–¼ç¾æœ‰ daily_data_updater.py ä¿®æ”¹ï¼Œå°ˆé–€ç”¨æ–¼æ­·å²è³‡æ–™è£œå¼·
æ—¥æœŸç¯„åœï¼š2025/01/01 åˆ°ä»Šå¤©
"""

import os
import re
import requests
import urllib3
import pandas as pd
import time
import random
from datetime import datetime, timedelta

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# ===== è¨­å®šå€åŸŸ =====
RAW_DIR = r"C:\05model\raw"
CLEANED_DIR = r"C:\05model\cleaned"

# æ—¥æœŸç¯„åœè¨­å®š
START_DATE = datetime(2025, 1, 1)
END_DATE = datetime.today()

# ä¸‹è¼‰è¨­å®š
MIN_DELAY = 3.0      # æœ€å°é–“éš”ç§’æ•¸
MAX_DELAY = 6.0      # æœ€å¤§é–“éš”ç§’æ•¸
MAX_RETRIES = 3      # æœ€å¤§é‡è©¦æ¬¡æ•¸
RETRY_DELAY = 10     # é‡è©¦é–“éš”ç§’æ•¸

# HTTP è¨­å®š
HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/115.0.0.0 Safari/537.36"
    )
}

REFERER = {
    "t86": "https://www.twse.com.tw/exchangeReport/TWT86U",
    "twt44u": "https://www.twse.com.tw/fund/TWT44U",
    "twt38u": "https://www.twse.com.tw/fund/TWT38U",
    "mi_margn": "https://www.twse.com.tw/exchangeReport/MI_MARGN",
    "mi_index": "https://www.twse.com.tw/rwd/zh/afterTrading/MI_INDEX"
}

URLS = {
    "t86": lambda d, tw: f"https://www.twse.com.tw/rwd/zh/fund/T86?response=csv&date={d}&selectType=ALLBUT0999",
    "twt44u": lambda d, tw: f"https://www.twse.com.tw/fund/TWT44U?response=csv&date={d}&selectType=ALL",
    "twt38u": lambda d, tw: f"https://www.twse.com.tw/fund/TWT38U?response=csv&date={d}&selectType=ALL",
    "mi_margn": lambda d, tw: f"https://www.twse.com.tw/exchangeReport/MI_MARGN?response=csv&date={d}&selectType=ALL",
    "mi_index": lambda d, tw: f"https://www.twse.com.tw/rwd/zh/afterTrading/MI_INDEX?response=csv&date={d}&type=ALL"
}

# ===== å·¥å…·å‡½æ•¸ =====
def ensure_dir(path):
    """ç¢ºä¿ç›®éŒ„å­˜åœ¨"""
    if not os.path.exists(path):
        os.makedirs(path)

def is_html_bytes(b: bytes) -> bool:
    """æª¢æŸ¥å›æ‡‰æ˜¯å¦ç‚ºHTMLï¼ˆè¡¨ç¤ºç„¡è³‡æ–™ï¼‰"""
    text = b.decode("utf-8", errors="ignore").lower()
    return any(tag in text[:500] for tag in ("<html", "<!doctype", "<head", "<script"))

def smart_delay():
    """æ™ºèƒ½å»¶é²ï¼Œé¿å…è¢«åµæ¸¬"""
    delay = random.uniform(MIN_DELAY, MAX_DELAY)
    print(f"[â³] ç­‰å¾… {delay:.1f} ç§’...")
    time.sleep(delay)

def get_existing_dates():
    """å–å¾—å·²å­˜åœ¨çš„æ—¥æœŸï¼Œé¿å…é‡è¤‡ä¸‹è¼‰"""
    if not os.path.exists(RAW_DIR):
        return set()
    
    existing_dates = set()
    for filename in os.listdir(RAW_DIR):
        if filename.endswith('.csv'):
            match = re.search(r'(\d{8})_', filename)
            if match:
                existing_dates.add(match.group(1))
    
    return existing_dates

def generate_trading_dates():
    """ç”Ÿæˆäº¤æ˜“æ—¥æœŸåˆ—è¡¨ï¼ˆè·³éé€±æœ«ï¼‰"""
    dates = []
    current_date = START_DATE
    
    while current_date <= END_DATE:
        # è·³éé€±æœ«
        if current_date.weekday() < 5:  # 0-4 æ˜¯é€±ä¸€åˆ°é€±äº”
            dates.append(current_date)
        current_date += timedelta(days=1)
    
    return dates

# ===== ä¸‹è¼‰åŠŸèƒ½ =====
def download_one_date(session, name, url_func, date_obj):
    """ä¸‹è¼‰å–®ä¸€æ—¥æœŸçš„å–®ä¸€è³‡æ–™æº"""
    d = date_obj.strftime("%Y%m%d")
    tw = f"{date_obj.year-1911}/{date_obj.month:02}/{date_obj.day:02}"
    
    # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å·²å­˜åœ¨
    fn = os.path.join(RAW_DIR, f"{d}_{name}.csv")
    if os.path.exists(fn):
        print(f"[â­] {name} {d} å·²å­˜åœ¨ï¼Œè·³é")
        return True
    
    # å˜—è©¦ä¸‹è¼‰
    for retry in range(MAX_RETRIES):
        try:
            print(f"[ğŸ”„] ä¸‹è¼‰ {name} {d} (å˜—è©¦ {retry+1}/{MAX_RETRIES})")
            
            r = session.get(
                url_func(d, tw),
                headers={**HEADERS, "Referer": REFERER[name]},
                verify=False,
                timeout=15
            )
            
            if r.status_code == 200 and len(r.content) > 500:
                # t86 ç‰¹æ®Šè™•ç†ï¼Œå…¶ä»–æª¢æŸ¥æ˜¯å¦ç‚ºHTML
                if name == "t86" or not is_html_bytes(r.content):
                    ensure_dir(RAW_DIR)
                    with open(fn, "wb") as f:
                        f.write(r.content)
                    print(f"[âœ…] {name} {d} â†’ {fn}")
                    return True
                else:
                    print(f"[âš ] {name} {d} å›å‚³ HTML (å¯èƒ½ç„¡è³‡æ–™)")
                    return False
            else:
                print(f"[âš ] {name} {d} ç‹€æ…‹ç¢¼: {r.status_code}, å¤§å°: {len(r.content)}")
                
        except Exception as e:
            print(f"[âŒ] {name} {d} éŒ¯èª¤: {e}")
            if retry < MAX_RETRIES - 1:
                wait_time = RETRY_DELAY * (2 ** retry)  # æŒ‡æ•¸é€€é¿
                print(f"[â³] ç­‰å¾… {wait_time} ç§’å¾Œé‡è©¦...")
                time.sleep(wait_time)
    
    print(f"[âŒ] {name} {d} ä¸‹è¼‰å¤±æ•—")
    return False

def download_all_historical():
    """ä¸‹è¼‰æ‰€æœ‰æ­·å²è³‡æ–™"""
    print("=== æ­·å²è³‡æ–™æ‰¹é‡ä¸‹è¼‰é–‹å§‹ ===")
    
    # ç”Ÿæˆæ—¥æœŸåˆ—è¡¨
    dates = generate_trading_dates()
    existing_dates = get_existing_dates()
    
    total_dates = len(dates)
    total_requests = total_dates * len(URLS)
    
    print(f"[â„¹] æ—¥æœŸç¯„åœ: {START_DATE.strftime('%Y-%m-%d')} ~ {END_DATE.strftime('%Y-%m-%d')}")
    print(f"[â„¹] ç¸½äº¤æ˜“æ—¥: {total_dates} å¤©")
    print(f"[â„¹] å·²å­˜åœ¨è³‡æ–™: {len(existing_dates)} å€‹æ—¥æœŸ")
    print(f"[â„¹] ç¸½è«‹æ±‚æ•¸: {total_requests}")
    print(f"[â„¹] é ä¼°æ™‚é–“: {total_requests * 4.5 / 60:.1f} åˆ†é˜")
    
    # çµ±è¨ˆè®Šæ•¸
    success_count = 0
    fail_count = 0
    skip_count = 0
    
    sess = requests.Session()
    
    # é–‹å§‹ä¸‹è¼‰
    for i, date_obj in enumerate(dates, 1):
        d = date_obj.strftime("%Y%m%d")
        print(f"\nâ”€â”€ è™•ç†æ—¥æœŸ {date_obj.strftime('%Y-%m-%d')} ({i}/{total_dates}) â”€â”€")
        
        # æª¢æŸ¥é€™å€‹æ—¥æœŸæ˜¯å¦å·²æœ‰å®Œæ•´è³‡æ–™
        date_files_exist = all(
            os.path.exists(os.path.join(RAW_DIR, f"{d}_{name}.csv"))
            for name in URLS.keys()
        )
        
        if date_files_exist:
            print(f"[â­] {d} æ‰€æœ‰æª”æ¡ˆå·²å­˜åœ¨ï¼Œè·³éæ•´æ—¥")
            skip_count += len(URLS)
            continue
        
        # ä¸‹è¼‰å„å€‹è³‡æ–™æº
        for j, (name, url_func) in enumerate(URLS.items()):
            if download_one_date(sess, name, url_func, date_obj):
                success_count += 1
            else:
                fail_count += 1
            
            # é™¤äº†æœ€å¾Œä¸€å€‹è«‹æ±‚ï¼Œéƒ½è¦å»¶é²
            if not (i == total_dates and j == len(URLS) - 1):
                smart_delay()
    
    print(f"\n[ğŸ“Š] ä¸‹è¼‰çµ±è¨ˆ:")
    print(f"    - æˆåŠŸ: {success_count}")
    print(f"    - å¤±æ•—: {fail_count}")
    print(f"    - è·³é: {skip_count}")
    print(f"    - ç¸½è¨ˆ: {success_count + fail_count + skip_count}")

# ===== æ¸…æ´—åŠŸèƒ½ï¼ˆä¿æŒåŸæœ‰é‚è¼¯ï¼‰ =====
def clean_numeric(val):
    """æ¸…æ´—æ•¸å€¼è³‡æ–™"""
    s = str(val).replace(",", "").strip()
    if s in ("", "-", "NA") or all(ch == "#" for ch in s):
        return 0.0
    try:
        return float(s)
    except:
        return 0.0

def read_csv_auto(path, **kwargs):
    """è‡ªå‹•åµæ¸¬ç·¨ç¢¼è®€å–CSV"""
    for enc in ("cp950", "utf-8"):
        try:
            return pd.read_csv(path, encoding=enc, **kwargs)
        except:
            pass
    return pd.read_csv(path, encoding="cp950", errors="ignore", **kwargs)

def get_raw_files_by_date(date_str):
    """å–å¾—æŒ‡å®šæ—¥æœŸçš„æ‰€æœ‰åŸå§‹æª”æ¡ˆ"""
    if not os.path.exists(RAW_DIR):
        return {}
    
    files = {}
    for name in URLS.keys():
        filepath = os.path.join(RAW_DIR, f"{date_str}_{name}.csv")
        if os.path.exists(filepath):
            files[name] = filepath
    
    return files

def process_date_t86(date_str, filepath):
    """è™•ç†æŒ‡å®šæ—¥æœŸçš„T86è³‡æ–™"""
    try:
        df = read_csv_auto(filepath, skiprows=1, dtype=str)
        df.columns = df.columns.str.strip()
        df = df.rename(columns={
            "è­‰åˆ¸ä»£è™Ÿ": "stock_id",
            "å¤–é™¸è³‡è²·è³£è¶…è‚¡æ•¸(ä¸å«å¤–è³‡è‡ªç‡Ÿå•†)": "foreign_buy",
            "ä¸‰å¤§æ³•äººè²·è³£è¶…è‚¡æ•¸": "insti_net"
        })[["stock_id", "foreign_buy", "insti_net"]]
        df = df[df["stock_id"].str.match(r"^\d{4}$", na=False)]
        df["foreign_buy"] = df["foreign_buy"].apply(clean_numeric)
        df["insti_net"] = df["insti_net"].apply(clean_numeric)
        
        ensure_dir(CLEANED_DIR)
        out = os.path.join(CLEANED_DIR, f"{date_str}_cleaned_t86.csv")
        df.to_csv(out, index=False, encoding="utf-8-sig")
        print(f"[âœ…] t86 {date_str} cleaned â†’ {out}")
        return True
    except Exception as e:
        print(f"[âŒ] t86 {date_str} æ¸…æ´—å¤±æ•—: {e}")
        return False

def process_date_twt44u(date_str, filepath):
    """è™•ç†æŒ‡å®šæ—¥æœŸçš„TWT44Uè³‡æ–™"""
    try:
        df = read_csv_auto(filepath, skiprows=1, dtype=str)
        df.columns = df.columns.str.strip()
        df.iloc[:, 1] = df.iloc[:, 1].str.replace("=", "").str.strip()
        df = df.iloc[:, [1, 3, 4, 5]].copy()
        df.columns = ["stock_id", "trust_buy", "trust_sell", "trust_net"]
        df = df[df["stock_id"].str.match(r"^\d{4}$", na=False)]
        df["trust_buy"] = df["trust_buy"].apply(clean_numeric)
        df["trust_sell"] = df["trust_sell"].apply(clean_numeric)
        df["trust_net"] = df["trust_net"].apply(clean_numeric)
        
        ensure_dir(CLEANED_DIR)
        out = os.path.join(CLEANED_DIR, f"{date_str}_cleaned_twt44u.csv")
        df.to_csv(out, index=False, encoding="utf-8-sig")
        print(f"[âœ…] twt44u {date_str} cleaned â†’ {out}")
        return True
    except Exception as e:
        print(f"[âŒ] twt44u {date_str} æ¸…æ´—å¤±æ•—: {e}")
        return False

def process_date_twt38u(date_str, filepath):
    """è™•ç†æŒ‡å®šæ—¥æœŸçš„TWT38Uè³‡æ–™"""
    try:
        df = read_csv_auto(filepath, skiprows=2, dtype=str)
        df.columns = df.columns.str.strip()
        df.iloc[:, 1] = df.iloc[:, 1].str.replace("=", "").str.strip()
        result_df = pd.DataFrame()
        result_df["stock_id"] = df.iloc[:, 1]
        result_df["FI_Buy"] = df.iloc[:, 3].apply(clean_numeric)
        result_df["FI_Sell"] = df.iloc[:, 4].apply(clean_numeric)
        result_df["FI_Net"] = df.iloc[:, 5].apply(clean_numeric)
        result_df["PD_Buy"] = 0
        result_df["PD_Sell"] = 0
        result_df["PD_Net"] = 0
        result_df["FA_Buy"] = df.iloc[:, 9].apply(clean_numeric)
        result_df["FA_Sell"] = df.iloc[:, 10].apply(clean_numeric)
        result_df["FA_Net"] = df.iloc[:, 11].apply(clean_numeric)
        result_df = result_df[result_df["stock_id"].str.match(r"^\d{4}$", na=False)]
        
        ensure_dir(CLEANED_DIR)
        out = os.path.join(CLEANED_DIR, f"{date_str}_cleaned_twt38u.csv")
        result_df.to_csv(out, index=False, encoding="utf-8-sig")
        print(f"[âœ…] twt38u {date_str} cleaned â†’ {out}")
        return True
    except Exception as e:
        print(f"[âŒ] twt38u {date_str} æ¸…æ´—å¤±æ•—: {e}")
        return False

def process_date_margen(date_str, filepath):
    """è™•ç†æŒ‡å®šæ—¥æœŸçš„MI_MARGNè³‡æ–™"""
    try:
        df = read_csv_auto(filepath, skiprows=7, dtype=str)
        df.columns = df.columns.str.strip()
        df["stock_id"] = df.iloc[:, 0].str.strip()
        df = df[df["stock_id"].str.match(r"^\d{4}$", na=False)]
        df["margin_diff"] = df.iloc[:, 6].apply(clean_numeric) - df.iloc[:, 5].apply(clean_numeric)
        df["short_diff"] = df.iloc[:, 12].apply(clean_numeric) - df.iloc[:, 11].apply(clean_numeric)
        
        ensure_dir(CLEANED_DIR)
        out = os.path.join(CLEANED_DIR, f"{date_str}_cleaned_margen.csv")
        df[["stock_id", "margin_diff", "short_diff"]].to_csv(out, index=False, encoding="utf-8-sig")
        print(f"[âœ…] mi_margn {date_str} cleaned â†’ {out}")
        return True
    except Exception as e:
        print(f"[âŒ] mi_margn {date_str} æ¸…æ´—å¤±æ•—: {e}")
        return False

def process_date_mi_index(date_str, filepath):
    """è™•ç†æŒ‡å®šæ—¥æœŸçš„MI_INDEXè³‡æ–™"""
    try:
        # æ‰¾åˆ°æ¨™é¡Œåˆ—
        header_row = None
        with open(filepath, "r", encoding="cp950", errors="ignore") as f:
            for idx, line in enumerate(f):
                if "è­‰åˆ¸ä»£è™Ÿ" in line and "æ”¶ç›¤åƒ¹" in line:
                    header_row = idx
                    break
        
        if header_row is None:
            print(f"[âš ] {filepath} æ‰¾ä¸åˆ°æ¨™é¡Œåˆ—")
            return False
        
        df = read_csv_auto(filepath, skiprows=header_row, dtype=str)
        df.columns = df.columns.str.strip()
        
        # ç§»é™¤ Unnamed æ¬„ä½
        df = df.drop(columns=[c for c in df.columns if c.startswith("Unnamed")], errors="ignore")
        
        # åªä¿ç•™ 4 ä½æ•¸è‚¡ç¥¨ä»£è™Ÿ
        df = df[df["è­‰åˆ¸ä»£è™Ÿ"].str.match(r"^\d{4}$", na=False)]
        
        # é‡æ–°å‘½åæ¬„ä½
        df = df.rename(columns={
            "è­‰åˆ¸ä»£è™Ÿ": "stock_id",
            "è­‰åˆ¸åç¨±": "name",
            "æˆäº¤è‚¡æ•¸": "volume",
            "æˆäº¤é‡‘é¡": "value",
            "æˆäº¤ç­†æ•¸": "transactions",
            "é–‹ç›¤åƒ¹": "open",
            "æœ€é«˜åƒ¹": "high",
            "æœ€ä½åƒ¹": "low",
            "æ”¶ç›¤åƒ¹": "close",
            "æ¼²è·Œåƒ¹å·®": "change",
            "æœ€å¾Œæ­ç¤ºè²·åƒ¹": "last_bid_price",
            "æœ€å¾Œæ­ç¤ºè²·é‡": "last_bid_volume",
            "æœ€å¾Œæ­ç¤ºè³£åƒ¹": "last_ask_price",
            "æœ€å¾Œæ­ç¤ºè³£é‡": "last_ask_volume",
            "æœ¬ç›Šæ¯”": "per"
        })
        
        # é‡å°æ•¸å€¼æ¬„ä½åš clean_numeric
        for col in df.columns:
            if col not in ["stock_id", "name"]:
                df[col] = df[col].apply(clean_numeric)
        
        ensure_dir(CLEANED_DIR)
        out = os.path.join(CLEANED_DIR, f"{date_str}_cleaned_mi_index.csv")
        df.to_csv(out, index=False, encoding="utf-8-sig")
        print(f"[âœ…] mi_index {date_str} cleaned â†’ {out}")
        return True
    except Exception as e:
        print(f"[âŒ] mi_index {date_str} æ¸…æ´—å¤±æ•—: {e}")
        return False

def clean_all_downloaded():
    """æ¸…æ´—æ‰€æœ‰å·²ä¸‹è¼‰çš„åŸå§‹è³‡æ–™"""
    print("\n=== é–‹å§‹æ¸…æ´—æ‰€æœ‰è³‡æ–™ ===")
    
    if not os.path.exists(RAW_DIR):
        print("[âš ] Raw è³‡æ–™å¤¾ä¸å­˜åœ¨")
        return
    
    # å–å¾—æ‰€æœ‰æ—¥æœŸ
    all_dates = set()
    for filename in os.listdir(RAW_DIR):
        if filename.endswith('.csv'):
            match = re.search(r'(\d{8})_', filename)
            if match:
                all_dates.add(match.group(1))
    
    all_dates = sorted(all_dates)
    print(f"[â„¹] æ‰¾åˆ° {len(all_dates)} å€‹æ—¥æœŸçš„è³‡æ–™")
    
    # è™•ç†å™¨æ˜ å°„
    processors = {
        't86': process_date_t86,
        'twt44u': process_date_twt44u,
        'twt38u': process_date_twt38u,
        'mi_margn': process_date_margen,
        'mi_index': process_date_mi_index
    }
    
    total_success = 0
    total_fail = 0
    
    # é€æ—¥è™•ç†
    for i, date_str in enumerate(all_dates, 1):
        print(f"\nâ”€â”€ æ¸…æ´—æ—¥æœŸ {date_str} ({i}/{len(all_dates)}) â”€â”€")
        
        # å–å¾—é€™å€‹æ—¥æœŸçš„æ‰€æœ‰æª”æ¡ˆ
        date_files = get_raw_files_by_date(date_str)
        
        if not date_files:
            print(f"[âš ] {date_str} æ²’æœ‰æ‰¾åˆ°ä»»ä½•åŸå§‹æª”æ¡ˆ")
            continue
        
        # è™•ç†å„å€‹è³‡æ–™æº
        for name, filepath in date_files.items():
            if name in processors:
                if processors[name](date_str, filepath):
                    total_success += 1
                else:
                    total_fail += 1
            else:
                print(f"[âš ] ä¸èªè­˜çš„è³‡æ–™æº: {name}")
    
    print(f"\n[ğŸ“Š] æ¸…æ´—çµ±è¨ˆ:")
    print(f"    - æˆåŠŸ: {total_success}")
    print(f"    - å¤±æ•—: {total_fail}")

# ===== ä¸»ç¨‹å¼ =====
def main():
    """ä¸»åŸ·è¡Œå‡½æ•¸"""
    print("=== å°è‚¡æ­·å²è³‡æ–™æ‰¹é‡ä¸‹è¼‰å™¨ ===")
    print(f"ç›®æ¨™æ—¥æœŸç¯„åœ: {START_DATE.strftime('%Y-%m-%d')} ~ {END_DATE.strftime('%Y-%m-%d')}")
    print(f"è³‡æ–™é¡å‹: ä¸Šå¸‚è‚¡ç¥¨")
    print(f"è¼¸å‡ºç›®éŒ„: {RAW_DIR} (åŸå§‹), {CLEANED_DIR} (æ¸…æ´—)")
    
    # ç¢ºèªåŸ·è¡Œ
    response = input("\næ˜¯å¦é–‹å§‹åŸ·è¡Œ? (y/N): ").strip().lower()
    if response != 'y':
        print("å–æ¶ˆåŸ·è¡Œ")
        return
    
    start_time = datetime.now()
    
    try:
        # æ­¥é©Ÿ 1: ä¸‹è¼‰æ‰€æœ‰æ­·å²è³‡æ–™
        download_all_historical()
        
        # æ­¥é©Ÿ 2: æ¸…æ´—æ‰€æœ‰å·²ä¸‹è¼‰çš„è³‡æ–™
        clean_all_downloaded()
        
        # å®Œæˆ
        end_time = datetime.now()
        duration = end_time - start_time
        
        print(f"\n[ğŸ‰] æ‰€æœ‰ç¨‹åºå®Œæˆ!")
        print(f"[â±] ç¸½åŸ·è¡Œæ™‚é–“: {duration}")
        print(f"[ğŸ“] åŸå§‹è³‡æ–™: {RAW_DIR}")
        print(f"[ğŸ“] æ¸…æ´—è³‡æ–™: {CLEANED_DIR}")
        
    except KeyboardInterrupt:
        print("\n[â¹] ä½¿ç”¨è€…ä¸­æ–·åŸ·è¡Œ")
    except Exception as e:
        print(f"\n[âŒ] åŸ·è¡Œéç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
        raise

if __name__ == "__main__":
    main()
