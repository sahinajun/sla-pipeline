#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
OTC Historical Batch Downloader - ä¸Šæ«ƒæ­·å²è³‡æ–™æ‰¹é‡ä¸‹è¼‰å™¨
åŸºæ–¼ç¾æœ‰ otc_downloader_optimized.py ä¿®æ”¹ï¼Œå°ˆé–€ç”¨æ–¼æ­·å²è³‡æ–™è£œå¼·
æ—¥æœŸç¯„åœï¼š2025/01/01 åˆ°ä»Šå¤©
"""

import os
import json
import urllib3
import pandas as pd
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.ui import Select
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from datetime import datetime, timedelta
import time
import shutil
import holidays
import re
import logging
import psutil
from pathlib import Path
from typing import Dict, Any, Optional, List
from contextlib import contextmanager
import traceback
import random

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# ===== è¨­å®šå€åŸŸ =====
BASE_DIR = Path(__file__).parent
RAW_DIR = BASE_DIR / "otc_raw"
DOWNLOAD_DIR = Path.home() / "Downloads"
CLEAN_DIR = BASE_DIR / "otc_cleaned"
LOG_DIR = BASE_DIR / "logs"

# æ—¥æœŸç¯„åœè¨­å®š
START_DATE = datetime(2025, 1, 1)
END_DATE = datetime.today()

# ä¸‹è¼‰è¨­å®š
MIN_DELAY = 10.0     # æœ€å°é–“éš”ç§’æ•¸ï¼ˆæ¯”ä¸Šå¸‚æ›´ä¿å®ˆï¼‰
MAX_DELAY = 20.0     # æœ€å¤§é–“éš”ç§’æ•¸
MAX_RETRIES = 2      # æœ€å¤§é‡è©¦æ¬¡æ•¸
RETRY_DELAY = 30     # é‡è©¦é–“éš”ç§’æ•¸

# é è¨­è¨­å®š - æ­·å²æ‰¹é‡ä¸‹è¼‰å„ªåŒ–
DEFAULT_CONFIG = {
    "download_items": {
        "daily_close_no1430": {
            "name": "ä¸Šæ«ƒè‚¡ç¥¨æ¯æ—¥æ”¶ç›¤è¡Œæƒ…(ä¸å«å®šåƒ¹)",
            "url": "https://www.tpex.org.tw/zh-tw/mainboard/trading/info/mi-pricing.html",
            "wait_element": "table.table-default",
            "download_text": "å¦å­˜ CSV",
            "needs_query": True,
            "retry_count": 2,
            "skiprows": 3,
            "priority": 1  # æœ€é‡è¦çš„è³‡æ–™
        },
        "margin_transactions": {
            "name": "ä¸Šæ«ƒè‚¡ç¥¨èè³‡èåˆ¸é¤˜é¡",
            "url": "https://www.tpex.org.tw/zh-tw/mainboard/trading/margin-trading/transactions.html",
            "wait_element": "table.table-default",
            "download_text": "ä¸‹è¼‰ CSV æª”(UTF-8)",
            "needs_query": False,
            "retry_count": 2,
            "skiprows": 3,
            "priority": 2
        },
        "institutional_detail": {
            "name": "ä¸‰å¤§æ³•äººè²·è³£æ˜ç´°è³‡è¨Š",
            "url": "https://www.tpex.org.tw/zh-tw/mainboard/trading/major-institutional/detail/day.html",
            "select_element": {"name": "sect", "value": "AL"},
            "wait_element": "table.table-default",
            "download_text": "å¦å­˜ CSV",
            "needs_query": False,
            "retry_count": 2,
            "skiprows": 1,
            "priority": 2
        },
        "day_trading": {
            "name": "ç¾è‚¡ç•¶æ²–äº¤æ˜“çµ±è¨ˆè³‡è¨Š",
            "url": "https://www.tpex.org.tw/zh-tw/mainboard/trading/day-trading/statistics/day.html",
            "wait_element": "table.table-default",
            "download_text": "å¦å­˜ CSV",
            "needs_query": False,
            "retry_count": 2,
            "skiprows": 6,
            "priority": 3
        },
        "sec_trading": {
            "name": "å„åˆ¸å•†ç•¶æ—¥ç‡Ÿæ¥­é‡‘é¡çµ±è¨ˆè¡¨",
            "url": "https://www.tpex.org.tw/zh-tw/mainboard/trading/info/sec-trading.html",
            "wait_element": "table.table-default",
            "download_text": "ä¸‹è¼‰ CSV",
            "needs_query": False,
            "retry_count": 2,
            "skiprows": 2,
            "priority": 4
        },
        "investment_trust_buy": {
            "name": "æŠ•ä¿¡è²·è³£è¶…å½™ç¸½è¡¨ï¼ˆè²·è¶…ï¼‰",
            "url": "https://www.tpex.org.tw/zh-tw/mainboard/trading/major-institutional/domestic-inst/day.html",
            "select_element": {"name": "searchType", "value": "buy"},
            "wait_element": "table.table-default",
            "download_text": "å¦å­˜ CSV",
            "needs_query": False,
            "retry_count": 2,
            "skiprows": 1,
            "priority": 3
        },
        "investment_trust_sell": {
            "name": "æŠ•ä¿¡è²·è³£è¶…å½™ç¸½è¡¨ï¼ˆè³£è¶…ï¼‰",
            "url": "https://www.tpex.org.tw/zh-tw/mainboard/trading/major-institutional/domestic-inst/day.html",
            "select_element": {"name": "searchType", "value": "sell"},
            "wait_element": "table.table-default",
            "download_text": "å¦å­˜ CSV",
            "needs_query": False,
            "retry_count": 2,
            "skiprows": 1,
            "priority": 3
        },
        "highlight": {
            "name": "ä¸Šæ«ƒè‚¡ç¥¨ä¿¡ç”¨äº¤æ˜“èè³‡èåˆ¸é¤˜é¡æ¦‚æ³è¡¨",
            "url": "https://www.tpex.org.tw/zh-tw/mainboard/trading/margin-trading/highlight.html",
            "wait_element": "table.table-default",
            "download_text": "å¦å­˜ CSV",
            "needs_query": False,
            "retry_count": 2,
            "skiprows": 2,
            "priority": 4
        },
        "sbl": {
            "name": "ä¿¡ç”¨é¡åº¦ç¸½é‡ç®¡åˆ¶é¤˜é¡è¡¨",
            "url": "https://www.tpex.org.tw/zh-tw/mainboard/trading/margin-trading/sbl.html",
            "wait_element": "table.table-default",
            "download_text": "å¦å­˜ CSV",
            "needs_query": False,
            "retry_count": 2,
            "skiprows": 2,
            "priority": 5
        },
        "exempted": {
            "name": "å¹³ç›¤ä¸‹å¾—è(å€Ÿ)åˆ¸è³£å‡ºä¹‹è­‰åˆ¸åå–®",
            "url": "https://www.tpex.org.tw/zh-tw/mainboard/trading/margin-trading/exempted.html",
            "wait_element": "table.table-default",
            "download_text": "å¦å­˜ CSV",
            "needs_query": False,
            "retry_count": 2,
            "skiprows": 1,
            "priority": 5
        }
    },
    "settings": {
        "max_retry_days": 7,
        "download_timeout": 30,     # å¢åŠ é€¾æ™‚æ™‚é–“
        "page_load_timeout": 15,    # å¢åŠ é é¢è¼‰å…¥æ™‚é–“
        "implicit_wait": 8,         # å¢åŠ éš±å¼ç­‰å¾…
        "headless": True,           # å»ºè­°ç„¡é ­æ¨¡å¼æé«˜ç©©å®šæ€§
        "user_agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
    }
}

def setup_logging():
    """è¨­å®šæ—¥èªŒç³»çµ±"""
    LOG_DIR.mkdir(exist_ok=True)
    
    formatter = logging.Formatter(
        '%(asctime)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    
    logger = logging.getLogger()
    logger.setLevel(logging.INFO)
    
    for handler in logger.handlers[:]:
        logger.removeHandler(handler)
    
    # æ—¥èªŒæª”æ¡ˆåŒ…å«æ‰¹é‡ä¸‹è¼‰æ¨™è­˜
    file_handler = logging.FileHandler(
        LOG_DIR / f"otc_historical_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log",
        encoding='utf-8'
    )
    file_handler.setFormatter(formatter)
    logger.addHandler(file_handler)
    
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)

class PerformanceMonitor:
    """æ•ˆèƒ½ç›£æ§å™¨"""
    
    def __init__(self):
        self.metrics = {}
    
    @contextmanager
    def measure_time(self, operation_name: str):
        start_time = time.time()
        start_memory = psutil.virtual_memory().used / 1024 / 1024
        
        try:
            yield
        finally:
            end_time = time.time()
            end_memory = psutil.virtual_memory().used / 1024 / 1024
            duration = end_time - start_time
            memory_diff = end_memory - start_memory
            
            self.metrics[operation_name] = {
                "duration_seconds": round(duration, 2),
                "memory_change_mb": round(memory_diff, 2),
                "timestamp": datetime.now().isoformat()
            }
            logging.info(f"{operation_name} å®Œæˆ - è€—æ™‚: {duration:.2f}ç§’, è¨˜æ†¶é«”è®ŠåŒ–: {memory_diff:+.2f}MB")
    
    def get_summary(self) -> Dict[str, Any]:
        if not self.metrics:
            return {"message": "ç„¡æ•ˆèƒ½è³‡æ–™"}
        total_time = sum(m["duration_seconds"] for m in self.metrics.values())
        return {
            "total_operations": len(self.metrics),
            "total_duration_seconds": round(total_time, 2),
            "operations": self.metrics
        }

class DataValidator:
    """è³‡æ–™é©—è­‰å™¨ - ä¿æŒåŸæœ‰é‚è¼¯"""
    
    def validate_dataframe(self, df: pd.DataFrame, file_type: str) -> Dict[str, Any]:
        results = {
            "is_valid": True,
            "errors": [],
            "warnings": [],
            "statistics": {}
        }
        results["statistics"] = {
            "total_rows": len(df),
            "total_columns": len(df.columns),
            "null_counts": df.isnull().sum().to_dict(),
            "unique_stocks": df['stock_id'].nunique() if 'stock_id' in df.columns else 0
        }
        
        if 'stock_id' in df.columns:
            invalid_stock_ids = df[~df['stock_id'].str.match(r'^\d{4}$', na=False)]
            if not invalid_stock_ids.empty:
                results["errors"].append({
                    "type": "invalid_stock_id",
                    "count": len(invalid_stock_ids),
                    "samples": invalid_stock_ids['stock_id'].head(5).tolist()
                })
                results["is_valid"] = False
        
        return results

class OTCHistoricalDownloader:
    """OTCæ­·å²è³‡æ–™æ‰¹é‡ä¸‹è¼‰å™¨"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.download_items = config.get("download_items", {})
        self.settings = config.get("settings", {})
        self.driver = None
        self.performance_monitor = PerformanceMonitor()
    
    def ensure_dir(self, path: Path) -> None:
        path.mkdir(parents=True, exist_ok=True)
        logging.info(f"ç¢ºä¿ç›®éŒ„å­˜åœ¨: {path}")
    
    def setup_chrome_driver(self) -> webdriver.Chrome:
        """è¨­å®š Chrome ç€è¦½å™¨ - é‡å°é•·æ™‚é–“åŸ·è¡Œå„ªåŒ–"""
        options = Options()
        prefs = {
            "download.default_directory": str(DOWNLOAD_DIR),
            "download.prompt_for_download": False,
            "download.directory_upgrade": True,
            "safebrowsing.enabled": True,
            "profile.default_content_setting_values": {
                "images": 2,
                "plugins": 2,
                "popups": 2,
                "geolocation": 2,
                "notifications": 2,
                "media_stream": 2,
            }
        }
        options.add_experimental_option("prefs", prefs)
        options.add_argument('--disable-notifications')
        options.add_argument('--disable-popup-blocking')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-blink-features=AutomationControlled')
        options.add_argument('--disable-extensions')
        options.add_argument('--disable-plugins')
        options.add_argument('--disable-images')
        options.add_argument('--disable-gpu')  # æ¸›å°‘è³‡æºä½¿ç”¨
        options.add_argument('--disable-background-timer-throttling')
        options.add_argument('--disable-renderer-backgrounding')
        options.add_argument('--disable-backgrounding-occluded-windows')
        options.add_experimental_option("excludeSwitches", ["enable-automation"])
        options.add_experimental_option('useAutomationExtension', False)
        
        if self.settings.get('headless', True):
            options.add_argument('--headless')
        
        if 'user_agent' in self.settings:
            options.add_argument(f'--user-agent={self.settings["user_agent"]}')
        
        try:
            service = Service(ChromeDriverManager().install())
            driver = webdriver.Chrome(service=service, options=options)
            driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            driver.implicitly_wait(self.settings.get('implicit_wait', 8))
            driver.set_page_load_timeout(self.settings.get('page_load_timeout', 15))
            logging.info("Chrome WebDriver åˆå§‹åŒ–æˆåŠŸï¼ˆæ­·å²æ‰¹é‡æ¨¡å¼ï¼‰")
            return driver
        except Exception as e:
            logging.error(f"Chrome WebDriver åˆå§‹åŒ–å¤±æ•—: {e}")
            raise
    
    def smart_delay(self):
        """æ™ºèƒ½å»¶é²ï¼Œé¿å…è¢«åµæ¸¬"""
        delay = random.uniform(MIN_DELAY, MAX_DELAY)
        logging.info(f"[â³] ç­‰å¾… {delay:.1f} ç§’...")
        time.sleep(delay)
    
    def convert_date_to_roc(self, date_obj: datetime) -> tuple:
        """è½‰æ›ç‚ºæ°‘åœ‹å¹´æ ¼å¼ï¼Œå›å‚³ (å¹´, æœˆ, æ—¥)"""
        roc_year = date_obj.year - 1911
        return roc_year, date_obj.month, date_obj.day
    
    def generate_trading_dates(self) -> List[datetime]:
        """ç”Ÿæˆäº¤æ˜“æ—¥æœŸåˆ—è¡¨ï¼ˆè·³éé€±æœ«å’Œå‡æ—¥ï¼‰"""
        tw_holidays = holidays.TW()
        dates = []
        current_date = START_DATE
        
        while current_date <= END_DATE:
            # è·³éé€±æœ«å’Œå‡æ—¥
            if current_date.weekday() < 5 and current_date.date() not in tw_holidays:
                dates.append(current_date)
            current_date += timedelta(days=1)
        
        return dates
    
    def get_existing_files(self) -> set:
        """å–å¾—å·²å­˜åœ¨çš„æª”æ¡ˆï¼Œé¿å…é‡è¤‡ä¸‹è¼‰"""
        if not RAW_DIR.exists():
            return set()
        
        existing_files = set()
        for file_path in RAW_DIR.glob("*.csv"):
            # æå–æ—¥æœŸå’Œè³‡æ–™é¡å‹
            match = re.search(r'(\d{8})_(.+)\.csv', file_path.name)
            if match:
                date_str, data_type = match.groups()
                existing_files.add(f"{date_str}_{data_type}")
        
        return existing_files
    
    def set_date_on_page(self, date_obj: datetime) -> bool:
        """åœ¨é é¢ä¸Šè¨­å®šæ—¥æœŸ - æ”¯æ´å¹´æœˆä¸‹æ‹‰ + æ—¥æœŸé»é¸"""
        try:
            roc_year, month, day = self.convert_date_to_roc(date_obj)
            logging.info(f"  è¨­å®šæ—¥æœŸï¼šæ°‘åœ‹{roc_year}å¹´{month}æœˆ{day}æ—¥")
            
            # ç­‰å¾…é é¢è¼‰å…¥
            time.sleep(2)
            
            # æ–¹æ³•1: å˜—è©¦ä½¿ç”¨æ–‡å­—è¼¸å…¥ï¼ˆå¦‚ daily_close_no1430ï¼‰
            try:
                date_input = self.driver.find_element(By.CSS_SELECTOR, "input[name='date'], input[type='text'].date")
                roc_date_str = f"{roc_year}/{month:02d}/{day:02d}"
                self.driver.execute_script(f"""
                    var dateInput = arguments[0];
                    dateInput.removeAttribute('readonly');
                    dateInput.value = '{roc_date_str}';
                    dateInput.dispatchEvent(new Event('change', {{ bubbles: true }}));
                """, date_input)
                logging.info(f"    ä½¿ç”¨æ–‡å­—è¼¸å…¥è¨­å®šæ—¥æœŸï¼š{roc_date_str}")
                time.sleep(1)
                return True
            except:
                pass
            
            # æ–¹æ³•2: ä½¿ç”¨å¹´æœˆä¸‹æ‹‰ + æ—¥æœŸé»é¸
            try:
                # è¨­å®šå¹´ä»½
                year_elements = self.driver.find_elements(By.NAME, "year")
                if year_elements:
                    year_select = Select(year_elements[0])
                    year_select.select_by_value(str(roc_year))
                    logging.info(f"    è¨­å®šå¹´ä»½ï¼š{roc_year}")
                    time.sleep(1)
                
                # è¨­å®šæœˆä»½
                month_elements = self.driver.find_elements(By.NAME, "month")
                if month_elements:
                    month_select = Select(month_elements[0])
                    month_select.select_by_value(str(month))
                    logging.info(f"    è¨­å®šæœˆä»½ï¼š{month}")
                    time.sleep(1)
                
                # é»é¸æ—¥æœŸï¼ˆå¦‚æœæœ‰æ—¥æ›†ï¼‰
                try:
                    day_elements = self.driver.find_elements(By.XPATH, f"//td[text()='{day}' and not(contains(@class, 'disabled'))]")
                    if day_elements:
                        day_elements[0].click()
                        logging.info(f"    é»é¸æ—¥æœŸï¼š{day}")
                        time.sleep(1)
                except:
                    logging.debug("    æœªæ‰¾åˆ°å¯é»é¸çš„æ—¥æœŸå…ƒç´ ")
                
                return True
            except Exception as e:
                logging.warning(f"    å¹´æœˆæ—¥è¨­å®šå¤±æ•—ï¼š{e}")
            
            # æ–¹æ³•3: ç›´æ¥ä½¿ç”¨è³‡æ–™æ—¥æœŸè¼¸å…¥æ¡†
            try:
                date_inputs = self.driver.find_elements(By.CSS_SELECTOR, "input[placeholder*='æ—¥æœŸ'], input[id*='date'], input[class*='date']")
                for date_input in date_inputs:
                    try:
                        roc_date_str = f"{roc_year}/{month:02d}/{day:02d}"
                        self.driver.execute_script(f"""
                            arguments[0].value = '{roc_date_str}';
                            arguments[0].dispatchEvent(new Event('change'));
                        """, date_input)
                        logging.info(f"    ä½¿ç”¨é€šç”¨æ—¥æœŸè¼¸å…¥ï¼š{roc_date_str}")
                        time.sleep(1)
                        return True
                    except:
                        continue
            except:
                pass
            
            logging.warning("  æ‰€æœ‰æ—¥æœŸè¨­å®šæ–¹æ³•å‡å¤±æ•—")
            return False
            
        except Exception as e:
            logging.error(f"  æ—¥æœŸè¨­å®šéŒ¯èª¤ï¼š{e}")
            return False
    
    def close_cookie_banner(self) -> None:
        """é—œé–‰ Cookie æ©«å¹…"""
        try:
            cookie_btn = WebDriverWait(self.driver, 2).until(
                EC.element_to_be_clickable((By.CSS_SELECTOR, ".cookie-banner .btn-close"))
            )
            cookie_btn.click()
            time.sleep(0.5)
            logging.debug("Cookie banner å·²é—œé–‰")
        except:
            logging.debug("æœªæ‰¾åˆ° cookie banner")
    
    def wait_for_download(self, filename_pattern: str, timeout: int = None) -> Optional[Path]:
        """ç­‰å¾…æª”æ¡ˆä¸‹è¼‰å®Œæˆ"""
        if timeout is None:
            timeout = self.settings.get('download_timeout', 30)
        
        start_time = time.time()
        while time.time() - start_time < timeout:
            for filename in DOWNLOAD_DIR.iterdir():
                if filename_pattern in filename.name and not filename.name.endswith('.crdownload'):
                    logging.info(f"    ä¸‹è¼‰å®Œæˆ: {filename}")
                    return filename
            time.sleep(0.5)
        logging.warning(f"    ä¸‹è¼‰é€¾æ™‚: {filename_pattern}")
        return None
    
    def download_single_item(self, name: str, config: Dict[str, Any], date_obj: datetime) -> bool:
        """ä¸‹è¼‰å–®ä¸€é …ç›®çš„å–®ä¸€æ—¥æœŸè³‡æ–™"""
        try:
            date_str = date_obj.strftime("%Y%m%d")
            logging.info(f"  [è™•ç†] {name} - {config['name']}")
            
            # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å·²å­˜åœ¨
            expected_filename_patterns = [
                f"{date_str}_{name}.csv",
                f"{date_str}_{name}_buy.csv",
                f"{date_str}_{name}_sell.csv"
            ]
            
            for pattern in expected_filename_patterns:
                if (RAW_DIR / pattern).exists():
                    logging.info(f"    [â­] æª”æ¡ˆå·²å­˜åœ¨ï¼Œè·³éï¼š{pattern}")
                    return True
            
            # å‰å¾€é é¢
            self.driver.get(config['url'])
            time.sleep(3)
            self.close_cookie_banner()
            
            # è¨­å®šæ—¥æœŸ
            if not self.set_date_on_page(date_obj):
                logging.warning(f"    æ—¥æœŸè¨­å®šå¤±æ•—ï¼Œè·³é")
                return False
            
            # è™•ç†ç‰¹æ®Šè¨­å®š
            if "select_element" in config:
                try:
                    sel_name = config["select_element"]["name"]
                    sel_val = config["select_element"]["value"]
                    select_elem = self.driver.find_element(By.NAME, sel_name)
                    select = Select(select_elem)
                    select.select_by_value(sel_val)
                    logging.info(f"    è¨­å®šä¸‹æ‹‰ {sel_name} = {sel_val}")
                    time.sleep(1)
                except Exception as e:
                    logging.warning(f"    ä¸‹æ‹‰è¨­å®šå¤±æ•—ï¼š{e}")
            
            # é»æ“ŠæŸ¥è©¢æŒ‰éˆ•ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if config.get("needs_query", False):
                try:
                    query_btns = self.driver.find_elements(By.CSS_SELECTOR, "button.btn-primary, button[type='submit']")
                    if query_btns:
                        query_btns[0].click()
                        logging.info("    é»æ“ŠæŸ¥è©¢æŒ‰éˆ•")
                        time.sleep(4)
                except Exception as e:
                    logging.warning(f"    æŸ¥è©¢æŒ‰éˆ•é»æ“Šå¤±æ•—ï¼š{e}")
            
            # ç­‰å¾…è¡¨æ ¼è¼‰å…¥
            try:
                WebDriverWait(self.driver, 10).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, config["wait_element"]))
                )
                logging.info("    è³‡æ–™è¡¨æ ¼è¼‰å…¥å®Œæˆ")
            except:
                logging.warning("    è³‡æ–™è¡¨æ ¼è¼‰å…¥é€¾æ™‚ï¼Œå˜—è©¦ç¹¼çºŒä¸‹è¼‰")
            
            # åŸ·è¡Œä¸‹è¼‰
            return self._execute_download(name, config, date_str)
            
        except Exception as e:
            logging.error(f"    {name} è™•ç†å¤±æ•—ï¼š{e}")
            return False
    
    def _execute_download(self, name: str, config: Dict[str, Any], date_str: str) -> bool:
        """åŸ·è¡Œä¸‹è¼‰å‹•ä½œ"""
        download_texts = [config["download_text"], "ä¸‹è¼‰CSV", "å¦å­˜CSV", "ä¸‹è¼‰ CSV", "å¦å­˜ CSV"]
        
        for txt in download_texts:
            try:
                # å°‹æ‰¾ä¸‹è¼‰æŒ‰éˆ•
                xpath = f"//a[contains(text(), '{txt}')] | //button[contains(text(), '{txt}')]"
                btn = WebDriverWait(self.driver, 5).until(
                    EC.element_to_be_clickable((By.XPATH, xpath))
                )
                self.driver.execute_script("arguments[0].click();", btn)
                logging.info(f"    é»æ“Šä¸‹è¼‰ï¼šã€{txt}ã€")
                time.sleep(3)
                
                # ç­‰å¾…ä¸‹è¼‰å®Œæˆ
                dl_file = self.wait_for_download(".csv", 30)
                if dl_file:
                    return self._move_downloaded_file(dl_file, name, date_str)
                    
            except Exception as e:
                logging.debug(f"    ä¸‹è¼‰æ–¹æ³• '{txt}' å¤±æ•—ï¼š{e}")
                continue
        
        logging.error("    [âŒ] æ‰€æœ‰ä¸‹è¼‰æ–¹æ³•å‡å¤±æ•—")
        return False
    
    def _move_downloaded_file(self, dl_file: Path, name: str, date_str: str) -> bool:
        """ç§»å‹•ä¸‹è¼‰çš„æª”æ¡ˆåˆ°æŒ‡å®šä½ç½®"""
        try:
            orig_name = dl_file.name
            
            # æ ¹æ“šä¸åŒé¡å‹è¨­å®šæª”æ¡ˆåç¨±
            if name == "investment_trust_buy":
                base, ext = os.path.splitext(orig_name)
                new_name = f"{date_str}_{name}.csv"
            elif name == "investment_trust_sell":
                base, ext = os.path.splitext(orig_name)
                new_name = f"{date_str}_{name}.csv"
            else:
                new_name = f"{date_str}_{name}.csv"
            
            new_path = RAW_DIR / new_name
            shutil.move(str(dl_file), str(new_path))
            logging.info(f"    [âœ…] ä¸‹è¼‰æˆåŠŸ â†’ {new_path}")
            return True
            
        except Exception as e:
            logging.error(f"    ç§»å‹•æª”æ¡ˆå¤±æ•—ï¼š{e}")
            return False
    
    def download_all_historical(self) -> Dict[str, int]:
        """ä¸‹è¼‰æ‰€æœ‰æ­·å²è³‡æ–™"""
        self.ensure_dir(RAW_DIR)
        self.driver = self.setup_chrome_driver()
        
        # ç”Ÿæˆäº¤æ˜“æ—¥æœŸåˆ—è¡¨
        trading_dates = self.generate_trading_dates()
        existing_files = self.get_existing_files()
        
        # çµ±è¨ˆè³‡è¨Š
        total_dates = len(trading_dates)
        total_items = len(self.download_items)
        total_tasks = total_dates * total_items
        
        # æŒ‰å„ªå…ˆé †åºæ’åºä¸‹è¼‰é …ç›®
        sorted_items = sorted(
            self.download_items.items(),
            key=lambda x: x[1].get('priority', 999)
        )
        
        logging.info(f"\n=== ä¸Šæ«ƒæ­·å²è³‡æ–™æ‰¹é‡ä¸‹è¼‰é–‹å§‹ ===")
        logging.info(f"æ—¥æœŸç¯„åœ: {START_DATE.strftime('%Y-%m-%d')} ~ {END_DATE.strftime('%Y-%m-%d')}")
        logging.info(f"äº¤æ˜“æ—¥ç¸½æ•¸: {total_dates}")
        logging.info(f"è³‡æ–™é …ç›®æ•¸: {total_items}")
        logging.info(f"é è¨ˆç¸½ä»»å‹™: {total_tasks}")
        logging.info(f"å·²å­˜åœ¨æª”æ¡ˆ: {len(existing_files)}")
        estimated_time = total_tasks * 15 / 60  # æ¯å€‹ä»»å‹™ç´„15ç§’
        logging.info(f"é ä¼°åŸ·è¡Œæ™‚é–“: {estimated_time:.1f} åˆ†é˜")
        
        # çµ±è¨ˆè®Šæ•¸
        results = {
            "success": 0,
            "failed": 0,
            "skipped": 0,
            "failed_tasks": []
        }
        
        try:
            # é›™é‡è¿´åœˆï¼šå¤–å±¤æ—¥æœŸï¼Œå…§å±¤è³‡æ–™é …ç›®
            for date_idx, date_obj in enumerate(trading_dates, 1):
                date_str = date_obj.strftime("%Y%m%d")
                logging.info(f"\nâ”€â”€ è™•ç†æ—¥æœŸ {date_obj.strftime('%Y-%m-%d')} ({date_idx}/{total_dates}) â”€â”€")
                
                # å…§å±¤ï¼šå„å€‹è³‡æ–™é …ç›®
                for item_idx, (name, config) in enumerate(sorted_items, 1):
                    task_desc = f"{name}_{date_str}"
                    logging.info(f"\n  ä»»å‹™ {item_idx}/{total_items}: {name}")
                    
                    # æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨
                    if task_desc in existing_files:
                        logging.info(f"    [â­] å·²å­˜åœ¨ï¼Œè·³é")
                        results["skipped"] += 1
                        continue
                    
                    # åŸ·è¡Œä¸‹è¼‰
                    try:
                        if self.download_single_item(name, config, date_obj):
                            results["success"] += 1
                        else:
                            results["failed"] += 1
                            results["failed_tasks"].append(task_desc)
                    except Exception as e:
                        logging.error(f"    ä»»å‹™åŸ·è¡Œç•°å¸¸ï¼š{e}")
                        results["failed"] += 1
                        results["failed_tasks"].append(task_desc)
                    
                    # æ™ºèƒ½å»¶é²ï¼ˆé™¤äº†æœ€å¾Œä¸€å€‹ä»»å‹™ï¼‰
                    if not (date_idx == total_dates and item_idx == total_items):
                        self.smart_delay()
                
                # æ¯å®Œæˆä¸€å€‹æ—¥æœŸï¼Œè¨˜éŒ„é€²åº¦
                progress = (date_idx / total_dates) * 100
                logging.info(f"  æ—¥æœŸ {date_str} å®Œæˆï¼Œæ•´é«”é€²åº¦ï¼š{progress:.1f}%")
        
        except KeyboardInterrupt:
            logging.warning("\n[â¹] ä½¿ç”¨è€…ä¸­æ–·ä¸‹è¼‰")
        except Exception as e:
            logging.error(f"\n[âŒ] ä¸‹è¼‰éç¨‹ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        finally:
            if self.driver:
                self.driver.quit()
                logging.info("Chrome WebDriver å·²é—œé–‰")
        
        # è¼¸å‡ºæœ€çµ‚çµ±è¨ˆ
        logging.info(f"\n[ğŸ“Š] ä¸‹è¼‰çµ±è¨ˆ:")
        logging.info(f"    - æˆåŠŸ: {results['success']}")
        logging.info(f"    - å¤±æ•—: {results['failed']}")
        logging.info(f"    - è·³é: {results['skipped']}")
        logging.info(f"    - ç¸½è¨ˆ: {results['success'] + results['failed'] + results['skipped']}")
        
        if results["failed_tasks"]:
            logging.warning(f"å¤±æ•—ä»»å‹™æ¸…å–®: {results['failed_tasks'][:10]}...")  # åªé¡¯ç¤ºå‰10å€‹
        
        return results

# ===== æ¸…æ´—åŠŸèƒ½ï¼ˆä¿æŒåŸæœ‰é‚è¼¯ï¼‰ =====
class OTCDataCleaner:
    """OTCè³‡æ–™æ¸…æ´—å™¨é¡åˆ¥ - å®Œå…¨ä¿æŒåŸæœ‰é‚è¼¯"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.download_items = config.get("download_items", {})
        self.validator = DataValidator()
        self.performance_monitor = PerformanceMonitor()
    
    def ensure_dir(self, path: Path) -> None:
        path.mkdir(parents=True, exist_ok=True)
    
    def read_csv_with_encoding(self, file_path: Path, skiprows: int = 0) -> Optional[pd.DataFrame]:
        encodings = ["cp950", "big5", "utf-8-sig", "utf-8", "gb2312", "gbk", "gb18030"]
        for encoding in encodings:
            try:
                df = pd.read_csv(file_path, encoding=encoding, skiprows=skiprows,
                                 dtype=object, low_memory=False, thousands=',')
                logging.debug(f"æˆåŠŸä½¿ç”¨ {encoding} ç·¨ç¢¼è®€å– {file_path.name}ï¼Œskiprows={skiprows}")
                if df is not None and len(df.columns) > 0:
                    first_col = str(df.columns[0]).replace(',', '').replace('.', '')
                    if first_col.isdigit() and skiprows > 0:
                        logging.debug(f"åµæ¸¬åˆ°æ¬„ä½åç¨±ç•°å¸¸ï¼ˆ{df.columns[0]}ï¼‰ï¼Œå˜—è©¦ skiprows={skiprows-1}")
                        return self.read_csv_with_encoding(file_path, skiprows - 1)
                return df
            except Exception as e:
                logging.debug(f"ä½¿ç”¨ {encoding} ç·¨ç¢¼è®€å–å¤±æ•—ï¼ˆskiprows={skiprows}ï¼‰ï¼š{e}")
                continue
        logging.error(f"ç„¡æ³•è®€å–æª”æ¡ˆ {file_path.name}ï¼Œskiprows={skiprows}")
        return None
    
    def get_file_type_and_config(self, filename: str) -> tuple:
        filename_lower = filename.lower()
        file_patterns = {
            "daily_close_no1430": ("daily_close_no1430", 3),
            "bigd_": ("institutional_detail", 1),
            "brktop1_": ("sec_trading", 2),
            "daytraderpt_": ("day_trading", 5),
            "margratio_": ("highlight", 2),
            "owz66u_": ("sbl", 2),
            "rsta3106_": ("margin_transactions", 2),
            "margmark_": ("exempted", 1)
        }
        for pattern, (config_key, skiprows) in file_patterns.items():
            if pattern in filename_lower:
                logging.debug(f"  æª”æ¡ˆ {filename} åŒ¹é…æ¨¡å¼ {pattern}ï¼Œé¡å‹={config_key}ï¼Œè·³éè¡Œæ•¸={skiprows}")
                return config_key, skiprows
        
        if filename_lower.startswith("sit_"):
            if "_buy" in filename_lower:
                return "investment_trust_buy", 1
            elif "_sell" in filename_lower:
                return "investment_trust_sell", 1
        
        return None, 0
    
    def clean_numeric_column(self, series: pd.Series) -> pd.Series:
        cleaned = series.astype(str).str.replace(",", "").str.strip()
        cleaned = cleaned.replace(["--", "---", "----", "ã€€", ""], "0")
        numeric_values = pd.to_numeric(cleaned, errors="coerce").fillna(0)
        
        result = []
        for val in numeric_values:
            if pd.isna(val):
                result.append(0)
            elif val == int(val):
                result.append(int(val))
            else:
                result.append(float(val))
        
        return pd.Series(result, index=series.index)
    
    def extract_stock_id(self, series: pd.Series) -> pd.Series:
        """æå–4ä½æ•¸è‚¡ç¥¨ä»£è™Ÿï¼Œæ’é™¤å«å­—æ¯çš„ä»£è™Ÿ"""
        def extract_4_digits(code_str):
            if pd.isna(code_str):
                return None
            code_str = str(code_str).strip()
            
            if code_str.isdigit():
                if len(code_str) < 3:
                    return None
                elif len(code_str) == 3:
                    return code_str.zfill(4)
                elif len(code_str) == 4:
                    return code_str
                else:
                    return None
            
            if any('\u4e00' <= char <= '\u9fff' for char in code_str):
                return None
            
            if re.search(r'[A-Za-z]', code_str):
                return None
            
            match = re.match(r'^(\d{3,4})', code_str)
            if match:
                digits = match.group(1)
                if len(digits) == 3:
                    return digits.zfill(4)
                elif len(digits) == 4:
                    return digits
            
            return None
        
        return series.apply(extract_4_digits)
    
    def get_all_raw_files_by_date(self) -> Dict[str, List[Path]]:
        """å–å¾—æ‰€æœ‰åŸå§‹æª”æ¡ˆï¼ŒæŒ‰æ—¥æœŸåˆ†çµ„"""
        if not RAW_DIR.exists():
            return {}
        
        files_by_date = {}
        for file_path in RAW_DIR.glob("*.csv"):
            # æå–æ—¥æœŸ
            match = re.search(r'(\d{8})_', file_path.name)
            if match:
                date_str = match.group(1)
                if date_str not in files_by_date:
                    files_by_date[date_str] = []
                files_by_date[date_str].append(file_path)
        
        return files_by_date
    
    def clean_single_file(self, file_path: Path) -> bool:
        """æ¸…æ´—å–®ä¸€æª”æ¡ˆ - ä¿æŒåŸæœ‰æ¸…æ´—é‚è¼¯"""
        filename = file_path.name
        logging.info(f"  è™•ç†ï¼š{filename}")
        
        file_type, skiprows = self.get_file_type_and_config(filename)
        skiprows = max(skiprows, 0)
        if file_type is None:
            logging.warning("    [âŒ] æœªåŒ¹é…æ¸…æ´—è¦å‰‡ï¼Œè·³é")
            return False
        
        df = self.read_csv_with_encoding(file_path, skiprows)
        if df is None:
            return False
        
        df.columns = df.columns.str.strip()
        df = df.dropna(axis=1, how="all").dropna(axis=0, how="all")
        if len(df) == 0:
            logging.warning(f"    [âŒ] æª”æ¡ˆ {filename} æ¸…ç†å¾Œç„¡è³‡æ–™")
            return False
        
        try:
            clean_df = self._clean_by_type(df, file_type, filename)
            if clean_df is None or len(clean_df) == 0:
                logging.error(f"    [âŒ] æª”æ¡ˆ {filename} æ¸…ç†å¤±æ•—")
                return False
            
            # æœ€çµ‚éæ¿¾ï¼šåªä¿ç•™ç´”4ä½æ•¸ä¸” >=1000 çš„è‚¡ç¥¨ä»£è™Ÿ
            if 'stock_id' in clean_df.columns:
                clean_df = clean_df[
                    (clean_df['stock_id'].str.len() == 4) &
                    (clean_df['stock_id'].str.isdigit()) &
                    (clean_df['stock_id'].astype(int) >= 1000)
                ]
            
            validation_result = self.validator.validate_dataframe(clean_df, file_type)
            if not validation_result["is_valid"]:
                logging.warning(f"    [âš ï¸] è³‡æ–™é©—è­‰ç™¼ç¾å•é¡Œï¼š{validation_result['errors']}")
            
            # è¼¸å‡ºåˆ° otc_cleaned ç›®éŒ„ï¼Œä¿æŒåŸæª”å
            output_path = CLEAN_DIR / filename
            with open(output_path, 'w', encoding='utf-8-sig', newline='') as f:
                float_format = lambda x: '{:.0f}'.format(x) if isinstance(x, (int, float)) and x == int(x) else '{:.2f}'.format(x) if isinstance(x, float) else str(x)
                clean_df.to_csv(f, index=False, float_format=float_format)
            
            logging.info(f"    [âœ…] æ¸…æ´—å®Œæˆ: {filename} ({len(clean_df)} è¡Œ)")
            return True
            
        except Exception as e:
            logging.error(f"    [âŒ] æ¸…æ´—å¤±æ•—ï¼š{e}")
            logging.error(traceback.format_exc())
            return False
    
    def _clean_by_type(self, df: pd.DataFrame, file_type: str, filename: str) -> Optional[pd.DataFrame]:
        """æ ¹æ“šæª”æ¡ˆé¡å‹é¸æ“‡æ¸…æ´—æ–¹æ³• - ä¿æŒåŸæœ‰é‚è¼¯"""
        if file_type == "daily_close_no1430" or "daily_close_no1430" in filename.lower():
            return self._clean_daily_close(df)
        elif file_type == "institutional_detail" or "bigd_" in filename.lower():
            return self._clean_institutional_detail(df)
        elif file_type == "sec_trading" or "brktop1_" in filename.lower():
            return self._clean_sec_trading(df)
        elif file_type == "day_trading" or "daytraderpt_" in filename.lower():
            return self._clean_day_trading(df)
        elif file_type == "highlight" or "margratio_" in filename.lower():
            return self._clean_highlight(df)
        elif file_type == "sbl" or "owz66u_" in filename.lower():
            return self._clean_sbl(df)
        elif file_type == "margin_transactions" or "rsta3106_" in filename.lower():
            return self._clean_margin_transactions(df)
        elif file_type == "exempted" or "margmark_" in filename.lower():
            return self._clean_exempted(df)
        elif "investment_trust" in file_type or filename.lower().startswith("sit_"):
            return self._clean_investment_trust(df)
        else:
            logging.warning(f"  æœªçŸ¥æª”æ¡ˆé¡å‹ï¼š{file_type}")
            return None
    
    def _clean_daily_close(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ¸…æ´—æ¯æ—¥æ”¶ç›¤è¡Œæƒ…è³‡æ–™"""
        cols = list(df.columns)
        
        if len(cols) > 0 and str(cols[0]).replace(',', '').replace('.', '').isdigit():
            logging.error(f"  æ¬„ä½è­˜åˆ¥ç•°å¸¸ï¼Œç¬¬ä¸€æ¬„ç‚ºæ•¸å­—ï¼š{cols[0]}")
            return None
        
        code_col = next((c for c in cols if "ä»£è™Ÿ" in c or "ä»£ç¢¼" in c), None)
        name_col = next((c for c in cols if "åç¨±" in c), None)
        close_col = next((c for c in cols if "æ”¶ç›¤" in c and "æ”¶ç›¤" == c[:2]), None)
        
        if not all([code_col, name_col, close_col]):
            logging.error("  ç¼ºå°‘å¿…è¦æ¬„ä½")
            return None
        
        column_mapping = {
            code_col: "stock_id",
            name_col: "name",
            close_col: "close"
        }
        
        optional_fields = {
            "æ¼²è·Œ": "change", "é–‹ç›¤": "open", "æœ€é«˜": "high", "æœ€ä½": "low",
            "å‡åƒ¹": "avg_price", "æˆäº¤è‚¡æ•¸": "volume", "æˆäº¤é‡‘é¡": "amount",
            "æˆäº¤ç­†æ•¸": "trades", "æœ€å¾Œè²·åƒ¹": "last_bid_price", "æœ€å¾Œè²·é‡": "last_bid_vol",
            "æœ€å¾Œè³£åƒ¹": "last_ask_price", "æœ€å¾Œè³£é‡": "last_ask_vol", "ç™¼è¡Œè‚¡æ•¸": "issued_shares",
            "æ¬¡æ—¥åƒè€ƒåƒ¹": "next_ref_price", "æ¬¡æ—¥æ¼²åœåƒ¹": "next_up_limit", "æ¬¡æ—¥è·Œåœåƒ¹": "next_down_limit"
        }
        
        for pattern, new_name in optional_fields.items():
            matching_col = next((c for c in cols if pattern in c), None)
            if matching_col:
                column_mapping[matching_col] = new_name
        
        available_cols = [col for col in column_mapping.keys() if col in df.columns]
        clean_df = df[available_cols].rename(columns=column_mapping).copy()
        
        clean_df["stock_id"] = self.extract_stock_id(clean_df["stock_id"])
        clean_df = clean_df.dropna(subset=["stock_id"])
        clean_df = clean_df[clean_df["stock_id"].str.match(r'^\d{4}
    
def verify_clean_data():
    """é©—è­‰æ¸…ç†å¾Œçš„è³‡æ–™å“è³ª"""
    logging.info("\n=== è³‡æ–™å“è³ªé©—è­‰ ===")
    
    issues = []
    
    for csv_file in CLEAN_DIR.glob("*.csv"):
        try:
            df = pd.read_csv(csv_file, encoding='utf-8-sig')
            
            if 'stock_id' in df.columns:
                invalid_ids = df[~df['stock_id'].astype(str).str.match(r'^\d{4}
        """æ¸…æ´—æ‰€æœ‰æ­·å²æª”æ¡ˆ"""
        self.ensure_dir(CLEAN_DIR)
        results = {"success": 0, "failed": 0, "failed_files": []}
        
        # å–å¾—æ‰€æœ‰æª”æ¡ˆï¼ŒæŒ‰æ—¥æœŸåˆ†çµ„
        files_by_date = self.get_all_raw_files_by_date()
        total_dates = len(files_by_date)
        
        logging.info(f"\n=== é–‹å§‹æ¸…æ´—æ­·å²è³‡æ–™ ===")
        logging.info(f"æ‰¾åˆ° {total_dates} å€‹æ—¥æœŸçš„è³‡æ–™")
        
        with self.performance_monitor.measure_time("æ­·å²è³‡æ–™æ¸…æ´—"):
            for date_idx, (date_str, file_list) in enumerate(sorted(files_by_date.items()), 1):
                logging.info(f"\nâ”€â”€ æ¸…æ´—æ—¥æœŸ {date_str} ({date_idx}/{total_dates}) â”€â”€")
                
                for file_path in file_list:
                    try:
                        if self.clean_single_file(file_path):
                            results["success"] += 1
                        else:
                            results["failed"] += 1
                            results["failed_files"].append(file_path.name)
                    except Exception as e:
                        logging.error(f"æ¸…ç†æª”æ¡ˆ {file_path.name} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                        results["failed"] += 1
                        results["failed_files"].append(file_path.name)
        
        logging.info(f"\n[ğŸ“Š] æ¸…æ´—çµ±è¨ˆ:")
        logging.info(f"    - æˆåŠŸ: {results['success']}")
        logging.info(f"    - å¤±æ•—: {results['failed']}")
        
        return results

def main():
    """ä¸»è¦åŸ·è¡Œå‡½æ•¸"""
    setup_logging()
    logging.info("=== ä¸Šæ«ƒæ­·å²è³‡æ–™æ‰¹é‡ä¸‹è¼‰ + æ¸…æ´—ç³»çµ± ===")
    
    config = load_config()
    
    # ç¢ºèªåŸ·è¡Œ
    print(f"\nç›®æ¨™æ—¥æœŸç¯„åœ: {START_DATE.strftime('%Y-%m-%d')} ~ {END_DATE.strftime('%Y-%m-%d')}")
    print(f"é ä¼°äº¤æ˜“æ—¥: ~{len([d for d in pd.date_range(START_DATE, END_DATE) if d.weekday() < 5])} å¤©")
    print(f"è³‡æ–™é …ç›®: {len(config['download_items'])} ç¨®")
    print(f"é ä¼°ç¸½æ™‚é–“: 4-6 å°æ™‚")
    print(f"è¼¸å‡ºç›®éŒ„: {RAW_DIR} (åŸå§‹), {CLEAN_DIR} (æ¸…æ´—)")
    
    response = input("\nâš ï¸  é€™æ˜¯é•·æ™‚é–“åŸ·è¡Œä»»å‹™ï¼Œæ˜¯å¦ç¢ºå®šé–‹å§‹? (y/N): ").strip().lower()
    if response != 'y':
        print("å–æ¶ˆåŸ·è¡Œ")
        return
    
    start_time = datetime.now()
    performance_monitor = PerformanceMonitor()
    
    try:
        # æ­¥é©Ÿ 1: æ‰¹é‡ä¸‹è¼‰æ­·å²è³‡æ–™
        logging.info("\n=== æ­¥é©Ÿ 1: æ‰¹é‡ä¸‹è¼‰æ­·å²è³‡æ–™ ===")
        downloader = OTCHistoricalDownloader(config)
        
        with performance_monitor.measure_time("ç¸½ä¸‹è¼‰æ™‚é–“"):
            download_results = downloader.download_all_historical()
        
        # æ­¥é©Ÿ 2: æ¸…æ´—æ‰€æœ‰ä¸‹è¼‰çš„è³‡æ–™
        logging.info("\n=== æ­¥é©Ÿ 2: æ¸…æ´—æ­·å²è³‡æ–™ ===")
        cleaner = OTCDataCleaner(config)
        
        with performance_monitor.measure_time("ç¸½æ¸…æ´—æ™‚é–“"):
            clean_results = cleaner.clean_all_historical_files()
        
        # å®Œæˆå ±å‘Š
        end_time = datetime.now()
        duration = end_time - start_time
        
        logging.info(f"\n[ğŸ‰] æ‰€æœ‰ç¨‹åºå®Œæˆ!")
        logging.info(f"[â±] ç¸½åŸ·è¡Œæ™‚é–“: {duration}")
        logging.info(f"[ğŸ“Š] ä¸‹è¼‰çµæœ: æˆåŠŸ {download_results['success']}, å¤±æ•— {download_results['failed']}, è·³é {download_results['skipped']}")
        logging.info(f"[ğŸ“Š] æ¸…æ´—çµæœ: æˆåŠŸ {clean_results['success']}, å¤±æ•— {clean_results['failed']}")
        logging.info(f"[ğŸ“] åŸå§‹è³‡æ–™: {RAW_DIR}")
        logging.info(f"[ğŸ“] æ¸…æ´—è³‡æ–™: {CLEAN_DIR}")
        
        # ä¿å­˜åŸ·è¡Œå ±å‘Š
        performance_report_path = LOG_DIR / f"historical_performance_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        performance_monitor.save_report(performance_report_path)
        
    except KeyboardInterrupt:
        logging.warning("\n[â¹] ä½¿ç”¨è€…ä¸­æ–·åŸ·è¡Œ")
    except Exception as e:
        logging.error(f"\n[âŒ] åŸ·è¡Œéç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
        logging.error(traceback.format_exc())
        raise

def load_config() -> Dict[str, Any]:
    """è¼‰å…¥è¨­å®šæª”"""
    config_file = BASE_DIR / "otc_config.json"
    
    if config_file.exists():
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
            logging.info(f"å·²è¼‰å…¥è¨­å®šæª”: {config_file}")
            return config
        except Exception as e:
            logging.warning(f"è¨­å®šæª”è¼‰å…¥å¤±æ•—ï¼Œä½¿ç”¨é è¨­è¨­å®š: {e}")
    else:
        logging.info("æœªæ‰¾åˆ°è¨­å®šæª”ï¼Œä½¿ç”¨é è¨­è¨­å®š")
    
    return DEFAULT_CONFIG

if __name__ == "__main__":
    main(), na=False)]
        
        numeric_cols = [col for col in clean_df.columns if col not in ["stock_id", "name"]]
        for col in numeric_cols:
            clean_df[col] = self.clean_numeric_column(clean_df[col])
        
        return clean_df.sort_values("stock_id").reset_index(drop=True)
    
    def _clean_institutional_detail(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ¸…æ´—ä¸‰å¤§æ³•äººè²·è³£æ˜ç´°è³‡æ–™"""
        cols = list(df.columns)
        
        code_col = next((c for c in cols if "ä»£è™Ÿ" in c or "ä»£ç¢¼" in c), None)
        name_col = next((c for c in cols if "åç¨±" in c), None)
        
        if not all([code_col, name_col]):
            logging.error("  ç¼ºå°‘å¿…è¦æ¬„ä½")
            return None
        
        column_mapping = {
            code_col: "stock_id",
            name_col: "name"
        }
        
        institutional_fields = {
            "å¤–è³‡åŠé™¸è³‡": "ii_foreign_net",
            "å¤–è³‡è‡ªç‡Ÿå•†": "ii_foreign_self_net",
            "æŠ•ä¿¡": "ii_trust_net",
            "è‡ªç‡Ÿå•†(è‡ªè¡Œè²·è³£)": "ii_dealer_self_net",
            "è‡ªç‡Ÿå•†(é¿éšª)": "ii_dealer_hedge_net",
            "åˆè¨ˆ": "ii_total_net"
        }
        
        for pattern, new_name in institutional_fields.items():
            matching_col = next((c for c in cols if pattern in c and "è²·è³£è¶…" in c), None)
            if matching_col:
                column_mapping[matching_col] = new_name
        
        available_cols = [col for col in column_mapping.keys() if col in df.columns]
        clean_df = df[available_cols].rename(columns=column_mapping).copy()
        
        clean_df["stock_id"] = self.extract_stock_id(clean_df["stock_id"])
        clean_df = clean_df.dropna(subset=["stock_id"])
        clean_df = clean_df[clean_df["stock_id"].str.match(r'^\d{4}
    
    def clean_all_historical_files(self) -> Dict[str, int]:
        """æ¸…æ´—æ‰€æœ‰æ­·å²æª”æ¡ˆ"""
        self.ensure_dir(CLEAN_DIR)
        results = {"success": 0, "failed": 0, "failed_files": []}
        
        # å–å¾—æ‰€æœ‰æª”æ¡ˆï¼ŒæŒ‰æ—¥æœŸåˆ†çµ„
        files_by_date = self.get_all_raw_files_by_date()
        total_dates = len(files_by_date)
        
        logging.info(f"\n=== é–‹å§‹æ¸…æ´—æ­·å²è³‡æ–™ ===")
        logging.info(f"æ‰¾åˆ° {total_dates} å€‹æ—¥æœŸçš„è³‡æ–™")
        
        with self.performance_monitor.measure_time("æ­·å²è³‡æ–™æ¸…æ´—"):
            for date_idx, (date_str, file_list) in enumerate(sorted(files_by_date.items()), 1):
                logging.info(f"\nâ”€â”€ æ¸…æ´—æ—¥æœŸ {date_str} ({date_idx}/{total_dates}) â”€â”€")
                
                for file_path in file_list:
                    try:
                        if self.clean_single_file(file_path):
                            results["success"] += 1
                        else:
                            results["failed"] += 1
                            results["failed_files"].append(file_path.name)
                    except Exception as e:
                        logging.error(f"æ¸…ç†æª”æ¡ˆ {file_path.name} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                        results["failed"] += 1
                        results["failed_files"].append(file_path.name)
        
        logging.info(f"\n[ğŸ“Š] æ¸…æ´—çµ±è¨ˆ:")
        logging.info(f"    - æˆåŠŸ: {results['success']}")
        logging.info(f"    - å¤±æ•—: {results['failed']}")
        
        return results

def main():
    """ä¸»è¦åŸ·è¡Œå‡½æ•¸"""
    setup_logging()
    logging.info("=== ä¸Šæ«ƒæ­·å²è³‡æ–™æ‰¹é‡ä¸‹è¼‰ + æ¸…æ´—ç³»çµ± ===")
    
    config = load_config()
    
    # ç¢ºèªåŸ·è¡Œ
    print(f"\nç›®æ¨™æ—¥æœŸç¯„åœ: {START_DATE.strftime('%Y-%m-%d')} ~ {END_DATE.strftime('%Y-%m-%d')}")
    print(f"é ä¼°äº¤æ˜“æ—¥: ~{len([d for d in pd.date_range(START_DATE, END_DATE) if d.weekday() < 5])} å¤©")
    print(f"è³‡æ–™é …ç›®: {len(config['download_items'])} ç¨®")
    print(f"é ä¼°ç¸½æ™‚é–“: 4-6 å°æ™‚")
    print(f"è¼¸å‡ºç›®éŒ„: {RAW_DIR} (åŸå§‹), {CLEAN_DIR} (æ¸…æ´—)")
    
    response = input("\nâš ï¸  é€™æ˜¯é•·æ™‚é–“åŸ·è¡Œä»»å‹™ï¼Œæ˜¯å¦ç¢ºå®šé–‹å§‹? (y/N): ").strip().lower()
    if response != 'y':
        print("å–æ¶ˆåŸ·è¡Œ")
        return
    
    start_time = datetime.now()
    performance_monitor = PerformanceMonitor()
    
    try:
        # æ­¥é©Ÿ 1: æ‰¹é‡ä¸‹è¼‰æ­·å²è³‡æ–™
        logging.info("\n=== æ­¥é©Ÿ 1: æ‰¹é‡ä¸‹è¼‰æ­·å²è³‡æ–™ ===")
        downloader = OTCHistoricalDownloader(config)
        
        with performance_monitor.measure_time("ç¸½ä¸‹è¼‰æ™‚é–“"):
            download_results = downloader.download_all_historical()
        
        # æ­¥é©Ÿ 2: æ¸…æ´—æ‰€æœ‰ä¸‹è¼‰çš„è³‡æ–™
        logging.info("\n=== æ­¥é©Ÿ 2: æ¸…æ´—æ­·å²è³‡æ–™ ===")
        cleaner = OTCDataCleaner(config)
        
        with performance_monitor.measure_time("ç¸½æ¸…æ´—æ™‚é–“"):
            clean_results = cleaner.clean_all_historical_files()
        
        # å®Œæˆå ±å‘Š
        end_time = datetime.now()
        duration = end_time - start_time
        
        logging.info(f"\n[ğŸ‰] æ‰€æœ‰ç¨‹åºå®Œæˆ!")
        logging.info(f"[â±] ç¸½åŸ·è¡Œæ™‚é–“: {duration}")
        logging.info(f"[ğŸ“Š] ä¸‹è¼‰çµæœ: æˆåŠŸ {download_results['success']}, å¤±æ•— {download_results['failed']}, è·³é {download_results['skipped']}")
        logging.info(f"[ğŸ“Š] æ¸…æ´—çµæœ: æˆåŠŸ {clean_results['success']}, å¤±æ•— {clean_results['failed']}")
        logging.info(f"[ğŸ“] åŸå§‹è³‡æ–™: {RAW_DIR}")
        logging.info(f"[ğŸ“] æ¸…æ´—è³‡æ–™: {CLEAN_DIR}")
        
        # ä¿å­˜åŸ·è¡Œå ±å‘Š
        performance_report_path = LOG_DIR / f"historical_performance_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        performance_monitor.save_report(performance_report_path)
        
    except KeyboardInterrupt:
        logging.warning("\n[â¹] ä½¿ç”¨è€…ä¸­æ–·åŸ·è¡Œ")
    except Exception as e:
        logging.error(f"\n[âŒ] åŸ·è¡Œéç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
        logging.error(traceback.format_exc())
        raise

def load_config() -> Dict[str, Any]:
    """è¼‰å…¥è¨­å®šæª”"""
    config_file = BASE_DIR / "otc_config.json"
    
    if config_file.exists():
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
            logging.info(f"å·²è¼‰å…¥è¨­å®šæª”: {config_file}")
            return config
        except Exception as e:
            logging.warning(f"è¨­å®šæª”è¼‰å…¥å¤±æ•—ï¼Œä½¿ç”¨é è¨­è¨­å®š: {e}")
    else:
        logging.info("æœªæ‰¾åˆ°è¨­å®šæª”ï¼Œä½¿ç”¨é è¨­è¨­å®š")
    
    return DEFAULT_CONFIG

if __name__ == "__main__":
    main(), na=False)]
        
        numeric_cols = [col for col in clean_df.columns if col not in ["stock_id", "name"]]
        for col in numeric_cols:
            clean_df[col] = self.clean_numeric_column(clean_df[col])
        
        return clean_df.sort_values("stock_id").reset_index(drop=True)
    
    def _clean_sec_trading(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ¸…æ´—åˆ¸å•†ç‡Ÿæ¥­é¡çµ±è¨ˆè³‡æ–™"""
        cols = list(df.columns)
        
        if len(cols) < 5:
            logging.error("  æ¬„ä½æ•¸é‡ä¸è¶³")
            return None
        
        left_cols = cols[:5]
        column_mapping = {
            left_cols[0]: "rank",
            left_cols[1]: "prev_rank",
            left_cols[2]: "broker",
            left_cols[3]: "name",
            left_cols[4]: "amount_thousands"
        }
        
        clean_df = df[left_cols].rename(columns=column_mapping).copy()
        
        numeric_cols = ["rank", "prev_rank", "amount_thousands"]
        for col in numeric_cols:
            if col in clean_df.columns:
                clean_df[col] = self.clean_numeric_column(clean_df[col])
        
        return clean_df.sort_values("broker").reset_index(drop=True)
    
    def _clean_day_trading(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ¸…æ´—ç¾è‚¡ç•¶æ²–äº¤æ˜“çµ±è¨ˆè³‡æ–™"""
        cols = list(df.columns)
        
        # éæ¿¾çµ±è¨ˆè¡Œ
        if len(df) > 0:
            first_col = df.iloc[:, 0].astype(str)
            mask = ~first_col.str.contains('å…±è¨ˆ|åˆè¨ˆ|ç¸½è¨ˆ|çµ±è¨ˆ|èªªæ˜|è¨»[ï¼š:]', na=False, regex=True)
            df = df[mask].copy()
        
        code_col = None
        name_col = None
        
        for pattern in ["è­‰åˆ¸ä»£è™Ÿ", "ä»£è™Ÿ", "è‚¡ç¥¨ä»£è™Ÿ", "ä»£ç¢¼"]:
            for c in cols:
                if pattern in c:
                    code_col = c
                    break
            if code_col:
                break
        
        for pattern in ["è­‰åˆ¸åç¨±", "åç¨±", "è‚¡ç¥¨åç¨±"]:
            for c in cols:
                if pattern in c:
                    name_col = c
                    break
            if name_col:
                break
        
        if not code_col and len(cols) >= 2:
            if df.iloc[0, 0] and str(df.iloc[0, 0]).strip().replace(' ', ''):
                if any(char.isdigit() for char in str(df.iloc[0, 0])):
                    code_col = cols[0]
                    name_col = cols[1] if len(cols) > 1 else None
        
        if not code_col:
            logging.error(f"  ç„¡æ³•è­˜åˆ¥è­‰åˆ¸ä»£è™Ÿæ¬„ä½ï¼Œæ¬„ä½åˆ—è¡¨ï¼š{cols}")
            return None
        
        column_mapping = {code_col: "stock_id"}
        if name_col:
            column_mapping[name_col] = "name"
        
        dt_fields = {
            "æš«åœ": "flag",
            "æˆäº¤è‚¡æ•¸": "dt_volume",
            "è²·é€²æˆäº¤é‡‘é¡": "dt_buy_amount",
            "è³£å‡ºæˆäº¤é‡‘é¡": "dt_sell_amount",
            "è²·è³£ç¸½é¡": "dt_total_amount",
            "ç•¶æ²–ç‡": "dt_rate"
        }
        
        for pattern, new_name in dt_fields.items():
            matching_col = next((c for c in cols if pattern in c), None)
            if matching_col:
                column_mapping[matching_col] = new_name
        
        available_cols = [col for col in column_mapping.keys() if col in df.columns]
        clean_df = df[available_cols].rename(columns=column_mapping).copy()
        
        clean_df["stock_id"] = self.extract_stock_id(clean_df["stock_id"])
        clean_df = clean_df.dropna(subset=["stock_id"])
        clean_df = clean_df[clean_df["stock_id"].str.match(r'^\d{4}
    
    def clean_all_historical_files(self) -> Dict[str, int]:
        """æ¸…æ´—æ‰€æœ‰æ­·å²æª”æ¡ˆ"""
        self.ensure_dir(CLEAN_DIR)
        results = {"success": 0, "failed": 0, "failed_files": []}
        
        # å–å¾—æ‰€æœ‰æª”æ¡ˆï¼ŒæŒ‰æ—¥æœŸåˆ†çµ„
        files_by_date = self.get_all_raw_files_by_date()
        total_dates = len(files_by_date)
        
        logging.info(f"\n=== é–‹å§‹æ¸…æ´—æ­·å²è³‡æ–™ ===")
        logging.info(f"æ‰¾åˆ° {total_dates} å€‹æ—¥æœŸçš„è³‡æ–™")
        
        with self.performance_monitor.measure_time("æ­·å²è³‡æ–™æ¸…æ´—"):
            for date_idx, (date_str, file_list) in enumerate(sorted(files_by_date.items()), 1):
                logging.info(f"\nâ”€â”€ æ¸…æ´—æ—¥æœŸ {date_str} ({date_idx}/{total_dates}) â”€â”€")
                
                for file_path in file_list:
                    try:
                        if self.clean_single_file(file_path):
                            results["success"] += 1
                        else:
                            results["failed"] += 1
                            results["failed_files"].append(file_path.name)
                    except Exception as e:
                        logging.error(f"æ¸…ç†æª”æ¡ˆ {file_path.name} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                        results["failed"] += 1
                        results["failed_files"].append(file_path.name)
        
        logging.info(f"\n[ğŸ“Š] æ¸…æ´—çµ±è¨ˆ:")
        logging.info(f"    - æˆåŠŸ: {results['success']}")
        logging.info(f"    - å¤±æ•—: {results['failed']}")
        
        return results

def main():
    """ä¸»è¦åŸ·è¡Œå‡½æ•¸"""
    setup_logging()
    logging.info("=== ä¸Šæ«ƒæ­·å²è³‡æ–™æ‰¹é‡ä¸‹è¼‰ + æ¸…æ´—ç³»çµ± ===")
    
    config = load_config()
    
    # ç¢ºèªåŸ·è¡Œ
    print(f"\nç›®æ¨™æ—¥æœŸç¯„åœ: {START_DATE.strftime('%Y-%m-%d')} ~ {END_DATE.strftime('%Y-%m-%d')}")
    print(f"é ä¼°äº¤æ˜“æ—¥: ~{len([d for d in pd.date_range(START_DATE, END_DATE) if d.weekday() < 5])} å¤©")
    print(f"è³‡æ–™é …ç›®: {len(config['download_items'])} ç¨®")
    print(f"é ä¼°ç¸½æ™‚é–“: 4-6 å°æ™‚")
    print(f"è¼¸å‡ºç›®éŒ„: {RAW_DIR} (åŸå§‹), {CLEAN_DIR} (æ¸…æ´—)")
    
    response = input("\nâš ï¸  é€™æ˜¯é•·æ™‚é–“åŸ·è¡Œä»»å‹™ï¼Œæ˜¯å¦ç¢ºå®šé–‹å§‹? (y/N): ").strip().lower()
    if response != 'y':
        print("å–æ¶ˆåŸ·è¡Œ")
        return
    
    start_time = datetime.now()
    performance_monitor = PerformanceMonitor()
    
    try:
        # æ­¥é©Ÿ 1: æ‰¹é‡ä¸‹è¼‰æ­·å²è³‡æ–™
        logging.info("\n=== æ­¥é©Ÿ 1: æ‰¹é‡ä¸‹è¼‰æ­·å²è³‡æ–™ ===")
        downloader = OTCHistoricalDownloader(config)
        
        with performance_monitor.measure_time("ç¸½ä¸‹è¼‰æ™‚é–“"):
            download_results = downloader.download_all_historical()
        
        # æ­¥é©Ÿ 2: æ¸…æ´—æ‰€æœ‰ä¸‹è¼‰çš„è³‡æ–™
        logging.info("\n=== æ­¥é©Ÿ 2: æ¸…æ´—æ­·å²è³‡æ–™ ===")
        cleaner = OTCDataCleaner(config)
        
        with performance_monitor.measure_time("ç¸½æ¸…æ´—æ™‚é–“"):
            clean_results = cleaner.clean_all_historical_files()
        
        # å®Œæˆå ±å‘Š
        end_time = datetime.now()
        duration = end_time - start_time
        
        logging.info(f"\n[ğŸ‰] æ‰€æœ‰ç¨‹åºå®Œæˆ!")
        logging.info(f"[â±] ç¸½åŸ·è¡Œæ™‚é–“: {duration}")
        logging.info(f"[ğŸ“Š] ä¸‹è¼‰çµæœ: æˆåŠŸ {download_results['success']}, å¤±æ•— {download_results['failed']}, è·³é {download_results['skipped']}")
        logging.info(f"[ğŸ“Š] æ¸…æ´—çµæœ: æˆåŠŸ {clean_results['success']}, å¤±æ•— {clean_results['failed']}")
        logging.info(f"[ğŸ“] åŸå§‹è³‡æ–™: {RAW_DIR}")
        logging.info(f"[ğŸ“] æ¸…æ´—è³‡æ–™: {CLEAN_DIR}")
        
        # ä¿å­˜åŸ·è¡Œå ±å‘Š
        performance_report_path = LOG_DIR / f"historical_performance_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        performance_monitor.save_report(performance_report_path)
        
    except KeyboardInterrupt:
        logging.warning("\n[â¹] ä½¿ç”¨è€…ä¸­æ–·åŸ·è¡Œ")
    except Exception as e:
        logging.error(f"\n[âŒ] åŸ·è¡Œéç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
        logging.error(traceback.format_exc())
        raise

def load_config() -> Dict[str, Any]:
    """è¼‰å…¥è¨­å®šæª”"""
    config_file = BASE_DIR / "otc_config.json"
    
    if config_file.exists():
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
            logging.info(f"å·²è¼‰å…¥è¨­å®šæª”: {config_file}")
            return config
        except Exception as e:
            logging.warning(f"è¨­å®šæª”è¼‰å…¥å¤±æ•—ï¼Œä½¿ç”¨é è¨­è¨­å®š: {e}")
    else:
        logging.info("æœªæ‰¾åˆ°è¨­å®šæª”ï¼Œä½¿ç”¨é è¨­è¨­å®š")
    
    return DEFAULT_CONFIG

if __name__ == "__main__":
    main(), na=False)]
        
        numeric_cols = [col for col in clean_df.columns if col not in ["stock_id", "name", "flag"]]
        for col in numeric_cols:
            clean_df[col] = self.clean_numeric_column(clean_df[col])
        
        return clean_df.sort_values("stock_id").reset_index(drop=True)
    
    def _clean_highlight(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ¸…æ´—èè³‡èåˆ¸é¤˜é¡æ¦‚æ³è³‡æ–™"""
        cols = list(df.columns)
        
        rank_col = next((c for c in cols if "æ’å" in c), None)
        code_col = next((c for c in cols if c == "ä»£è™Ÿ"), None)
        name_col = next((c for c in cols if c == "åç¨±"), None)
        
        if not all([rank_col, code_col, name_col]):
            logging.error("  ç¼ºå°‘å¿…è¦æ¬„ä½")
            return None
        
        column_mapping = {
            rank_col: "rank",
            code_col: "stock_id",
            name_col: "name"
        }
        
        margin_fields = {
            "æœˆå‡èè³‡é¤˜é¡": "hg_margin_balance",
            "æœˆå‡èåˆ¸é¤˜é¡": "hg_short_balance",
            "åˆ¸è³‡æ¯”": "hg_ratio"
        }
        
        for pattern, new_name in margin_fields.items():
            matching_col = next((c for c in cols if pattern in c), None)
            if matching_col:
                column_mapping[matching_col] = new_name
        
        available_cols = [col for col in column_mapping.keys() if col in df.columns]
        clean_df = df[available_cols].rename(columns=column_mapping).copy()
        
        clean_df["stock_id"] = self.extract_stock_id(clean_df["stock_id"])
        clean_df = clean_df.dropna(subset=["stock_id"])
        clean_df = clean_df[clean_df["stock_id"].str.match(r'^\d{4}
    
    def clean_all_historical_files(self) -> Dict[str, int]:
        """æ¸…æ´—æ‰€æœ‰æ­·å²æª”æ¡ˆ"""
        self.ensure_dir(CLEAN_DIR)
        results = {"success": 0, "failed": 0, "failed_files": []}
        
        # å–å¾—æ‰€æœ‰æª”æ¡ˆï¼ŒæŒ‰æ—¥æœŸåˆ†çµ„
        files_by_date = self.get_all_raw_files_by_date()
        total_dates = len(files_by_date)
        
        logging.info(f"\n=== é–‹å§‹æ¸…æ´—æ­·å²è³‡æ–™ ===")
        logging.info(f"æ‰¾åˆ° {total_dates} å€‹æ—¥æœŸçš„è³‡æ–™")
        
        with self.performance_monitor.measure_time("æ­·å²è³‡æ–™æ¸…æ´—"):
            for date_idx, (date_str, file_list) in enumerate(sorted(files_by_date.items()), 1):
                logging.info(f"\nâ”€â”€ æ¸…æ´—æ—¥æœŸ {date_str} ({date_idx}/{total_dates}) â”€â”€")
                
                for file_path in file_list:
                    try:
                        if self.clean_single_file(file_path):
                            results["success"] += 1
                        else:
                            results["failed"] += 1
                            results["failed_files"].append(file_path.name)
                    except Exception as e:
                        logging.error(f"æ¸…ç†æª”æ¡ˆ {file_path.name} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                        results["failed"] += 1
                        results["failed_files"].append(file_path.name)
        
        logging.info(f"\n[ğŸ“Š] æ¸…æ´—çµ±è¨ˆ:")
        logging.info(f"    - æˆåŠŸ: {results['success']}")
        logging.info(f"    - å¤±æ•—: {results['failed']}")
        
        return results

def main():
    """ä¸»è¦åŸ·è¡Œå‡½æ•¸"""
    setup_logging()
    logging.info("=== ä¸Šæ«ƒæ­·å²è³‡æ–™æ‰¹é‡ä¸‹è¼‰ + æ¸…æ´—ç³»çµ± ===")
    
    config = load_config()
    
    # ç¢ºèªåŸ·è¡Œ
    print(f"\nç›®æ¨™æ—¥æœŸç¯„åœ: {START_DATE.strftime('%Y-%m-%d')} ~ {END_DATE.strftime('%Y-%m-%d')}")
    print(f"é ä¼°äº¤æ˜“æ—¥: ~{len([d for d in pd.date_range(START_DATE, END_DATE) if d.weekday() < 5])} å¤©")
    print(f"è³‡æ–™é …ç›®: {len(config['download_items'])} ç¨®")
    print(f"é ä¼°ç¸½æ™‚é–“: 4-6 å°æ™‚")
    print(f"è¼¸å‡ºç›®éŒ„: {RAW_DIR} (åŸå§‹), {CLEAN_DIR} (æ¸…æ´—)")
    
    response = input("\nâš ï¸  é€™æ˜¯é•·æ™‚é–“åŸ·è¡Œä»»å‹™ï¼Œæ˜¯å¦ç¢ºå®šé–‹å§‹? (y/N): ").strip().lower()
    if response != 'y':
        print("å–æ¶ˆåŸ·è¡Œ")
        return
    
    start_time = datetime.now()
    performance_monitor = PerformanceMonitor()
    
    try:
        # æ­¥é©Ÿ 1: æ‰¹é‡ä¸‹è¼‰æ­·å²è³‡æ–™
        logging.info("\n=== æ­¥é©Ÿ 1: æ‰¹é‡ä¸‹è¼‰æ­·å²è³‡æ–™ ===")
        downloader = OTCHistoricalDownloader(config)
        
        with performance_monitor.measure_time("ç¸½ä¸‹è¼‰æ™‚é–“"):
            download_results = downloader.download_all_historical()
        
        # æ­¥é©Ÿ 2: æ¸…æ´—æ‰€æœ‰ä¸‹è¼‰çš„è³‡æ–™
        logging.info("\n=== æ­¥é©Ÿ 2: æ¸…æ´—æ­·å²è³‡æ–™ ===")
        cleaner = OTCDataCleaner(config)
        
        with performance_monitor.measure_time("ç¸½æ¸…æ´—æ™‚é–“"):
            clean_results = cleaner.clean_all_historical_files()
        
        # å®Œæˆå ±å‘Š
        end_time = datetime.now()
        duration = end_time - start_time
        
        logging.info(f"\n[ğŸ‰] æ‰€æœ‰ç¨‹åºå®Œæˆ!")
        logging.info(f"[â±] ç¸½åŸ·è¡Œæ™‚é–“: {duration}")
        logging.info(f"[ğŸ“Š] ä¸‹è¼‰çµæœ: æˆåŠŸ {download_results['success']}, å¤±æ•— {download_results['failed']}, è·³é {download_results['skipped']}")
        logging.info(f"[ğŸ“Š] æ¸…æ´—çµæœ: æˆåŠŸ {clean_results['success']}, å¤±æ•— {clean_results['failed']}")
        logging.info(f"[ğŸ“] åŸå§‹è³‡æ–™: {RAW_DIR}")
        logging.info(f"[ğŸ“] æ¸…æ´—è³‡æ–™: {CLEAN_DIR}")
        
        # ä¿å­˜åŸ·è¡Œå ±å‘Š
        performance_report_path = LOG_DIR / f"historical_performance_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        performance_monitor.save_report(performance_report_path)
        
    except KeyboardInterrupt:
        logging.warning("\n[â¹] ä½¿ç”¨è€…ä¸­æ–·åŸ·è¡Œ")
    except Exception as e:
        logging.error(f"\n[âŒ] åŸ·è¡Œéç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
        logging.error(traceback.format_exc())
        raise

def load_config() -> Dict[str, Any]:
    """è¼‰å…¥è¨­å®šæª”"""
    config_file = BASE_DIR / "otc_config.json"
    
    if config_file.exists():
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
            logging.info(f"å·²è¼‰å…¥è¨­å®šæª”: {config_file}")
            return config
        except Exception as e:
            logging.warning(f"è¨­å®šæª”è¼‰å…¥å¤±æ•—ï¼Œä½¿ç”¨é è¨­è¨­å®š: {e}")
    else:
        logging.info("æœªæ‰¾åˆ°è¨­å®šæª”ï¼Œä½¿ç”¨é è¨­è¨­å®š")
    
    return DEFAULT_CONFIG

if __name__ == "__main__":
    main(), na=False)]
        
        numeric_cols = [col for col in clean_df.columns if col not in ["stock_id", "name"]]
        for col in numeric_cols:
            clean_df[col] = self.clean_numeric_column(clean_df[col])
        
        return clean_df.sort_values("stock_id").reset_index(drop=True)
    
    def _clean_sbl(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ¸…æ´—ä¿¡ç”¨é¡åº¦ç¸½é‡ç®¡åˆ¶é¤˜é¡è³‡æ–™"""
        cols = list(df.columns)
        
        code_col = next((c for c in cols if "è‚¡ç¥¨ä»£è™Ÿ" in c), None)
        name_col = next((c for c in cols if "è‚¡ç¥¨åç¨±" in c), None)
        
        if not all([code_col, name_col]):
            logging.error("  ç¼ºå°‘å¿…è¦æ¬„ä½")
            return None
        
        column_mapping = {
            code_col: "stock_id",
            name_col: "name"
        }
        
        sbl_fields = {
            "èåˆ¸å‰æ—¥é¤˜é¡": "owz_short_prev_balance",
            "èåˆ¸è³£å‡º": "owz_short_sell",
            "èåˆ¸è²·é€²": "owz_short_buy",
            "èåˆ¸ç¾åˆ¸": "owz_short_spot",
            "èåˆ¸ç•¶æ—¥é¤˜é¡": "owz_short_today_balance",
            "èåˆ¸é™é¡": "owz_short_limit",
            "å€Ÿåˆ¸å‰æ—¥é¤˜é¡": "owz_borrow_prev_balance",
            "å€Ÿåˆ¸ç•¶æ—¥è³£å‡º": "owz_borrow_sell",
            "å€Ÿåˆ¸ç•¶æ—¥é‚„åˆ¸": "owz_borrow_return",
            "å€Ÿåˆ¸ç•¶æ—¥èª¿æ•´æ•¸é¡": "owz_borrow_adj",
            "å€Ÿåˆ¸ç•¶æ—¥é¤˜é¡": "owz_borrow_today_balance",
            "å€Ÿåˆ¸æ¬¡ä¸€ç‡Ÿæ¥­æ—¥å¯å€Ÿåˆ¸è³£å‡ºé™é¡": "owz_borrow_next_limit",
            "å‚™è¨»": "remark"
        }
        
        for pattern, new_name in sbl_fields.items():
            matching_col = next((c for c in cols if pattern in c), None)
            if matching_col:
                column_mapping[matching_col] = new_name
        
        available_cols = [col for col in column_mapping.keys() if col in df.columns]
        clean_df = df[available_cols].rename(columns=column_mapping).copy()
        
        clean_df["stock_id"] = self.extract_stock_id(clean_df["stock_id"])
        clean_df = clean_df.dropna(subset=["stock_id"])
        clean_df = clean_df[clean_df["stock_id"].str.match(r'^\d{4}
    
    def clean_all_historical_files(self) -> Dict[str, int]:
        """æ¸…æ´—æ‰€æœ‰æ­·å²æª”æ¡ˆ"""
        self.ensure_dir(CLEAN_DIR)
        results = {"success": 0, "failed": 0, "failed_files": []}
        
        # å–å¾—æ‰€æœ‰æª”æ¡ˆï¼ŒæŒ‰æ—¥æœŸåˆ†çµ„
        files_by_date = self.get_all_raw_files_by_date()
        total_dates = len(files_by_date)
        
        logging.info(f"\n=== é–‹å§‹æ¸…æ´—æ­·å²è³‡æ–™ ===")
        logging.info(f"æ‰¾åˆ° {total_dates} å€‹æ—¥æœŸçš„è³‡æ–™")
        
        with self.performance_monitor.measure_time("æ­·å²è³‡æ–™æ¸…æ´—"):
            for date_idx, (date_str, file_list) in enumerate(sorted(files_by_date.items()), 1):
                logging.info(f"\nâ”€â”€ æ¸…æ´—æ—¥æœŸ {date_str} ({date_idx}/{total_dates}) â”€â”€")
                
                for file_path in file_list:
                    try:
                        if self.clean_single_file(file_path):
                            results["success"] += 1
                        else:
                            results["failed"] += 1
                            results["failed_files"].append(file_path.name)
                    except Exception as e:
                        logging.error(f"æ¸…ç†æª”æ¡ˆ {file_path.name} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                        results["failed"] += 1
                        results["failed_files"].append(file_path.name)
        
        logging.info(f"\n[ğŸ“Š] æ¸…æ´—çµ±è¨ˆ:")
        logging.info(f"    - æˆåŠŸ: {results['success']}")
        logging.info(f"    - å¤±æ•—: {results['failed']}")
        
        return results

def main():
    """ä¸»è¦åŸ·è¡Œå‡½æ•¸"""
    setup_logging()
    logging.info("=== ä¸Šæ«ƒæ­·å²è³‡æ–™æ‰¹é‡ä¸‹è¼‰ + æ¸…æ´—ç³»çµ± ===")
    
    config = load_config()
    
    # ç¢ºèªåŸ·è¡Œ
    print(f"\nç›®æ¨™æ—¥æœŸç¯„åœ: {START_DATE.strftime('%Y-%m-%d')} ~ {END_DATE.strftime('%Y-%m-%d')}")
    print(f"é ä¼°äº¤æ˜“æ—¥: ~{len([d for d in pd.date_range(START_DATE, END_DATE) if d.weekday() < 5])} å¤©")
    print(f"è³‡æ–™é …ç›®: {len(config['download_items'])} ç¨®")
    print(f"é ä¼°ç¸½æ™‚é–“: 4-6 å°æ™‚")
    print(f"è¼¸å‡ºç›®éŒ„: {RAW_DIR} (åŸå§‹), {CLEAN_DIR} (æ¸…æ´—)")
    
    response = input("\nâš ï¸  é€™æ˜¯é•·æ™‚é–“åŸ·è¡Œä»»å‹™ï¼Œæ˜¯å¦ç¢ºå®šé–‹å§‹? (y/N): ").strip().lower()
    if response != 'y':
        print("å–æ¶ˆåŸ·è¡Œ")
        return
    
    start_time = datetime.now()
    performance_monitor = PerformanceMonitor()
    
    try:
        # æ­¥é©Ÿ 1: æ‰¹é‡ä¸‹è¼‰æ­·å²è³‡æ–™
        logging.info("\n=== æ­¥é©Ÿ 1: æ‰¹é‡ä¸‹è¼‰æ­·å²è³‡æ–™ ===")
        downloader = OTCHistoricalDownloader(config)
        
        with performance_monitor.measure_time("ç¸½ä¸‹è¼‰æ™‚é–“"):
            download_results = downloader.download_all_historical()
        
        # æ­¥é©Ÿ 2: æ¸…æ´—æ‰€æœ‰ä¸‹è¼‰çš„è³‡æ–™
        logging.info("\n=== æ­¥é©Ÿ 2: æ¸…æ´—æ­·å²è³‡æ–™ ===")
        cleaner = OTCDataCleaner(config)
        
        with performance_monitor.measure_time("ç¸½æ¸…æ´—æ™‚é–“"):
            clean_results = cleaner.clean_all_historical_files()
        
        # å®Œæˆå ±å‘Š
        end_time = datetime.now()
        duration = end_time - start_time
        
        logging.info(f"\n[ğŸ‰] æ‰€æœ‰ç¨‹åºå®Œæˆ!")
        logging.info(f"[â±] ç¸½åŸ·è¡Œæ™‚é–“: {duration}")
        logging.info(f"[ğŸ“Š] ä¸‹è¼‰çµæœ: æˆåŠŸ {download_results['success']}, å¤±æ•— {download_results['failed']}, è·³é {download_results['skipped']}")
        logging.info(f"[ğŸ“Š] æ¸…æ´—çµæœ: æˆåŠŸ {clean_results['success']}, å¤±æ•— {clean_results['failed']}")
        logging.info(f"[ğŸ“] åŸå§‹è³‡æ–™: {RAW_DIR}")
        logging.info(f"[ğŸ“] æ¸…æ´—è³‡æ–™: {CLEAN_DIR}")
        
        # ä¿å­˜åŸ·è¡Œå ±å‘Š
        performance_report_path = LOG_DIR / f"historical_performance_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        performance_monitor.save_report(performance_report_path)
        
    except KeyboardInterrupt:
        logging.warning("\n[â¹] ä½¿ç”¨è€…ä¸­æ–·åŸ·è¡Œ")
    except Exception as e:
        logging.error(f"\n[âŒ] åŸ·è¡Œéç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
        logging.error(traceback.format_exc())
        raise

def load_config() -> Dict[str, Any]:
    """è¼‰å…¥è¨­å®šæª”"""
    config_file = BASE_DIR / "otc_config.json"
    
    if config_file.exists():
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
            logging.info(f"å·²è¼‰å…¥è¨­å®šæª”: {config_file}")
            return config
        except Exception as e:
            logging.warning(f"è¨­å®šæª”è¼‰å…¥å¤±æ•—ï¼Œä½¿ç”¨é è¨­è¨­å®š: {e}")
    else:
        logging.info("æœªæ‰¾åˆ°è¨­å®šæª”ï¼Œä½¿ç”¨é è¨­è¨­å®š")
    
    return DEFAULT_CONFIG

if __name__ == "__main__":
    main(), na=False)]
        
        numeric_cols = [col for col in clean_df.columns if col not in ["stock_id", "name", "remark"]]
        for col in numeric_cols:
            clean_df[col] = self.clean_numeric_column(clean_df[col])
        
        return clean_df.sort_values("stock_id").reset_index(drop=True)
    
    def _clean_margin_transactions(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ¸…æ´—èè³‡èåˆ¸é¤˜é¡è³‡æ–™"""
        cols = list(df.columns)
        
        # éæ¿¾åªä¿ç•™æ•¸å­—é–‹é ­çš„è¡Œ
        if len(df) > 0:
            first_col_str = df.iloc[:, 0].astype(str)
            mask = first_col_str.str.match(r'^\d', na=False)
            df = df[mask].copy()
        
        code_col = None
        name_col = None
        
        for pattern in ["ä»£è™Ÿ", "ä»£ç¢¼", "è‚¡ç¥¨ä»£è™Ÿ", "è­‰åˆ¸ä»£è™Ÿ"]:
            for c in cols:
                if pattern in c:
                    code_col = c
                    break
            if code_col:
                break
        
        for pattern in ["åç¨±", "è‚¡ç¥¨åç¨±", "è­‰åˆ¸åç¨±"]:
            for c in cols:
                if pattern in c:
                    name_col = c
                    break
            if name_col:
                break
        
        if not code_col and len(cols) >= 2:
            if df.iloc[0, 0] and re.match(r'^\d+', str(df.iloc[0, 0])):
                code_col = cols[0]
                name_col = cols[1]
        
        if not code_col:
            logging.error(f"  ç„¡æ³•è­˜åˆ¥è­‰åˆ¸ä»£è™Ÿæ¬„ä½ï¼Œæ¬„ä½åˆ—è¡¨ï¼š{cols}")
            return None
        
        column_mapping = {code_col: "stock_id"}
        if name_col:
            column_mapping[name_col] = "name"
        
        mt_fields = {
            "å‰è³‡é¤˜é¡": "mt_prev_balance",
            "è³‡è²·": "mt_buy",
            "è³‡è³£": "mt_sell",
            "ç¾å„Ÿ": "mt_pay",
            "è³‡é¤˜é¡": "mt_balance",
            "è³‡å±¬è­‰é‡‘": "mt_cash",
            "è³‡ä½¿ç”¨ç‡": "mt_usage_rate",
            "è³‡é™é¡": "mt_limit",
            "å‰åˆ¸é¤˜é¡": "st_prev_balance",
            "åˆ¸è³£": "st_sell",
            "åˆ¸è²·": "st_buy",
            "åˆ¸å„Ÿ": "st_pay",
            "åˆ¸é¤˜é¡": "st_balance",
            "åˆ¸å±¬è­‰é‡‘": "st_cash",
            "åˆ¸ä½¿ç”¨ç‡": "st_usage_rate",
            "åˆ¸é™é¡": "st_limit",
            "è³‡åˆ¸ç›¸æŠµ": "mt_st_offset",
            "å‚™è¨»": "remark"
        }
        
        for pattern, new_name in mt_fields.items():
            matching_col = next((c for c in cols if pattern in c), None)
            if matching_col:
                column_mapping[matching_col] = new_name
        
        available_cols = [col for col in column_mapping.keys() if col in df.columns]
        clean_df = df[available_cols].rename(columns=column_mapping).copy()
        
        clean_df["stock_id"] = self.extract_stock_id(clean_df["stock_id"])
        clean_df = clean_df.dropna(subset=["stock_id"])
        clean_df = clean_df[clean_df["stock_id"].str.match(r'^\d{4}
    
    def clean_all_historical_files(self) -> Dict[str, int]:
        """æ¸…æ´—æ‰€æœ‰æ­·å²æª”æ¡ˆ"""
        self.ensure_dir(CLEAN_DIR)
        results = {"success": 0, "failed": 0, "failed_files": []}
        
        # å–å¾—æ‰€æœ‰æª”æ¡ˆï¼ŒæŒ‰æ—¥æœŸåˆ†çµ„
        files_by_date = self.get_all_raw_files_by_date()
        total_dates = len(files_by_date)
        
        logging.info(f"\n=== é–‹å§‹æ¸…æ´—æ­·å²è³‡æ–™ ===")
        logging.info(f"æ‰¾åˆ° {total_dates} å€‹æ—¥æœŸçš„è³‡æ–™")
        
        with self.performance_monitor.measure_time("æ­·å²è³‡æ–™æ¸…æ´—"):
            for date_idx, (date_str, file_list) in enumerate(sorted(files_by_date.items()), 1):
                logging.info(f"\nâ”€â”€ æ¸…æ´—æ—¥æœŸ {date_str} ({date_idx}/{total_dates}) â”€â”€")
                
                for file_path in file_list:
                    try:
                        if self.clean_single_file(file_path):
                            results["success"] += 1
                        else:
                            results["failed"] += 1
                            results["failed_files"].append(file_path.name)
                    except Exception as e:
                        logging.error(f"æ¸…ç†æª”æ¡ˆ {file_path.name} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                        results["failed"] += 1
                        results["failed_files"].append(file_path.name)
        
        logging.info(f"\n[ğŸ“Š] æ¸…æ´—çµ±è¨ˆ:")
        logging.info(f"    - æˆåŠŸ: {results['success']}")
        logging.info(f"    - å¤±æ•—: {results['failed']}")
        
        return results

def main():
    """ä¸»è¦åŸ·è¡Œå‡½æ•¸"""
    setup_logging()
    logging.info("=== ä¸Šæ«ƒæ­·å²è³‡æ–™æ‰¹é‡ä¸‹è¼‰ + æ¸…æ´—ç³»çµ± ===")
    
    config = load_config()
    
    # ç¢ºèªåŸ·è¡Œ
    print(f"\nç›®æ¨™æ—¥æœŸç¯„åœ: {START_DATE.strftime('%Y-%m-%d')} ~ {END_DATE.strftime('%Y-%m-%d')}")
    print(f"é ä¼°äº¤æ˜“æ—¥: ~{len([d for d in pd.date_range(START_DATE, END_DATE) if d.weekday() < 5])} å¤©")
    print(f"è³‡æ–™é …ç›®: {len(config['download_items'])} ç¨®")
    print(f"é ä¼°ç¸½æ™‚é–“: 4-6 å°æ™‚")
    print(f"è¼¸å‡ºç›®éŒ„: {RAW_DIR} (åŸå§‹), {CLEAN_DIR} (æ¸…æ´—)")
    
    response = input("\nâš ï¸  é€™æ˜¯é•·æ™‚é–“åŸ·è¡Œä»»å‹™ï¼Œæ˜¯å¦ç¢ºå®šé–‹å§‹? (y/N): ").strip().lower()
    if response != 'y':
        print("å–æ¶ˆåŸ·è¡Œ")
        return
    
    start_time = datetime.now()
    performance_monitor = PerformanceMonitor()
    
    try:
        # æ­¥é©Ÿ 1: æ‰¹é‡ä¸‹è¼‰æ­·å²è³‡æ–™
        logging.info("\n=== æ­¥é©Ÿ 1: æ‰¹é‡ä¸‹è¼‰æ­·å²è³‡æ–™ ===")
        downloader = OTCHistoricalDownloader(config)
        
        with performance_monitor.measure_time("ç¸½ä¸‹è¼‰æ™‚é–“"):
            download_results = downloader.download_all_historical()
        
        # æ­¥é©Ÿ 2: æ¸…æ´—æ‰€æœ‰ä¸‹è¼‰çš„è³‡æ–™
        logging.info("\n=== æ­¥é©Ÿ 2: æ¸…æ´—æ­·å²è³‡æ–™ ===")
        cleaner = OTCDataCleaner(config)
        
        with performance_monitor.measure_time("ç¸½æ¸…æ´—æ™‚é–“"):
            clean_results = cleaner.clean_all_historical_files()
        
        # å®Œæˆå ±å‘Š
        end_time = datetime.now()
        duration = end_time - start_time
        
        logging.info(f"\n[ğŸ‰] æ‰€æœ‰ç¨‹åºå®Œæˆ!")
        logging.info(f"[â±] ç¸½åŸ·è¡Œæ™‚é–“: {duration}")
        logging.info(f"[ğŸ“Š] ä¸‹è¼‰çµæœ: æˆåŠŸ {download_results['success']}, å¤±æ•— {download_results['failed']}, è·³é {download_results['skipped']}")
        logging.info(f"[ğŸ“Š] æ¸…æ´—çµæœ: æˆåŠŸ {clean_results['success']}, å¤±æ•— {clean_results['failed']}")
        logging.info(f"[ğŸ“] åŸå§‹è³‡æ–™: {RAW_DIR}")
        logging.info(f"[ğŸ“] æ¸…æ´—è³‡æ–™: {CLEAN_DIR}")
        
        # ä¿å­˜åŸ·è¡Œå ±å‘Š
        performance_report_path = LOG_DIR / f"historical_performance_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        performance_monitor.save_report(performance_report_path)
        
    except KeyboardInterrupt:
        logging.warning("\n[â¹] ä½¿ç”¨è€…ä¸­æ–·åŸ·è¡Œ")
    except Exception as e:
        logging.error(f"\n[âŒ] åŸ·è¡Œéç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
        logging.error(traceback.format_exc())
        raise

def load_config() -> Dict[str, Any]:
    """è¼‰å…¥è¨­å®šæª”"""
    config_file = BASE_DIR / "otc_config.json"
    
    if config_file.exists():
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
            logging.info(f"å·²è¼‰å…¥è¨­å®šæª”: {config_file}")
            return config
        except Exception as e:
            logging.warning(f"è¨­å®šæª”è¼‰å…¥å¤±æ•—ï¼Œä½¿ç”¨é è¨­è¨­å®š: {e}")
    else:
        logging.info("æœªæ‰¾åˆ°è¨­å®šæª”ï¼Œä½¿ç”¨é è¨­è¨­å®š")
    
    return DEFAULT_CONFIG

if __name__ == "__main__":
    main(), na=False)]
        
        numeric_cols = [col for col in clean_df.columns if col not in ["stock_id", "name", "remark"]]
        for col in numeric_cols:
            clean_df[col] = self.clean_numeric_column(clean_df[col])
        
        return clean_df.sort_values("stock_id").reset_index(drop=True)
    
    def _clean_exempted(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ¸…æ´—å¹³ç›¤ä¸‹å¾—èåˆ¸è³£å‡ºè­‰åˆ¸åå–®"""
        cols = list(df.columns)
        
        # æ‰¾åˆ°æ•¸æ“šé–‹å§‹çš„è¡Œ
        data_start_row = 0
        for i, row in df.iterrows():
            first_val = str(row.iloc[0])
            if re.match(r'^\d{3,4}', first_val):
                data_start_row = i
                break
        
        if data_start_row > 0:
            df = df.iloc[data_start_row:].copy()
            df.columns = cols
        
        code_col = None
        name_col = None
        
        for c in cols:
            if "è­‰åˆ¸ä»£è™Ÿ" in c or "ä»£è™Ÿ" in c or "ä»£ç¢¼" in c:
                code_col = c
                break
        
        for c in cols:
            if "è­‰åˆ¸åç¨±" in c or "åç¨±" in c:
                name_col = c
                break
        
        if not code_col:
            if len(cols) >= 2:
                code_col = cols[0]
                name_col = cols[1] if not name_col else name_col
        
        if not code_col:
            logging.error("  ç„¡æ³•è­˜åˆ¥è­‰åˆ¸ä»£è™Ÿæ¬„ä½")
            return None
        
        column_mapping = {code_col: "stock_id"}
        if name_col:
            column_mapping[name_col] = "name"
        
        # æ‰¾åˆ°æ¨™è¨˜æ¬„ä½
        mark_cols = []
        for c in cols:
            if c not in [code_col, name_col]:
                if "æš«åœ" in c or "æ¨™è¨˜" in c or "è¨»è¨˜" in c or len(c) <= 3:
                    mark_cols.append(c)
        
        for i, mark_col in enumerate(mark_cols):
            column_mapping[mark_col] = f"mark_{i+1}" if i > 0 else "mark"
        
        available_cols = [col for col in column_mapping.keys() if col in df.columns]
        clean_df = df[available_cols].rename(columns=column_mapping).copy()
        
        clean_df["stock_id"] = self.extract_stock_id(clean_df["stock_id"])
        clean_df = clean_df.dropna(subset=["stock_id"])
        clean_df = clean_df[clean_df["stock_id"].str.match(r'^\d{4}
    
    def clean_all_historical_files(self) -> Dict[str, int]:
        """æ¸…æ´—æ‰€æœ‰æ­·å²æª”æ¡ˆ"""
        self.ensure_dir(CLEAN_DIR)
        results = {"success": 0, "failed": 0, "failed_files": []}
        
        # å–å¾—æ‰€æœ‰æª”æ¡ˆï¼ŒæŒ‰æ—¥æœŸåˆ†çµ„
        files_by_date = self.get_all_raw_files_by_date()
        total_dates = len(files_by_date)
        
        logging.info(f"\n=== é–‹å§‹æ¸…æ´—æ­·å²è³‡æ–™ ===")
        logging.info(f"æ‰¾åˆ° {total_dates} å€‹æ—¥æœŸçš„è³‡æ–™")
        
        with self.performance_monitor.measure_time("æ­·å²è³‡æ–™æ¸…æ´—"):
            for date_idx, (date_str, file_list) in enumerate(sorted(files_by_date.items()), 1):
                logging.info(f"\nâ”€â”€ æ¸…æ´—æ—¥æœŸ {date_str} ({date_idx}/{total_dates}) â”€â”€")
                
                for file_path in file_list:
                    try:
                        if self.clean_single_file(file_path):
                            results["success"] += 1
                        else:
                            results["failed"] += 1
                            results["failed_files"].append(file_path.name)
                    except Exception as e:
                        logging.error(f"æ¸…ç†æª”æ¡ˆ {file_path.name} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                        results["failed"] += 1
                        results["failed_files"].append(file_path.name)
        
        logging.info(f"\n[ğŸ“Š] æ¸…æ´—çµ±è¨ˆ:")
        logging.info(f"    - æˆåŠŸ: {results['success']}")
        logging.info(f"    - å¤±æ•—: {results['failed']}")
        
        return results

def main():
    """ä¸»è¦åŸ·è¡Œå‡½æ•¸"""
    setup_logging()
    logging.info("=== ä¸Šæ«ƒæ­·å²è³‡æ–™æ‰¹é‡ä¸‹è¼‰ + æ¸…æ´—ç³»çµ± ===")
    
    config = load_config()
    
    # ç¢ºèªåŸ·è¡Œ
    print(f"\nç›®æ¨™æ—¥æœŸç¯„åœ: {START_DATE.strftime('%Y-%m-%d')} ~ {END_DATE.strftime('%Y-%m-%d')}")
    print(f"é ä¼°äº¤æ˜“æ—¥: ~{len([d for d in pd.date_range(START_DATE, END_DATE) if d.weekday() < 5])} å¤©")
    print(f"è³‡æ–™é …ç›®: {len(config['download_items'])} ç¨®")
    print(f"é ä¼°ç¸½æ™‚é–“: 4-6 å°æ™‚")
    print(f"è¼¸å‡ºç›®éŒ„: {RAW_DIR} (åŸå§‹), {CLEAN_DIR} (æ¸…æ´—)")
    
    response = input("\nâš ï¸  é€™æ˜¯é•·æ™‚é–“åŸ·è¡Œä»»å‹™ï¼Œæ˜¯å¦ç¢ºå®šé–‹å§‹? (y/N): ").strip().lower()
    if response != 'y':
        print("å–æ¶ˆåŸ·è¡Œ")
        return
    
    start_time = datetime.now()
    performance_monitor = PerformanceMonitor()
    
    try:
        # æ­¥é©Ÿ 1: æ‰¹é‡ä¸‹è¼‰æ­·å²è³‡æ–™
        logging.info("\n=== æ­¥é©Ÿ 1: æ‰¹é‡ä¸‹è¼‰æ­·å²è³‡æ–™ ===")
        downloader = OTCHistoricalDownloader(config)
        
        with performance_monitor.measure_time("ç¸½ä¸‹è¼‰æ™‚é–“"):
            download_results = downloader.download_all_historical()
        
        # æ­¥é©Ÿ 2: æ¸…æ´—æ‰€æœ‰ä¸‹è¼‰çš„è³‡æ–™
        logging.info("\n=== æ­¥é©Ÿ 2: æ¸…æ´—æ­·å²è³‡æ–™ ===")
        cleaner = OTCDataCleaner(config)
        
        with performance_monitor.measure_time("ç¸½æ¸…æ´—æ™‚é–“"):
            clean_results = cleaner.clean_all_historical_files()
        
        # å®Œæˆå ±å‘Š
        end_time = datetime.now()
        duration = end_time - start_time
        
        logging.info(f"\n[ğŸ‰] æ‰€æœ‰ç¨‹åºå®Œæˆ!")
        logging.info(f"[â±] ç¸½åŸ·è¡Œæ™‚é–“: {duration}")
        logging.info(f"[ğŸ“Š] ä¸‹è¼‰çµæœ: æˆåŠŸ {download_results['success']}, å¤±æ•— {download_results['failed']}, è·³é {download_results['skipped']}")
        logging.info(f"[ğŸ“Š] æ¸…æ´—çµæœ: æˆåŠŸ {clean_results['success']}, å¤±æ•— {clean_results['failed']}")
        logging.info(f"[ğŸ“] åŸå§‹è³‡æ–™: {RAW_DIR}")
        logging.info(f"[ğŸ“] æ¸…æ´—è³‡æ–™: {CLEAN_DIR}")
        
        # ä¿å­˜åŸ·è¡Œå ±å‘Š
        performance_report_path = LOG_DIR / f"historical_performance_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        performance_monitor.save_report(performance_report_path)
        
    except KeyboardInterrupt:
        logging.warning("\n[â¹] ä½¿ç”¨è€…ä¸­æ–·åŸ·è¡Œ")
    except Exception as e:
        logging.error(f"\n[âŒ] åŸ·è¡Œéç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
        logging.error(traceback.format_exc())
        raise

def load_config() -> Dict[str, Any]:
    """è¼‰å…¥è¨­å®šæª”"""
    config_file = BASE_DIR / "otc_config.json"
    
    if config_file.exists():
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
            logging.info(f"å·²è¼‰å…¥è¨­å®šæª”: {config_file}")
            return config
        except Exception as e:
            logging.warning(f"è¨­å®šæª”è¼‰å…¥å¤±æ•—ï¼Œä½¿ç”¨é è¨­è¨­å®š: {e}")
    else:
        logging.info("æœªæ‰¾åˆ°è¨­å®šæª”ï¼Œä½¿ç”¨é è¨­è¨­å®š")
    
    return DEFAULT_CONFIG

if __name__ == "__main__":
    main(), na=False)]
        
        # éæ¿¾çµ±è¨ˆè¡Œ
        if "name" in clean_df.columns:
            clean_df = clean_df[~clean_df["name"].str.contains("å…±.*ç­†|åˆè¨ˆ|çµ±è¨ˆ|è¨»:|èªªæ˜:", na=False, regex=True)]
        
        return clean_df.sort_values("stock_id").reset_index(drop=True)
    
    def _clean_investment_trust(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ¸…æ´—æŠ•ä¿¡è²·è³£è¶…è³‡æ–™"""
        cols = list(df.columns)
        
        rank_col = next((c for c in cols if "æ’è¡Œ" in c), None)
        code_col = next((c for c in cols if "ä»£è™Ÿ" in c), None)
        name_col = next((c for c in cols if "åç¨±" in c), None)
        
        if not all([rank_col, code_col, name_col]):
            logging.error("  ç¼ºå°‘å¿…è¦æ¬„ä½")
            return None
        
        column_mapping = {
            rank_col: "rank",
            code_col: "stock_id",
            name_col: "name"
        }
        
        it_fields = {
            "è²·é€²": "it_buy_shares",
            "è³£å‡º": "it_sell_shares",
            "è²·è³£è¶…": "it_diff_shares",
            "è²·é€²é‡‘é¡": "it_buy_amount",
            "è³£å‡ºé‡‘é¡": "it_sell_amount",
            "è²·è³£è¶…é‡‘é¡": "it_diff_amount"
        }
        
        for pattern, new_name in it_fields.items():
            matching_col = next((c for c in cols if pattern in c), None)
            if matching_col:
                column_mapping[matching_col] = new_name
        
        available_cols = [col for col in column_mapping.keys() if col in df.columns]
        clean_df = df[available_cols].rename(columns=column_mapping).copy()
        
        clean_df["stock_id"] = self.extract_stock_id(clean_df["stock_id"])
        clean_df = clean_df.dropna(subset=["stock_id"])
        clean_df = clean_df[clean_df["stock_id"].str.match(r'^\d{4}
    
    def clean_all_historical_files(self) -> Dict[str, int]:
        """æ¸…æ´—æ‰€æœ‰æ­·å²æª”æ¡ˆ"""
        self.ensure_dir(CLEAN_DIR)
        results = {"success": 0, "failed": 0, "failed_files": []}
        
        # å–å¾—æ‰€æœ‰æª”æ¡ˆï¼ŒæŒ‰æ—¥æœŸåˆ†çµ„
        files_by_date = self.get_all_raw_files_by_date()
        total_dates = len(files_by_date)
        
        logging.info(f"\n=== é–‹å§‹æ¸…æ´—æ­·å²è³‡æ–™ ===")
        logging.info(f"æ‰¾åˆ° {total_dates} å€‹æ—¥æœŸçš„è³‡æ–™")
        
        with self.performance_monitor.measure_time("æ­·å²è³‡æ–™æ¸…æ´—"):
            for date_idx, (date_str, file_list) in enumerate(sorted(files_by_date.items()), 1):
                logging.info(f"\nâ”€â”€ æ¸…æ´—æ—¥æœŸ {date_str} ({date_idx}/{total_dates}) â”€â”€")
                
                for file_path in file_list:
                    try:
                        if self.clean_single_file(file_path):
                            results["success"] += 1
                        else:
                            results["failed"] += 1
                            results["failed_files"].append(file_path.name)
                    except Exception as e:
                        logging.error(f"æ¸…ç†æª”æ¡ˆ {file_path.name} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                        results["failed"] += 1
                        results["failed_files"].append(file_path.name)
        
        logging.info(f"\n[ğŸ“Š] æ¸…æ´—çµ±è¨ˆ:")
        logging.info(f"    - æˆåŠŸ: {results['success']}")
        logging.info(f"    - å¤±æ•—: {results['failed']}")
        
        return results

def main():
    """ä¸»è¦åŸ·è¡Œå‡½æ•¸"""
    setup_logging()
    logging.info("=== ä¸Šæ«ƒæ­·å²è³‡æ–™æ‰¹é‡ä¸‹è¼‰ + æ¸…æ´—ç³»çµ± ===")
    
    config = load_config()
    
    # ç¢ºèªåŸ·è¡Œ
    print(f"\nç›®æ¨™æ—¥æœŸç¯„åœ: {START_DATE.strftime('%Y-%m-%d')} ~ {END_DATE.strftime('%Y-%m-%d')}")
    print(f"é ä¼°äº¤æ˜“æ—¥: ~{len([d for d in pd.date_range(START_DATE, END_DATE) if d.weekday() < 5])} å¤©")
    print(f"è³‡æ–™é …ç›®: {len(config['download_items'])} ç¨®")
    print(f"é ä¼°ç¸½æ™‚é–“: 4-6 å°æ™‚")
    print(f"è¼¸å‡ºç›®éŒ„: {RAW_DIR} (åŸå§‹), {CLEAN_DIR} (æ¸…æ´—)")
    
    response = input("\nâš ï¸  é€™æ˜¯é•·æ™‚é–“åŸ·è¡Œä»»å‹™ï¼Œæ˜¯å¦ç¢ºå®šé–‹å§‹? (y/N): ").strip().lower()
    if response != 'y':
        print("å–æ¶ˆåŸ·è¡Œ")
        return
    
    start_time = datetime.now()
    performance_monitor = PerformanceMonitor()
    
    try:
        # æ­¥é©Ÿ 1: æ‰¹é‡ä¸‹è¼‰æ­·å²è³‡æ–™
        logging.info("\n=== æ­¥é©Ÿ 1: æ‰¹é‡ä¸‹è¼‰æ­·å²è³‡æ–™ ===")
        downloader = OTCHistoricalDownloader(config)
        
        with performance_monitor.measure_time("ç¸½ä¸‹è¼‰æ™‚é–“"):
            download_results = downloader.download_all_historical()
        
        # æ­¥é©Ÿ 2: æ¸…æ´—æ‰€æœ‰ä¸‹è¼‰çš„è³‡æ–™
        logging.info("\n=== æ­¥é©Ÿ 2: æ¸…æ´—æ­·å²è³‡æ–™ ===")
        cleaner = OTCDataCleaner(config)
        
        with performance_monitor.measure_time("ç¸½æ¸…æ´—æ™‚é–“"):
            clean_results = cleaner.clean_all_historical_files()
        
        # å®Œæˆå ±å‘Š
        end_time = datetime.now()
        duration = end_time - start_time
        
        logging.info(f"\n[ğŸ‰] æ‰€æœ‰ç¨‹åºå®Œæˆ!")
        logging.info(f"[â±] ç¸½åŸ·è¡Œæ™‚é–“: {duration}")
        logging.info(f"[ğŸ“Š] ä¸‹è¼‰çµæœ: æˆåŠŸ {download_results['success']}, å¤±æ•— {download_results['failed']}, è·³é {download_results['skipped']}")
        logging.info(f"[ğŸ“Š] æ¸…æ´—çµæœ: æˆåŠŸ {clean_results['success']}, å¤±æ•— {clean_results['failed']}")
        logging.info(f"[ğŸ“] åŸå§‹è³‡æ–™: {RAW_DIR}")
        logging.info(f"[ğŸ“] æ¸…æ´—è³‡æ–™: {CLEAN_DIR}")
        
        # ä¿å­˜åŸ·è¡Œå ±å‘Š
        performance_report_path = LOG_DIR / f"historical_performance_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        performance_monitor.save_report(performance_report_path)
        
    except KeyboardInterrupt:
        logging.warning("\n[â¹] ä½¿ç”¨è€…ä¸­æ–·åŸ·è¡Œ")
    except Exception as e:
        logging.error(f"\n[âŒ] åŸ·è¡Œéç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
        logging.error(traceback.format_exc())
        raise

def load_config() -> Dict[str, Any]:
    """è¼‰å…¥è¨­å®šæª”"""
    config_file = BASE_DIR / "otc_config.json"
    
    if config_file.exists():
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
            logging.info(f"å·²è¼‰å…¥è¨­å®šæª”: {config_file}")
            return config
        except Exception as e:
            logging.warning(f"è¨­å®šæª”è¼‰å…¥å¤±æ•—ï¼Œä½¿ç”¨é è¨­è¨­å®š: {e}")
    else:
        logging.info("æœªæ‰¾åˆ°è¨­å®šæª”ï¼Œä½¿ç”¨é è¨­è¨­å®š")
    
    return DEFAULT_CONFIG

if __name__ == "__main__":
    main(), na=False)]
        
        numeric_cols = [col for col in clean_df.columns if col not in ["rank", "stock_id", "name"]]
        for col in numeric_cols:
            clean_df[col] = self.clean_numeric_column(clean_df[col])
        
        return clean_df.sort_values("stock_id").reset_index(drop=True)
    
    def clean_all_historical_files(self) -> Dict[str, int]:
        """æ¸…æ´—æ‰€æœ‰æ­·å²æª”æ¡ˆ"""
        self.ensure_dir(CLEAN_DIR)
        results = {"success": 0, "failed": 0, "failed_files": []}
        
        # å–å¾—æ‰€æœ‰æª”æ¡ˆï¼ŒæŒ‰æ—¥æœŸåˆ†çµ„
        files_by_date = self.get_all_raw_files_by_date()
        total_dates = len(files_by_date)
        
        logging.info(f"\n=== é–‹å§‹æ¸…æ´—æ­·å²è³‡æ–™ ===")
        logging.info(f"æ‰¾åˆ° {total_dates} å€‹æ—¥æœŸçš„è³‡æ–™")
        
        with self.performance_monitor.measure_time("æ­·å²è³‡æ–™æ¸…æ´—"):
            for date_idx, (date_str, file_list) in enumerate(sorted(files_by_date.items()), 1):
                logging.info(f"\nâ”€â”€ æ¸…æ´—æ—¥æœŸ {date_str} ({date_idx}/{total_dates}) â”€â”€")
                
                for file_path in file_list:
                    try:
                        if self.clean_single_file(file_path):
                            results["success"] += 1
                        else:
                            results["failed"] += 1
                            results["failed_files"].append(file_path.name)
                    except Exception as e:
                        logging.error(f"æ¸…ç†æª”æ¡ˆ {file_path.name} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                        results["failed"] += 1
                        results["failed_files"].append(file_path.name)
        
        logging.info(f"\n[ğŸ“Š] æ¸…æ´—çµ±è¨ˆ:")
        logging.info(f"    - æˆåŠŸ: {results['success']}")
        logging.info(f"    - å¤±æ•—: {results['failed']}")
        
        return results

def main():
    """ä¸»è¦åŸ·è¡Œå‡½æ•¸"""
    setup_logging()
    logging.info("=== ä¸Šæ«ƒæ­·å²è³‡æ–™æ‰¹é‡ä¸‹è¼‰ + æ¸…æ´—ç³»çµ± ===")
    
    config = load_config()
    
    # ç¢ºèªåŸ·è¡Œ
    print(f"\nç›®æ¨™æ—¥æœŸç¯„åœ: {START_DATE.strftime('%Y-%m-%d')} ~ {END_DATE.strftime('%Y-%m-%d')}")
    print(f"é ä¼°äº¤æ˜“æ—¥: ~{len([d for d in pd.date_range(START_DATE, END_DATE) if d.weekday() < 5])} å¤©")
    print(f"è³‡æ–™é …ç›®: {len(config['download_items'])} ç¨®")
    print(f"é ä¼°ç¸½æ™‚é–“: 4-6 å°æ™‚")
    print(f"è¼¸å‡ºç›®éŒ„: {RAW_DIR} (åŸå§‹), {CLEAN_DIR} (æ¸…æ´—)")
    
    response = input("\nâš ï¸  é€™æ˜¯é•·æ™‚é–“åŸ·è¡Œä»»å‹™ï¼Œæ˜¯å¦ç¢ºå®šé–‹å§‹? (y/N): ").strip().lower()
    if response != 'y':
        print("å–æ¶ˆåŸ·è¡Œ")
        return
    
    start_time = datetime.now()
    performance_monitor = PerformanceMonitor()
    
    try:
        # æ­¥é©Ÿ 1: æ‰¹é‡ä¸‹è¼‰æ­·å²è³‡æ–™
        logging.info("\n=== æ­¥é©Ÿ 1: æ‰¹é‡ä¸‹è¼‰æ­·å²è³‡æ–™ ===")
        downloader = OTCHistoricalDownloader(config)
        
        with performance_monitor.measure_time("ç¸½ä¸‹è¼‰æ™‚é–“"):
            download_results = downloader.download_all_historical()
        
        # æ­¥é©Ÿ 2: æ¸…æ´—æ‰€æœ‰ä¸‹è¼‰çš„è³‡æ–™
        logging.info("\n=== æ­¥é©Ÿ 2: æ¸…æ´—æ­·å²è³‡æ–™ ===")
        cleaner = OTCDataCleaner(config)
        
        with performance_monitor.measure_time("ç¸½æ¸…æ´—æ™‚é–“"):
            clean_results = cleaner.clean_all_historical_files()
        
        # å®Œæˆå ±å‘Š
        end_time = datetime.now()
        duration = end_time - start_time
        
        logging.info(f"\n[ğŸ‰] æ‰€æœ‰ç¨‹åºå®Œæˆ!")
        logging.info(f"[â±] ç¸½åŸ·è¡Œæ™‚é–“: {duration}")
        logging.info(f"[ğŸ“Š] ä¸‹è¼‰çµæœ: æˆåŠŸ {download_results['success']}, å¤±æ•— {download_results['failed']}, è·³é {download_results['skipped']}")
        logging.info(f"[ğŸ“Š] æ¸…æ´—çµæœ: æˆåŠŸ {clean_results['success']}, å¤±æ•— {clean_results['failed']}")
        logging.info(f"[ğŸ“] åŸå§‹è³‡æ–™: {RAW_DIR}")
        logging.info(f"[ğŸ“] æ¸…æ´—è³‡æ–™: {CLEAN_DIR}")
        
        # ä¿å­˜åŸ·è¡Œå ±å‘Š
        performance_report_path = LOG_DIR / f"historical_performance_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        performance_monitor.save_report(performance_report_path)
        
    except KeyboardInterrupt:
        logging.warning("\n[â¹] ä½¿ç”¨è€…ä¸­æ–·åŸ·è¡Œ")
    except Exception as e:
        logging.error(f"\n[âŒ] åŸ·è¡Œéç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
        logging.error(traceback.format_exc())
        raise

def load_config() -> Dict[str, Any]:
    """è¼‰å…¥è¨­å®šæª”"""
    config_file = BASE_DIR / "otc_config.json"
    
    if config_file.exists():
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
            logging.info(f"å·²è¼‰å…¥è¨­å®šæª”: {config_file}")
            return config
        except Exception as e:
            logging.warning(f"è¨­å®šæª”è¼‰å…¥å¤±æ•—ï¼Œä½¿ç”¨é è¨­è¨­å®š: {e}")
    else:
        logging.info("æœªæ‰¾åˆ°è¨­å®šæª”ï¼Œä½¿ç”¨é è¨­è¨­å®š")
    
    return DEFAULT_CONFIG

if __name__ == "__main__":
    main(), na=False)]
                if len(invalid_ids) > 0:
                    issues.append({
                        'file': csv_file.name,
                        'issue': 'åŒ…å«é4ä½æ•¸å­—è‚¡ç¥¨ä»£è™Ÿ',
                        'count': len(invalid_ids),
                        'samples': invalid_ids['stock_id'].head(5).tolist()
                    })
                
                for col in df.select_dtypes(include=['float64', 'int64']).columns:
                    if df[col].astype(str).str.contains(r'[eE][+-]?\d+', regex=True).any():
                        issues.append({
                            'file': csv_file.name,
                            'issue': f'æ¬„ä½ {col} åŒ…å«ç§‘å­¸è¨˜è™Ÿ',
                            'column': col
                        })
        
        except Exception as e:
            logging.error(f"é©—è­‰æª”æ¡ˆ {csv_file.name} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    
    if issues:
        logging.warning("ç™¼ç¾ä»¥ä¸‹è³‡æ–™å“è³ªå•é¡Œï¼š")
        for issue in issues:
            logging.warning(f"  - {issue}")
    else:
        logging.info("æ‰€æœ‰æª”æ¡ˆè³‡æ–™å“è³ªè‰¯å¥½ï¼")
    
    return issues
        """æ¸…æ´—æ‰€æœ‰æ­·å²æª”æ¡ˆ"""
        self.ensure_dir(CLEAN_DIR)
        results = {"success": 0, "failed": 0, "failed_files": []}
        
        # å–å¾—æ‰€æœ‰æª”æ¡ˆï¼ŒæŒ‰æ—¥æœŸåˆ†çµ„
        files_by_date = self.get_all_raw_files_by_date()
        total_dates = len(files_by_date)
        
        logging.info(f"\n=== é–‹å§‹æ¸…æ´—æ­·å²è³‡æ–™ ===")
        logging.info(f"æ‰¾åˆ° {total_dates} å€‹æ—¥æœŸçš„è³‡æ–™")
        
        with self.performance_monitor.measure_time("æ­·å²è³‡æ–™æ¸…æ´—"):
            for date_idx, (date_str, file_list) in enumerate(sorted(files_by_date.items()), 1):
                logging.info(f"\nâ”€â”€ æ¸…æ´—æ—¥æœŸ {date_str} ({date_idx}/{total_dates}) â”€â”€")
                
                for file_path in file_list:
                    try:
                        if self.clean_single_file(file_path):
                            results["success"] += 1
                        else:
                            results["failed"] += 1
                            results["failed_files"].append(file_path.name)
                    except Exception as e:
                        logging.error(f"æ¸…ç†æª”æ¡ˆ {file_path.name} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                        results["failed"] += 1
                        results["failed_files"].append(file_path.name)
        
        logging.info(f"\n[ğŸ“Š] æ¸…æ´—çµ±è¨ˆ:")
        logging.info(f"    - æˆåŠŸ: {results['success']}")
        logging.info(f"    - å¤±æ•—: {results['failed']}")
        
        return results

def main():
    """ä¸»è¦åŸ·è¡Œå‡½æ•¸"""
    setup_logging()
    logging.info("=== ä¸Šæ«ƒæ­·å²è³‡æ–™æ‰¹é‡ä¸‹è¼‰ + æ¸…æ´—ç³»çµ± ===")
    
    config = load_config()
    
    # ç¢ºèªåŸ·è¡Œ
    print(f"\nç›®æ¨™æ—¥æœŸç¯„åœ: {START_DATE.strftime('%Y-%m-%d')} ~ {END_DATE.strftime('%Y-%m-%d')}")
    print(f"é ä¼°äº¤æ˜“æ—¥: ~{len([d for d in pd.date_range(START_DATE, END_DATE) if d.weekday() < 5])} å¤©")
    print(f"è³‡æ–™é …ç›®: {len(config['download_items'])} ç¨®")
    print(f"é ä¼°ç¸½æ™‚é–“: 4-6 å°æ™‚")
    print(f"è¼¸å‡ºç›®éŒ„: {RAW_DIR} (åŸå§‹), {CLEAN_DIR} (æ¸…æ´—)")
    
    response = input("\nâš ï¸  é€™æ˜¯é•·æ™‚é–“åŸ·è¡Œä»»å‹™ï¼Œæ˜¯å¦ç¢ºå®šé–‹å§‹? (y/N): ").strip().lower()
    if response != 'y':
        print("å–æ¶ˆåŸ·è¡Œ")
        return
    
    start_time = datetime.now()
    performance_monitor = PerformanceMonitor()
    
    try:
        # æ­¥é©Ÿ 1: æ‰¹é‡ä¸‹è¼‰æ­·å²è³‡æ–™
        logging.info("\n=== æ­¥é©Ÿ 1: æ‰¹é‡ä¸‹è¼‰æ­·å²è³‡æ–™ ===")
        downloader = OTCHistoricalDownloader(config)
        
        with performance_monitor.measure_time("ç¸½ä¸‹è¼‰æ™‚é–“"):
            download_results = downloader.download_all_historical()
        
        # æ­¥é©Ÿ 2: æ¸…æ´—æ‰€æœ‰ä¸‹è¼‰çš„è³‡æ–™
        logging.info("\n=== æ­¥é©Ÿ 2: æ¸…æ´—æ­·å²è³‡æ–™ ===")
        cleaner = OTCDataCleaner(config)
        
        with performance_monitor.measure_time("ç¸½æ¸…æ´—æ™‚é–“"):
            clean_results = cleaner.clean_all_historical_files()
        
        # å®Œæˆå ±å‘Š
        end_time = datetime.now()
        duration = end_time - start_time
        
        logging.info(f"\n[ğŸ‰] æ‰€æœ‰ç¨‹åºå®Œæˆ!")
        logging.info(f"[â±] ç¸½åŸ·è¡Œæ™‚é–“: {duration}")
        logging.info(f"[ğŸ“Š] ä¸‹è¼‰çµæœ: æˆåŠŸ {download_results['success']}, å¤±æ•— {download_results['failed']}, è·³é {download_results['skipped']}")
        logging.info(f"[ğŸ“Š] æ¸…æ´—çµæœ: æˆåŠŸ {clean_results['success']}, å¤±æ•— {clean_results['failed']}")
        logging.info(f"[ğŸ“] åŸå§‹è³‡æ–™: {RAW_DIR}")
        logging.info(f"[ğŸ“] æ¸…æ´—è³‡æ–™: {CLEAN_DIR}")
        
        # ä¿å­˜åŸ·è¡Œå ±å‘Š
        performance_report_path = LOG_DIR / f"historical_performance_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        performance_monitor.save_report(performance_report_path)
        
    except KeyboardInterrupt:
        logging.warning("\n[â¹] ä½¿ç”¨è€…ä¸­æ–·åŸ·è¡Œ")
    except Exception as e:
        logging.error(f"\n[âŒ] åŸ·è¡Œéç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
        logging.error(traceback.format_exc())
        raise

def load_config() -> Dict[str, Any]:
    """è¼‰å…¥è¨­å®šæª”"""
    config_file = BASE_DIR / "otc_config.json"
    
    if config_file.exists():
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
            logging.info(f"å·²è¼‰å…¥è¨­å®šæª”: {config_file}")
            return config
        except Exception as e:
            logging.warning(f"è¨­å®šæª”è¼‰å…¥å¤±æ•—ï¼Œä½¿ç”¨é è¨­è¨­å®š: {e}")
    else:
        logging.info("æœªæ‰¾åˆ°è¨­å®šæª”ï¼Œä½¿ç”¨é è¨­è¨­å®š")
    
    return DEFAULT_CONFIG

if __name__ == "__main__":
    main(), na=False)]
        
        numeric_cols = [col for col in clean_df.columns if col not in ["stock_id", "name"]]
        for col in numeric_cols:
            clean_df[col] = self.clean_numeric_column(clean_df[col])
        
        return clean_df.sort_values("stock_id").reset_index(drop=True)
    
    def _clean_institutional_detail(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ¸…æ´—ä¸‰å¤§æ³•äººè²·è³£æ˜ç´°è³‡æ–™"""
        cols = list(df.columns)
        
        code_col = next((c for c in cols if "ä»£è™Ÿ" in c or "ä»£ç¢¼" in c), None)
        name_col = next((c for c in cols if "åç¨±" in c), None)
        
        if not all([code_col, name_col]):
            logging.error("  ç¼ºå°‘å¿…è¦æ¬„ä½")
            return None
        
        column_mapping = {
            code_col: "stock_id",
            name_col: "name"
        }
        
        institutional_fields = {
            "å¤–è³‡åŠé™¸è³‡": "ii_foreign_net",
            "å¤–è³‡è‡ªç‡Ÿå•†": "ii_foreign_self_net",
            "æŠ•ä¿¡": "ii_trust_net",
            "è‡ªç‡Ÿå•†(è‡ªè¡Œè²·è³£)": "ii_dealer_self_net",
            "è‡ªç‡Ÿå•†(é¿éšª)": "ii_dealer_hedge_net",
            "åˆè¨ˆ": "ii_total_net"
        }
        
        for pattern, new_name in institutional_fields.items():
            matching_col = next((c for c in cols if pattern in c and "è²·è³£è¶…" in c), None)
            if matching_col:
                column_mapping[matching_col] = new_name
        
        available_cols = [col for col in column_mapping.keys() if col in df.columns]
        clean_df = df[available_cols].rename(columns=column_mapping).copy()
        
        clean_df["stock_id"] = self.extract_stock_id(clean_df["stock_id"])
        clean_df = clean_df.dropna(subset=["stock_id"])
        clean_df = clean_df[clean_df["stock_id"].str.match(r'^\d{4}
    
    def clean_all_historical_files(self) -> Dict[str, int]:
        """æ¸…æ´—æ‰€æœ‰æ­·å²æª”æ¡ˆ"""
        self.ensure_dir(CLEAN_DIR)
        results = {"success": 0, "failed": 0, "failed_files": []}
        
        # å–å¾—æ‰€æœ‰æª”æ¡ˆï¼ŒæŒ‰æ—¥æœŸåˆ†çµ„
        files_by_date = self.get_all_raw_files_by_date()
        total_dates = len(files_by_date)
        
        logging.info(f"\n=== é–‹å§‹æ¸…æ´—æ­·å²è³‡æ–™ ===")
        logging.info(f"æ‰¾åˆ° {total_dates} å€‹æ—¥æœŸçš„è³‡æ–™")
        
        with self.performance_monitor.measure_time("æ­·å²è³‡æ–™æ¸…æ´—"):
            for date_idx, (date_str, file_list) in enumerate(sorted(files_by_date.items()), 1):
                logging.info(f"\nâ”€â”€ æ¸…æ´—æ—¥æœŸ {date_str} ({date_idx}/{total_dates}) â”€â”€")
                
                for file_path in file_list:
                    try:
                        if self.clean_single_file(file_path):
                            results["success"] += 1
                        else:
                            results["failed"] += 1
                            results["failed_files"].append(file_path.name)
                    except Exception as e:
                        logging.error(f"æ¸…ç†æª”æ¡ˆ {file_path.name} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                        results["failed"] += 1
                        results["failed_files"].append(file_path.name)
        
        logging.info(f"\n[ğŸ“Š] æ¸…æ´—çµ±è¨ˆ:")
        logging.info(f"    - æˆåŠŸ: {results['success']}")
        logging.info(f"    - å¤±æ•—: {results['failed']}")
        
        return results

def main():
    """ä¸»è¦åŸ·è¡Œå‡½æ•¸"""
    setup_logging()
    logging.info("=== ä¸Šæ«ƒæ­·å²è³‡æ–™æ‰¹é‡ä¸‹è¼‰ + æ¸…æ´—ç³»çµ± ===")
    
    config = load_config()
    
    # ç¢ºèªåŸ·è¡Œ
    print(f"\nç›®æ¨™æ—¥æœŸç¯„åœ: {START_DATE.strftime('%Y-%m-%d')} ~ {END_DATE.strftime('%Y-%m-%d')}")
    print(f"é ä¼°äº¤æ˜“æ—¥: ~{len([d for d in pd.date_range(START_DATE, END_DATE) if d.weekday() < 5])} å¤©")
    print(f"è³‡æ–™é …ç›®: {len(config['download_items'])} ç¨®")
    print(f"é ä¼°ç¸½æ™‚é–“: 4-6 å°æ™‚")
    print(f"è¼¸å‡ºç›®éŒ„: {RAW_DIR} (åŸå§‹), {CLEAN_DIR} (æ¸…æ´—)")
    
    response = input("\nâš ï¸  é€™æ˜¯é•·æ™‚é–“åŸ·è¡Œä»»å‹™ï¼Œæ˜¯å¦ç¢ºå®šé–‹å§‹? (y/N): ").strip().lower()
    if response != 'y':
        print("å–æ¶ˆåŸ·è¡Œ")
        return
    
    start_time = datetime.now()
    performance_monitor = PerformanceMonitor()
    
    try:
        # æ­¥é©Ÿ 1: æ‰¹é‡ä¸‹è¼‰æ­·å²è³‡æ–™
        logging.info("\n=== æ­¥é©Ÿ 1: æ‰¹é‡ä¸‹è¼‰æ­·å²è³‡æ–™ ===")
        downloader = OTCHistoricalDownloader(config)
        
        with performance_monitor.measure_time("ç¸½ä¸‹è¼‰æ™‚é–“"):
            download_results = downloader.download_all_historical()
        
        # æ­¥é©Ÿ 2: æ¸…æ´—æ‰€æœ‰ä¸‹è¼‰çš„è³‡æ–™
        logging.info("\n=== æ­¥é©Ÿ 2: æ¸…æ´—æ­·å²è³‡æ–™ ===")
        cleaner = OTCDataCleaner(config)
        
        with performance_monitor.measure_time("ç¸½æ¸…æ´—æ™‚é–“"):
            clean_results = cleaner.clean_all_historical_files()
        
        # å®Œæˆå ±å‘Š
        end_time = datetime.now()
        duration = end_time - start_time
        
        logging.info(f"\n[ğŸ‰] æ‰€æœ‰ç¨‹åºå®Œæˆ!")
        logging.info(f"[â±] ç¸½åŸ·è¡Œæ™‚é–“: {duration}")
        logging.info(f"[ğŸ“Š] ä¸‹è¼‰çµæœ: æˆåŠŸ {download_results['success']}, å¤±æ•— {download_results['failed']}, è·³é {download_results['skipped']}")
        logging.info(f"[ğŸ“Š] æ¸…æ´—çµæœ: æˆåŠŸ {clean_results['success']}, å¤±æ•— {clean_results['failed']}")
        logging.info(f"[ğŸ“] åŸå§‹è³‡æ–™: {RAW_DIR}")
        logging.info(f"[ğŸ“] æ¸…æ´—è³‡æ–™: {CLEAN_DIR}")
        
        # ä¿å­˜åŸ·è¡Œå ±å‘Š
        performance_report_path = LOG_DIR / f"historical_performance_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        performance_monitor.save_report(performance_report_path)
        
    except KeyboardInterrupt:
        logging.warning("\n[â¹] ä½¿ç”¨è€…ä¸­æ–·åŸ·è¡Œ")
    except Exception as e:
        logging.error(f"\n[âŒ] åŸ·è¡Œéç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
        logging.error(traceback.format_exc())
        raise

def load_config() -> Dict[str, Any]:
    """è¼‰å…¥è¨­å®šæª”"""
    config_file = BASE_DIR / "otc_config.json"
    
    if config_file.exists():
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
            logging.info(f"å·²è¼‰å…¥è¨­å®šæª”: {config_file}")
            return config
        except Exception as e:
            logging.warning(f"è¨­å®šæª”è¼‰å…¥å¤±æ•—ï¼Œä½¿ç”¨é è¨­è¨­å®š: {e}")
    else:
        logging.info("æœªæ‰¾åˆ°è¨­å®šæª”ï¼Œä½¿ç”¨é è¨­è¨­å®š")
    
    return DEFAULT_CONFIG

if __name__ == "__main__":
    main(), na=False)]
        
        numeric_cols = [col for col in clean_df.columns if col not in ["stock_id", "name"]]
        for col in numeric_cols:
            clean_df[col] = self.clean_numeric_column(clean_df[col])
        
        return clean_df.sort_values("stock_id").reset_index(drop=True)
    
    def _clean_sec_trading(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ¸…æ´—åˆ¸å•†ç‡Ÿæ¥­é¡çµ±è¨ˆè³‡æ–™"""
        cols = list(df.columns)
        
        if len(cols) < 5:
            logging.error("  æ¬„ä½æ•¸é‡ä¸è¶³")
            return None
        
        left_cols = cols[:5]
        column_mapping = {
            left_cols[0]: "rank",
            left_cols[1]: "prev_rank",
            left_cols[2]: "broker",
            left_cols[3]: "name",
            left_cols[4]: "amount_thousands"
        }
        
        clean_df = df[left_cols].rename(columns=column_mapping).copy()
        
        numeric_cols = ["rank", "prev_rank", "amount_thousands"]
        for col in numeric_cols:
            if col in clean_df.columns:
                clean_df[col] = self.clean_numeric_column(clean_df[col])
        
        return clean_df.sort_values("broker").reset_index(drop=True)
    
    def _clean_day_trading(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ¸…æ´—ç¾è‚¡ç•¶æ²–äº¤æ˜“çµ±è¨ˆè³‡æ–™"""
        cols = list(df.columns)
        
        # éæ¿¾çµ±è¨ˆè¡Œ
        if len(df) > 0:
            first_col = df.iloc[:, 0].astype(str)
            mask = ~first_col.str.contains('å…±è¨ˆ|åˆè¨ˆ|ç¸½è¨ˆ|çµ±è¨ˆ|èªªæ˜|è¨»[ï¼š:]', na=False, regex=True)
            df = df[mask].copy()
        
        code_col = None
        name_col = None
        
        for pattern in ["è­‰åˆ¸ä»£è™Ÿ", "ä»£è™Ÿ", "è‚¡ç¥¨ä»£è™Ÿ", "ä»£ç¢¼"]:
            for c in cols:
                if pattern in c:
                    code_col = c
                    break
            if code_col:
                break
        
        for pattern in ["è­‰åˆ¸åç¨±", "åç¨±", "è‚¡ç¥¨åç¨±"]:
            for c in cols:
                if pattern in c:
                    name_col = c
                    break
            if name_col:
                break
        
        if not code_col and len(cols) >= 2:
            if df.iloc[0, 0] and str(df.iloc[0, 0]).strip().replace(' ', ''):
                if any(char.isdigit() for char in str(df.iloc[0, 0])):
                    code_col = cols[0]
                    name_col = cols[1] if len(cols) > 1 else None
        
        if not code_col:
            logging.error(f"  ç„¡æ³•è­˜åˆ¥è­‰åˆ¸ä»£è™Ÿæ¬„ä½ï¼Œæ¬„ä½åˆ—è¡¨ï¼š{cols}")
            return None
        
        column_mapping = {code_col: "stock_id"}
        if name_col:
            column_mapping[name_col] = "name"
        
        dt_fields = {
            "æš«åœ": "flag",
            "æˆäº¤è‚¡æ•¸": "dt_volume",
            "è²·é€²æˆäº¤é‡‘é¡": "dt_buy_amount",
            "è³£å‡ºæˆäº¤é‡‘é¡": "dt_sell_amount",
            "è²·è³£ç¸½é¡": "dt_total_amount",
            "ç•¶æ²–ç‡": "dt_rate"
        }
        
        for pattern, new_name in dt_fields.items():
            matching_col = next((c for c in cols if pattern in c), None)
            if matching_col:
                column_mapping[matching_col] = new_name
        
        available_cols = [col for col in column_mapping.keys() if col in df.columns]
        clean_df = df[available_cols].rename(columns=column_mapping).copy()
        
        clean_df["stock_id"] = self.extract_stock_id(clean_df["stock_id"])
        clean_df = clean_df.dropna(subset=["stock_id"])
        clean_df = clean_df[clean_df["stock_id"].str.match(r'^\d{4}
    
    def clean_all_historical_files(self) -> Dict[str, int]:
        """æ¸…æ´—æ‰€æœ‰æ­·å²æª”æ¡ˆ"""
        self.ensure_dir(CLEAN_DIR)
        results = {"success": 0, "failed": 0, "failed_files": []}
        
        # å–å¾—æ‰€æœ‰æª”æ¡ˆï¼ŒæŒ‰æ—¥æœŸåˆ†çµ„
        files_by_date = self.get_all_raw_files_by_date()
        total_dates = len(files_by_date)
        
        logging.info(f"\n=== é–‹å§‹æ¸…æ´—æ­·å²è³‡æ–™ ===")
        logging.info(f"æ‰¾åˆ° {total_dates} å€‹æ—¥æœŸçš„è³‡æ–™")
        
        with self.performance_monitor.measure_time("æ­·å²è³‡æ–™æ¸…æ´—"):
            for date_idx, (date_str, file_list) in enumerate(sorted(files_by_date.items()), 1):
                logging.info(f"\nâ”€â”€ æ¸…æ´—æ—¥æœŸ {date_str} ({date_idx}/{total_dates}) â”€â”€")
                
                for file_path in file_list:
                    try:
                        if self.clean_single_file(file_path):
                            results["success"] += 1
                        else:
                            results["failed"] += 1
                            results["failed_files"].append(file_path.name)
                    except Exception as e:
                        logging.error(f"æ¸…ç†æª”æ¡ˆ {file_path.name} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                        results["failed"] += 1
                        results["failed_files"].append(file_path.name)
        
        logging.info(f"\n[ğŸ“Š] æ¸…æ´—çµ±è¨ˆ:")
        logging.info(f"    - æˆåŠŸ: {results['success']}")
        logging.info(f"    - å¤±æ•—: {results['failed']}")
        
        return results

def main():
    """ä¸»è¦åŸ·è¡Œå‡½æ•¸"""
    setup_logging()
    logging.info("=== ä¸Šæ«ƒæ­·å²è³‡æ–™æ‰¹é‡ä¸‹è¼‰ + æ¸…æ´—ç³»çµ± ===")
    
    config = load_config()
    
    # ç¢ºèªåŸ·è¡Œ
    print(f"\nç›®æ¨™æ—¥æœŸç¯„åœ: {START_DATE.strftime('%Y-%m-%d')} ~ {END_DATE.strftime('%Y-%m-%d')}")
    print(f"é ä¼°äº¤æ˜“æ—¥: ~{len([d for d in pd.date_range(START_DATE, END_DATE) if d.weekday() < 5])} å¤©")
    print(f"è³‡æ–™é …ç›®: {len(config['download_items'])} ç¨®")
    print(f"é ä¼°ç¸½æ™‚é–“: 4-6 å°æ™‚")
    print(f"è¼¸å‡ºç›®éŒ„: {RAW_DIR} (åŸå§‹), {CLEAN_DIR} (æ¸…æ´—)")
    
    response = input("\nâš ï¸  é€™æ˜¯é•·æ™‚é–“åŸ·è¡Œä»»å‹™ï¼Œæ˜¯å¦ç¢ºå®šé–‹å§‹? (y/N): ").strip().lower()
    if response != 'y':
        print("å–æ¶ˆåŸ·è¡Œ")
        return
    
    start_time = datetime.now()
    performance_monitor = PerformanceMonitor()
    
    try:
        # æ­¥é©Ÿ 1: æ‰¹é‡ä¸‹è¼‰æ­·å²è³‡æ–™
        logging.info("\n=== æ­¥é©Ÿ 1: æ‰¹é‡ä¸‹è¼‰æ­·å²è³‡æ–™ ===")
        downloader = OTCHistoricalDownloader(config)
        
        with performance_monitor.measure_time("ç¸½ä¸‹è¼‰æ™‚é–“"):
            download_results = downloader.download_all_historical()
        
        # æ­¥é©Ÿ 2: æ¸…æ´—æ‰€æœ‰ä¸‹è¼‰çš„è³‡æ–™
        logging.info("\n=== æ­¥é©Ÿ 2: æ¸…æ´—æ­·å²è³‡æ–™ ===")
        cleaner = OTCDataCleaner(config)
        
        with performance_monitor.measure_time("ç¸½æ¸…æ´—æ™‚é–“"):
            clean_results = cleaner.clean_all_historical_files()
        
        # å®Œæˆå ±å‘Š
        end_time = datetime.now()
        duration = end_time - start_time
        
        logging.info(f"\n[ğŸ‰] æ‰€æœ‰ç¨‹åºå®Œæˆ!")
        logging.info(f"[â±] ç¸½åŸ·è¡Œæ™‚é–“: {duration}")
        logging.info(f"[ğŸ“Š] ä¸‹è¼‰çµæœ: æˆåŠŸ {download_results['success']}, å¤±æ•— {download_results['failed']}, è·³é {download_results['skipped']}")
        logging.info(f"[ğŸ“Š] æ¸…æ´—çµæœ: æˆåŠŸ {clean_results['success']}, å¤±æ•— {clean_results['failed']}")
        logging.info(f"[ğŸ“] åŸå§‹è³‡æ–™: {RAW_DIR}")
        logging.info(f"[ğŸ“] æ¸…æ´—è³‡æ–™: {CLEAN_DIR}")
        
        # ä¿å­˜åŸ·è¡Œå ±å‘Š
        performance_report_path = LOG_DIR / f"historical_performance_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        performance_monitor.save_report(performance_report_path)
        
    except KeyboardInterrupt:
        logging.warning("\n[â¹] ä½¿ç”¨è€…ä¸­æ–·åŸ·è¡Œ")
    except Exception as e:
        logging.error(f"\n[âŒ] åŸ·è¡Œéç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
        logging.error(traceback.format_exc())
        raise

def load_config() -> Dict[str, Any]:
    """è¼‰å…¥è¨­å®šæª”"""
    config_file = BASE_DIR / "otc_config.json"
    
    if config_file.exists():
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
            logging.info(f"å·²è¼‰å…¥è¨­å®šæª”: {config_file}")
            return config
        except Exception as e:
            logging.warning(f"è¨­å®šæª”è¼‰å…¥å¤±æ•—ï¼Œä½¿ç”¨é è¨­è¨­å®š: {e}")
    else:
        logging.info("æœªæ‰¾åˆ°è¨­å®šæª”ï¼Œä½¿ç”¨é è¨­è¨­å®š")
    
    return DEFAULT_CONFIG

if __name__ == "__main__":
    main(), na=False)]
        
        numeric_cols = [col for col in clean_df.columns if col not in ["stock_id", "name", "flag"]]
        for col in numeric_cols:
            clean_df[col] = self.clean_numeric_column(clean_df[col])
        
        return clean_df.sort_values("stock_id").reset_index(drop=True)
    
    def _clean_highlight(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ¸…æ´—èè³‡èåˆ¸é¤˜é¡æ¦‚æ³è³‡æ–™"""
        cols = list(df.columns)
        
        rank_col = next((c for c in cols if "æ’å" in c), None)
        code_col = next((c for c in cols if c == "ä»£è™Ÿ"), None)
        name_col = next((c for c in cols if c == "åç¨±"), None)
        
        if not all([rank_col, code_col, name_col]):
            logging.error("  ç¼ºå°‘å¿…è¦æ¬„ä½")
            return None
        
        column_mapping = {
            rank_col: "rank",
            code_col: "stock_id",
            name_col: "name"
        }
        
        margin_fields = {
            "æœˆå‡èè³‡é¤˜é¡": "hg_margin_balance",
            "æœˆå‡èåˆ¸é¤˜é¡": "hg_short_balance",
            "åˆ¸è³‡æ¯”": "hg_ratio"
        }
        
        for pattern, new_name in margin_fields.items():
            matching_col = next((c for c in cols if pattern in c), None)
            if matching_col:
                column_mapping[matching_col] = new_name
        
        available_cols = [col for col in column_mapping.keys() if col in df.columns]
        clean_df = df[available_cols].rename(columns=column_mapping).copy()
        
        clean_df["stock_id"] = self.extract_stock_id(clean_df["stock_id"])
        clean_df = clean_df.dropna(subset=["stock_id"])
        clean_df = clean_df[clean_df["stock_id"].str.match(r'^\d{4}
    
    def clean_all_historical_files(self) -> Dict[str, int]:
        """æ¸…æ´—æ‰€æœ‰æ­·å²æª”æ¡ˆ"""
        self.ensure_dir(CLEAN_DIR)
        results = {"success": 0, "failed": 0, "failed_files": []}
        
        # å–å¾—æ‰€æœ‰æª”æ¡ˆï¼ŒæŒ‰æ—¥æœŸåˆ†çµ„
        files_by_date = self.get_all_raw_files_by_date()
        total_dates = len(files_by_date)
        
        logging.info(f"\n=== é–‹å§‹æ¸…æ´—æ­·å²è³‡æ–™ ===")
        logging.info(f"æ‰¾åˆ° {total_dates} å€‹æ—¥æœŸçš„è³‡æ–™")
        
        with self.performance_monitor.measure_time("æ­·å²è³‡æ–™æ¸…æ´—"):
            for date_idx, (date_str, file_list) in enumerate(sorted(files_by_date.items()), 1):
                logging.info(f"\nâ”€â”€ æ¸…æ´—æ—¥æœŸ {date_str} ({date_idx}/{total_dates}) â”€â”€")
                
                for file_path in file_list:
                    try:
                        if self.clean_single_file(file_path):
                            results["success"] += 1
                        else:
                            results["failed"] += 1
                            results["failed_files"].append(file_path.name)
                    except Exception as e:
                        logging.error(f"æ¸…ç†æª”æ¡ˆ {file_path.name} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                        results["failed"] += 1
                        results["failed_files"].append(file_path.name)
        
        logging.info(f"\n[ğŸ“Š] æ¸…æ´—çµ±è¨ˆ:")
        logging.info(f"    - æˆåŠŸ: {results['success']}")
        logging.info(f"    - å¤±æ•—: {results['failed']}")
        
        return results

def main():
    """ä¸»è¦åŸ·è¡Œå‡½æ•¸"""
    setup_logging()
    logging.info("=== ä¸Šæ«ƒæ­·å²è³‡æ–™æ‰¹é‡ä¸‹è¼‰ + æ¸…æ´—ç³»çµ± ===")
    
    config = load_config()
    
    # ç¢ºèªåŸ·è¡Œ
    print(f"\nç›®æ¨™æ—¥æœŸç¯„åœ: {START_DATE.strftime('%Y-%m-%d')} ~ {END_DATE.strftime('%Y-%m-%d')}")
    print(f"é ä¼°äº¤æ˜“æ—¥: ~{len([d for d in pd.date_range(START_DATE, END_DATE) if d.weekday() < 5])} å¤©")
    print(f"è³‡æ–™é …ç›®: {len(config['download_items'])} ç¨®")
    print(f"é ä¼°ç¸½æ™‚é–“: 4-6 å°æ™‚")
    print(f"è¼¸å‡ºç›®éŒ„: {RAW_DIR} (åŸå§‹), {CLEAN_DIR} (æ¸…æ´—)")
    
    response = input("\nâš ï¸  é€™æ˜¯é•·æ™‚é–“åŸ·è¡Œä»»å‹™ï¼Œæ˜¯å¦ç¢ºå®šé–‹å§‹? (y/N): ").strip().lower()
    if response != 'y':
        print("å–æ¶ˆåŸ·è¡Œ")
        return
    
    start_time = datetime.now()
    performance_monitor = PerformanceMonitor()
    
    try:
        # æ­¥é©Ÿ 1: æ‰¹é‡ä¸‹è¼‰æ­·å²è³‡æ–™
        logging.info("\n=== æ­¥é©Ÿ 1: æ‰¹é‡ä¸‹è¼‰æ­·å²è³‡æ–™ ===")
        downloader = OTCHistoricalDownloader(config)
        
        with performance_monitor.measure_time("ç¸½ä¸‹è¼‰æ™‚é–“"):
            download_results = downloader.download_all_historical()
        
        # æ­¥é©Ÿ 2: æ¸…æ´—æ‰€æœ‰ä¸‹è¼‰çš„è³‡æ–™
        logging.info("\n=== æ­¥é©Ÿ 2: æ¸…æ´—æ­·å²è³‡æ–™ ===")
        cleaner = OTCDataCleaner(config)
        
        with performance_monitor.measure_time("ç¸½æ¸…æ´—æ™‚é–“"):
            clean_results = cleaner.clean_all_historical_files()
        
        # å®Œæˆå ±å‘Š
        end_time = datetime.now()
        duration = end_time - start_time
        
        logging.info(f"\n[ğŸ‰] æ‰€æœ‰ç¨‹åºå®Œæˆ!")
        logging.info(f"[â±] ç¸½åŸ·è¡Œæ™‚é–“: {duration}")
        logging.info(f"[ğŸ“Š] ä¸‹è¼‰çµæœ: æˆåŠŸ {download_results['success']}, å¤±æ•— {download_results['failed']}, è·³é {download_results['skipped']}")
        logging.info(f"[ğŸ“Š] æ¸…æ´—çµæœ: æˆåŠŸ {clean_results['success']}, å¤±æ•— {clean_results['failed']}")
        logging.info(f"[ğŸ“] åŸå§‹è³‡æ–™: {RAW_DIR}")
        logging.info(f"[ğŸ“] æ¸…æ´—è³‡æ–™: {CLEAN_DIR}")
        
        # ä¿å­˜åŸ·è¡Œå ±å‘Š
        performance_report_path = LOG_DIR / f"historical_performance_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        performance_monitor.save_report(performance_report_path)
        
    except KeyboardInterrupt:
        logging.warning("\n[â¹] ä½¿ç”¨è€…ä¸­æ–·åŸ·è¡Œ")
    except Exception as e:
        logging.error(f"\n[âŒ] åŸ·è¡Œéç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
        logging.error(traceback.format_exc())
        raise

def load_config() -> Dict[str, Any]:
    """è¼‰å…¥è¨­å®šæª”"""
    config_file = BASE_DIR / "otc_config.json"
    
    if config_file.exists():
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
            logging.info(f"å·²è¼‰å…¥è¨­å®šæª”: {config_file}")
            return config
        except Exception as e:
            logging.warning(f"è¨­å®šæª”è¼‰å…¥å¤±æ•—ï¼Œä½¿ç”¨é è¨­è¨­å®š: {e}")
    else:
        logging.info("æœªæ‰¾åˆ°è¨­å®šæª”ï¼Œä½¿ç”¨é è¨­è¨­å®š")
    
    return DEFAULT_CONFIG

if __name__ == "__main__":
    main(), na=False)]
        
        numeric_cols = [col for col in clean_df.columns if col not in ["stock_id", "name"]]
        for col in numeric_cols:
            clean_df[col] = self.clean_numeric_column(clean_df[col])
        
        return clean_df.sort_values("stock_id").reset_index(drop=True)
    
    def _clean_sbl(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ¸…æ´—ä¿¡ç”¨é¡åº¦ç¸½é‡ç®¡åˆ¶é¤˜é¡è³‡æ–™"""
        cols = list(df.columns)
        
        code_col = next((c for c in cols if "è‚¡ç¥¨ä»£è™Ÿ" in c), None)
        name_col = next((c for c in cols if "è‚¡ç¥¨åç¨±" in c), None)
        
        if not all([code_col, name_col]):
            logging.error("  ç¼ºå°‘å¿…è¦æ¬„ä½")
            return None
        
        column_mapping = {
            code_col: "stock_id",
            name_col: "name"
        }
        
        sbl_fields = {
            "èåˆ¸å‰æ—¥é¤˜é¡": "owz_short_prev_balance",
            "èåˆ¸è³£å‡º": "owz_short_sell",
            "èåˆ¸è²·é€²": "owz_short_buy",
            "èåˆ¸ç¾åˆ¸": "owz_short_spot",
            "èåˆ¸ç•¶æ—¥é¤˜é¡": "owz_short_today_balance",
            "èåˆ¸é™é¡": "owz_short_limit",
            "å€Ÿåˆ¸å‰æ—¥é¤˜é¡": "owz_borrow_prev_balance",
            "å€Ÿåˆ¸ç•¶æ—¥è³£å‡º": "owz_borrow_sell",
            "å€Ÿåˆ¸ç•¶æ—¥é‚„åˆ¸": "owz_borrow_return",
            "å€Ÿåˆ¸ç•¶æ—¥èª¿æ•´æ•¸é¡": "owz_borrow_adj",
            "å€Ÿåˆ¸ç•¶æ—¥é¤˜é¡": "owz_borrow_today_balance",
            "å€Ÿåˆ¸æ¬¡ä¸€ç‡Ÿæ¥­æ—¥å¯å€Ÿåˆ¸è³£å‡ºé™é¡": "owz_borrow_next_limit",
            "å‚™è¨»": "remark"
        }
        
        for pattern, new_name in sbl_fields.items():
            matching_col = next((c for c in cols if pattern in c), None)
            if matching_col:
                column_mapping[matching_col] = new_name
        
        available_cols = [col for col in column_mapping.keys() if col in df.columns]
        clean_df = df[available_cols].rename(columns=column_mapping).copy()
        
        clean_df["stock_id"] = self.extract_stock_id(clean_df["stock_id"])
        clean_df = clean_df.dropna(subset=["stock_id"])
        clean_df = clean_df[clean_df["stock_id"].str.match(r'^\d{4}
    
    def clean_all_historical_files(self) -> Dict[str, int]:
        """æ¸…æ´—æ‰€æœ‰æ­·å²æª”æ¡ˆ"""
        self.ensure_dir(CLEAN_DIR)
        results = {"success": 0, "failed": 0, "failed_files": []}
        
        # å–å¾—æ‰€æœ‰æª”æ¡ˆï¼ŒæŒ‰æ—¥æœŸåˆ†çµ„
        files_by_date = self.get_all_raw_files_by_date()
        total_dates = len(files_by_date)
        
        logging.info(f"\n=== é–‹å§‹æ¸…æ´—æ­·å²è³‡æ–™ ===")
        logging.info(f"æ‰¾åˆ° {total_dates} å€‹æ—¥æœŸçš„è³‡æ–™")
        
        with self.performance_monitor.measure_time("æ­·å²è³‡æ–™æ¸…æ´—"):
            for date_idx, (date_str, file_list) in enumerate(sorted(files_by_date.items()), 1):
                logging.info(f"\nâ”€â”€ æ¸…æ´—æ—¥æœŸ {date_str} ({date_idx}/{total_dates}) â”€â”€")
                
                for file_path in file_list:
                    try:
                        if self.clean_single_file(file_path):
                            results["success"] += 1
                        else:
                            results["failed"] += 1
                            results["failed_files"].append(file_path.name)
                    except Exception as e:
                        logging.error(f"æ¸…ç†æª”æ¡ˆ {file_path.name} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                        results["failed"] += 1
                        results["failed_files"].append(file_path.name)
        
        logging.info(f"\n[ğŸ“Š] æ¸…æ´—çµ±è¨ˆ:")
        logging.info(f"    - æˆåŠŸ: {results['success']}")
        logging.info(f"    - å¤±æ•—: {results['failed']}")
        
        return results

def main():
    """ä¸»è¦åŸ·è¡Œå‡½æ•¸"""
    setup_logging()
    logging.info("=== ä¸Šæ«ƒæ­·å²è³‡æ–™æ‰¹é‡ä¸‹è¼‰ + æ¸…æ´—ç³»çµ± ===")
    
    config = load_config()
    
    # ç¢ºèªåŸ·è¡Œ
    print(f"\nç›®æ¨™æ—¥æœŸç¯„åœ: {START_DATE.strftime('%Y-%m-%d')} ~ {END_DATE.strftime('%Y-%m-%d')}")
    print(f"é ä¼°äº¤æ˜“æ—¥: ~{len([d for d in pd.date_range(START_DATE, END_DATE) if d.weekday() < 5])} å¤©")
    print(f"è³‡æ–™é …ç›®: {len(config['download_items'])} ç¨®")
    print(f"é ä¼°ç¸½æ™‚é–“: 4-6 å°æ™‚")
    print(f"è¼¸å‡ºç›®éŒ„: {RAW_DIR} (åŸå§‹), {CLEAN_DIR} (æ¸…æ´—)")
    
    response = input("\nâš ï¸  é€™æ˜¯é•·æ™‚é–“åŸ·è¡Œä»»å‹™ï¼Œæ˜¯å¦ç¢ºå®šé–‹å§‹? (y/N): ").strip().lower()
    if response != 'y':
        print("å–æ¶ˆåŸ·è¡Œ")
        return
    
    start_time = datetime.now()
    performance_monitor = PerformanceMonitor()
    
    try:
        # æ­¥é©Ÿ 1: æ‰¹é‡ä¸‹è¼‰æ­·å²è³‡æ–™
        logging.info("\n=== æ­¥é©Ÿ 1: æ‰¹é‡ä¸‹è¼‰æ­·å²è³‡æ–™ ===")
        downloader = OTCHistoricalDownloader(config)
        
        with performance_monitor.measure_time("ç¸½ä¸‹è¼‰æ™‚é–“"):
            download_results = downloader.download_all_historical()
        
        # æ­¥é©Ÿ 2: æ¸…æ´—æ‰€æœ‰ä¸‹è¼‰çš„è³‡æ–™
        logging.info("\n=== æ­¥é©Ÿ 2: æ¸…æ´—æ­·å²è³‡æ–™ ===")
        cleaner = OTCDataCleaner(config)
        
        with performance_monitor.measure_time("ç¸½æ¸…æ´—æ™‚é–“"):
            clean_results = cleaner.clean_all_historical_files()
        
        # å®Œæˆå ±å‘Š
        end_time = datetime.now()
        duration = end_time - start_time
        
        logging.info(f"\n[ğŸ‰] æ‰€æœ‰ç¨‹åºå®Œæˆ!")
        logging.info(f"[â±] ç¸½åŸ·è¡Œæ™‚é–“: {duration}")
        logging.info(f"[ğŸ“Š] ä¸‹è¼‰çµæœ: æˆåŠŸ {download_results['success']}, å¤±æ•— {download_results['failed']}, è·³é {download_results['skipped']}")
        logging.info(f"[ğŸ“Š] æ¸…æ´—çµæœ: æˆåŠŸ {clean_results['success']}, å¤±æ•— {clean_results['failed']}")
        logging.info(f"[ğŸ“] åŸå§‹è³‡æ–™: {RAW_DIR}")
        logging.info(f"[ğŸ“] æ¸…æ´—è³‡æ–™: {CLEAN_DIR}")
        
        # ä¿å­˜åŸ·è¡Œå ±å‘Š
        performance_report_path = LOG_DIR / f"historical_performance_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        performance_monitor.save_report(performance_report_path)
        
    except KeyboardInterrupt:
        logging.warning("\n[â¹] ä½¿ç”¨è€…ä¸­æ–·åŸ·è¡Œ")
    except Exception as e:
        logging.error(f"\n[âŒ] åŸ·è¡Œéç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
        logging.error(traceback.format_exc())
        raise

def load_config() -> Dict[str, Any]:
    """è¼‰å…¥è¨­å®šæª”"""
    config_file = BASE_DIR / "otc_config.json"
    
    if config_file.exists():
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
            logging.info(f"å·²è¼‰å…¥è¨­å®šæª”: {config_file}")
            return config
        except Exception as e:
            logging.warning(f"è¨­å®šæª”è¼‰å…¥å¤±æ•—ï¼Œä½¿ç”¨é è¨­è¨­å®š: {e}")
    else:
        logging.info("æœªæ‰¾åˆ°è¨­å®šæª”ï¼Œä½¿ç”¨é è¨­è¨­å®š")
    
    return DEFAULT_CONFIG

if __name__ == "__main__":
    main(), na=False)]
        
        numeric_cols = [col for col in clean_df.columns if col not in ["stock_id", "name", "remark"]]
        for col in numeric_cols:
            clean_df[col] = self.clean_numeric_column(clean_df[col])
        
        return clean_df.sort_values("stock_id").reset_index(drop=True)
    
    def _clean_margin_transactions(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ¸…æ´—èè³‡èåˆ¸é¤˜é¡è³‡æ–™"""
        cols = list(df.columns)
        
        # éæ¿¾åªä¿ç•™æ•¸å­—é–‹é ­çš„è¡Œ
        if len(df) > 0:
            first_col_str = df.iloc[:, 0].astype(str)
            mask = first_col_str.str.match(r'^\d', na=False)
            df = df[mask].copy()
        
        code_col = None
        name_col = None
        
        for pattern in ["ä»£è™Ÿ", "ä»£ç¢¼", "è‚¡ç¥¨ä»£è™Ÿ", "è­‰åˆ¸ä»£è™Ÿ"]:
            for c in cols:
                if pattern in c:
                    code_col = c
                    break
            if code_col:
                break
        
        for pattern in ["åç¨±", "è‚¡ç¥¨åç¨±", "è­‰åˆ¸åç¨±"]:
            for c in cols:
                if pattern in c:
                    name_col = c
                    break
            if name_col:
                break
        
        if not code_col and len(cols) >= 2:
            if df.iloc[0, 0] and re.match(r'^\d+', str(df.iloc[0, 0])):
                code_col = cols[0]
                name_col = cols[1]
        
        if not code_col:
            logging.error(f"  ç„¡æ³•è­˜åˆ¥è­‰åˆ¸ä»£è™Ÿæ¬„ä½ï¼Œæ¬„ä½åˆ—è¡¨ï¼š{cols}")
            return None
        
        column_mapping = {code_col: "stock_id"}
        if name_col:
            column_mapping[name_col] = "name"
        
        mt_fields = {
            "å‰è³‡é¤˜é¡": "mt_prev_balance",
            "è³‡è²·": "mt_buy",
            "è³‡è³£": "mt_sell",
            "ç¾å„Ÿ": "mt_pay",
            "è³‡é¤˜é¡": "mt_balance",
            "è³‡å±¬è­‰é‡‘": "mt_cash",
            "è³‡ä½¿ç”¨ç‡": "mt_usage_rate",
            "è³‡é™é¡": "mt_limit",
            "å‰åˆ¸é¤˜é¡": "st_prev_balance",
            "åˆ¸è³£": "st_sell",
            "åˆ¸è²·": "st_buy",
            "åˆ¸å„Ÿ": "st_pay",
            "åˆ¸é¤˜é¡": "st_balance",
            "åˆ¸å±¬è­‰é‡‘": "st_cash",
            "åˆ¸ä½¿ç”¨ç‡": "st_usage_rate",
            "åˆ¸é™é¡": "st_limit",
            "è³‡åˆ¸ç›¸æŠµ": "mt_st_offset",
            "å‚™è¨»": "remark"
        }
        
        for pattern, new_name in mt_fields.items():
            matching_col = next((c for c in cols if pattern in c), None)
            if matching_col:
                column_mapping[matching_col] = new_name
        
        available_cols = [col for col in column_mapping.keys() if col in df.columns]
        clean_df = df[available_cols].rename(columns=column_mapping).copy()
        
        clean_df["stock_id"] = self.extract_stock_id(clean_df["stock_id"])
        clean_df = clean_df.dropna(subset=["stock_id"])
        clean_df = clean_df[clean_df["stock_id"].str.match(r'^\d{4}
    
    def clean_all_historical_files(self) -> Dict[str, int]:
        """æ¸…æ´—æ‰€æœ‰æ­·å²æª”æ¡ˆ"""
        self.ensure_dir(CLEAN_DIR)
        results = {"success": 0, "failed": 0, "failed_files": []}
        
        # å–å¾—æ‰€æœ‰æª”æ¡ˆï¼ŒæŒ‰æ—¥æœŸåˆ†çµ„
        files_by_date = self.get_all_raw_files_by_date()
        total_dates = len(files_by_date)
        
        logging.info(f"\n=== é–‹å§‹æ¸…æ´—æ­·å²è³‡æ–™ ===")
        logging.info(f"æ‰¾åˆ° {total_dates} å€‹æ—¥æœŸçš„è³‡æ–™")
        
        with self.performance_monitor.measure_time("æ­·å²è³‡æ–™æ¸…æ´—"):
            for date_idx, (date_str, file_list) in enumerate(sorted(files_by_date.items()), 1):
                logging.info(f"\nâ”€â”€ æ¸…æ´—æ—¥æœŸ {date_str} ({date_idx}/{total_dates}) â”€â”€")
                
                for file_path in file_list:
                    try:
                        if self.clean_single_file(file_path):
                            results["success"] += 1
                        else:
                            results["failed"] += 1
                            results["failed_files"].append(file_path.name)
                    except Exception as e:
                        logging.error(f"æ¸…ç†æª”æ¡ˆ {file_path.name} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                        results["failed"] += 1
                        results["failed_files"].append(file_path.name)
        
        logging.info(f"\n[ğŸ“Š] æ¸…æ´—çµ±è¨ˆ:")
        logging.info(f"    - æˆåŠŸ: {results['success']}")
        logging.info(f"    - å¤±æ•—: {results['failed']}")
        
        return results

def main():
    """ä¸»è¦åŸ·è¡Œå‡½æ•¸"""
    setup_logging()
    logging.info("=== ä¸Šæ«ƒæ­·å²è³‡æ–™æ‰¹é‡ä¸‹è¼‰ + æ¸…æ´—ç³»çµ± ===")
    
    config = load_config()
    
    # ç¢ºèªåŸ·è¡Œ
    print(f"\nç›®æ¨™æ—¥æœŸç¯„åœ: {START_DATE.strftime('%Y-%m-%d')} ~ {END_DATE.strftime('%Y-%m-%d')}")
    print(f"é ä¼°äº¤æ˜“æ—¥: ~{len([d for d in pd.date_range(START_DATE, END_DATE) if d.weekday() < 5])} å¤©")
    print(f"è³‡æ–™é …ç›®: {len(config['download_items'])} ç¨®")
    print(f"é ä¼°ç¸½æ™‚é–“: 4-6 å°æ™‚")
    print(f"è¼¸å‡ºç›®éŒ„: {RAW_DIR} (åŸå§‹), {CLEAN_DIR} (æ¸…æ´—)")
    
    response = input("\nâš ï¸  é€™æ˜¯é•·æ™‚é–“åŸ·è¡Œä»»å‹™ï¼Œæ˜¯å¦ç¢ºå®šé–‹å§‹? (y/N): ").strip().lower()
    if response != 'y':
        print("å–æ¶ˆåŸ·è¡Œ")
        return
    
    start_time = datetime.now()
    performance_monitor = PerformanceMonitor()
    
    try:
        # æ­¥é©Ÿ 1: æ‰¹é‡ä¸‹è¼‰æ­·å²è³‡æ–™
        logging.info("\n=== æ­¥é©Ÿ 1: æ‰¹é‡ä¸‹è¼‰æ­·å²è³‡æ–™ ===")
        downloader = OTCHistoricalDownloader(config)
        
        with performance_monitor.measure_time("ç¸½ä¸‹è¼‰æ™‚é–“"):
            download_results = downloader.download_all_historical()
        
        # æ­¥é©Ÿ 2: æ¸…æ´—æ‰€æœ‰ä¸‹è¼‰çš„è³‡æ–™
        logging.info("\n=== æ­¥é©Ÿ 2: æ¸…æ´—æ­·å²è³‡æ–™ ===")
        cleaner = OTCDataCleaner(config)
        
        with performance_monitor.measure_time("ç¸½æ¸…æ´—æ™‚é–“"):
            clean_results = cleaner.clean_all_historical_files()
        
        # å®Œæˆå ±å‘Š
        end_time = datetime.now()
        duration = end_time - start_time
        
        logging.info(f"\n[ğŸ‰] æ‰€æœ‰ç¨‹åºå®Œæˆ!")
        logging.info(f"[â±] ç¸½åŸ·è¡Œæ™‚é–“: {duration}")
        logging.info(f"[ğŸ“Š] ä¸‹è¼‰çµæœ: æˆåŠŸ {download_results['success']}, å¤±æ•— {download_results['failed']}, è·³é {download_results['skipped']}")
        logging.info(f"[ğŸ“Š] æ¸…æ´—çµæœ: æˆåŠŸ {clean_results['success']}, å¤±æ•— {clean_results['failed']}")
        logging.info(f"[ğŸ“] åŸå§‹è³‡æ–™: {RAW_DIR}")
        logging.info(f"[ğŸ“] æ¸…æ´—è³‡æ–™: {CLEAN_DIR}")
        
        # ä¿å­˜åŸ·è¡Œå ±å‘Š
        performance_report_path = LOG_DIR / f"historical_performance_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        performance_monitor.save_report(performance_report_path)
        
    except KeyboardInterrupt:
        logging.warning("\n[â¹] ä½¿ç”¨è€…ä¸­æ–·åŸ·è¡Œ")
    except Exception as e:
        logging.error(f"\n[âŒ] åŸ·è¡Œéç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
        logging.error(traceback.format_exc())
        raise

def load_config() -> Dict[str, Any]:
    """è¼‰å…¥è¨­å®šæª”"""
    config_file = BASE_DIR / "otc_config.json"
    
    if config_file.exists():
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
            logging.info(f"å·²è¼‰å…¥è¨­å®šæª”: {config_file}")
            return config
        except Exception as e:
            logging.warning(f"è¨­å®šæª”è¼‰å…¥å¤±æ•—ï¼Œä½¿ç”¨é è¨­è¨­å®š: {e}")
    else:
        logging.info("æœªæ‰¾åˆ°è¨­å®šæª”ï¼Œä½¿ç”¨é è¨­è¨­å®š")
    
    return DEFAULT_CONFIG

if __name__ == "__main__":
    main(), na=False)]
        
        numeric_cols = [col for col in clean_df.columns if col not in ["stock_id", "name", "remark"]]
        for col in numeric_cols:
            clean_df[col] = self.clean_numeric_column(clean_df[col])
        
        return clean_df.sort_values("stock_id").reset_index(drop=True)
    
    def _clean_exempted(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ¸…æ´—å¹³ç›¤ä¸‹å¾—èåˆ¸è³£å‡ºè­‰åˆ¸åå–®"""
        cols = list(df.columns)
        
        # æ‰¾åˆ°æ•¸æ“šé–‹å§‹çš„è¡Œ
        data_start_row = 0
        for i, row in df.iterrows():
            first_val = str(row.iloc[0])
            if re.match(r'^\d{3,4}', first_val):
                data_start_row = i
                break
        
        if data_start_row > 0:
            df = df.iloc[data_start_row:].copy()
            df.columns = cols
        
        code_col = None
        name_col = None
        
        for c in cols:
            if "è­‰åˆ¸ä»£è™Ÿ" in c or "ä»£è™Ÿ" in c or "ä»£ç¢¼" in c:
                code_col = c
                break
        
        for c in cols:
            if "è­‰åˆ¸åç¨±" in c or "åç¨±" in c:
                name_col = c
                break
        
        if not code_col:
            if len(cols) >= 2:
                code_col = cols[0]
                name_col = cols[1] if not name_col else name_col
        
        if not code_col:
            logging.error("  ç„¡æ³•è­˜åˆ¥è­‰åˆ¸ä»£è™Ÿæ¬„ä½")
            return None
        
        column_mapping = {code_col: "stock_id"}
        if name_col:
            column_mapping[name_col] = "name"
        
        # æ‰¾åˆ°æ¨™è¨˜æ¬„ä½
        mark_cols = []
        for c in cols:
            if c not in [code_col, name_col]:
                if "æš«åœ" in c or "æ¨™è¨˜" in c or "è¨»è¨˜" in c or len(c) <= 3:
                    mark_cols.append(c)
        
        for i, mark_col in enumerate(mark_cols):
            column_mapping[mark_col] = f"mark_{i+1}" if i > 0 else "mark"
        
        available_cols = [col for col in column_mapping.keys() if col in df.columns]
        clean_df = df[available_cols].rename(columns=column_mapping).copy()
        
        clean_df["stock_id"] = self.extract_stock_id(clean_df["stock_id"])
        clean_df = clean_df.dropna(subset=["stock_id"])
        clean_df = clean_df[clean_df["stock_id"].str.match(r'^\d{4}$'
    
    def clean_all_historical_files(self) -> Dict[str, int]:
        """æ¸…æ´—æ‰€æœ‰æ­·å²æª”æ¡ˆ"""
        self.ensure_dir(CLEAN_DIR)
        results = {"success": 0, "failed": 0, "failed_files": []}
        
        # å–å¾—æ‰€æœ‰æª”æ¡ˆï¼ŒæŒ‰æ—¥æœŸåˆ†çµ„
        files_by_date = self.get_all_raw_files_by_date()
        total_dates = len(files_by_date)
        
        logging.info(f"\n=== é–‹å§‹æ¸…æ´—æ­·å²è³‡æ–™ ===")
        logging.info(f"æ‰¾åˆ° {total_dates} å€‹æ—¥æœŸçš„è³‡æ–™")
        
        with self.performance_monitor.measure_time("æ­·å²è³‡æ–™æ¸…æ´—"):
            for date_idx, (date_str, file_list) in enumerate(sorted(files_by_date.items()), 1):
                logging.info(f"\nâ”€â”€ æ¸…æ´—æ—¥æœŸ {date_str} ({date_idx}/{total_dates}) â”€â”€")
                
                for file_path in file_list:
                    try:
                        if self.clean_single_file(file_path):
                            results["success"] += 1
                        else:
                            results["failed"] += 1
                            results["failed_files"].append(file_path.name)
                    except Exception as e:
                        logging.error(f"æ¸…ç†æª”æ¡ˆ {file_path.name} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                        results["failed"] += 1
                        results["failed_files"].append(file_path.name)
        
        logging.info(f"\n[ğŸ“Š] æ¸…æ´—çµ±è¨ˆ:")
        logging.info(f"    - æˆåŠŸ: {results['success']}")
        logging.info(f"    - å¤±æ•—: {results['failed']}")
        
        return results

def main():
    """ä¸»è¦åŸ·è¡Œå‡½æ•¸"""
    setup_logging()
    logging.info("=== ä¸Šæ«ƒæ­·å²è³‡æ–™æ‰¹é‡ä¸‹è¼‰ + æ¸…æ´—ç³»çµ± ===")
    
    config = load_config()
    
    # ç¢ºèªåŸ·è¡Œ
    print(f"\nç›®æ¨™æ—¥æœŸç¯„åœ: {START_DATE.strftime('%Y-%m-%d')} ~ {END_DATE.strftime('%Y-%m-%d')}")
    print(f"é ä¼°äº¤æ˜“æ—¥: ~{len([d for d in pd.date_range(START_DATE, END_DATE) if d.weekday() < 5])} å¤©")
    print(f"è³‡æ–™é …ç›®: {len(config['download_items'])} ç¨®")
    print(f"é ä¼°ç¸½æ™‚é–“: 4-6 å°æ™‚")
    print(f"è¼¸å‡ºç›®éŒ„: {RAW_DIR} (åŸå§‹), {CLEAN_DIR} (æ¸…æ´—)")
    
    response = input("\nâš ï¸  é€™æ˜¯é•·æ™‚é–“åŸ·è¡Œä»»å‹™ï¼Œæ˜¯å¦ç¢ºå®šé–‹å§‹? (y/N): ").strip().lower()
    if response != 'y':
        print("å–æ¶ˆåŸ·è¡Œ")
        return
    
    start_time = datetime.now()
    performance_monitor = PerformanceMonitor()
    
    try:
        # æ­¥é©Ÿ 1: æ‰¹é‡ä¸‹è¼‰æ­·å²è³‡æ–™
        logging.info("\n=== æ­¥é©Ÿ 1: æ‰¹é‡ä¸‹è¼‰æ­·å²è³‡æ–™ ===")
        downloader = OTCHistoricalDownloader(config)
        
        with performance_monitor.measure_time("ç¸½ä¸‹è¼‰æ™‚é–“"):
            download_results = downloader.download_all_historical()
        
        # æ­¥é©Ÿ 2: æ¸…æ´—æ‰€æœ‰ä¸‹è¼‰çš„è³‡æ–™
        logging.info("\n=== æ­¥é©Ÿ 2: æ¸…æ´—æ­·å²è³‡æ–™ ===")
        cleaner = OTCDataCleaner(config)
        
        with performance_monitor.measure_time("ç¸½æ¸…æ´—æ™‚é–“"):
            clean_results = cleaner.clean_all_historical_files()
        
        # å®Œæˆå ±å‘Š
        end_time = datetime.now()
        duration = end_time - start_time
        
        logging.info(f"\n[ğŸ‰] æ‰€æœ‰ç¨‹åºå®Œæˆ!")
        logging.info(f"[â±] ç¸½åŸ·è¡Œæ™‚é–“: {duration}")
        logging.info(f"[ğŸ“Š] ä¸‹è¼‰çµæœ: æˆåŠŸ {download_results['success']}, å¤±æ•— {download_results['failed']}, è·³é {download_results['skipped']}")
        logging.info(f"[ğŸ“Š] æ¸…æ´—çµæœ: æˆåŠŸ {clean_results['success']}, å¤±æ•— {clean_results['failed']}")
        logging.info(f"[ğŸ“] åŸå§‹è³‡æ–™: {RAW_DIR}")
        logging.info(f"[ğŸ“] æ¸…æ´—è³‡æ–™: {CLEAN_DIR}")
        
        # ä¿å­˜åŸ·è¡Œå ±å‘Š
        performance_report_path = LOG_DIR / f"historical_performance_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        performance_monitor.save_report(performance_report_path)
        
    except KeyboardInterrupt:
        logging.warning("\n[â¹] ä½¿ç”¨è€…ä¸­æ–·åŸ·è¡Œ")
    except Exception as e:
        logging.error(f"\n[âŒ] åŸ·è¡Œéç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
        logging.error(traceback.format_exc())
        raise

def load_config() -> Dict[str, Any]:
    """è¼‰å…¥è¨­å®šæª”"""
    config_file = BASE_DIR / "otc_config.json"
    
    if config_file.exists():
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
            logging.info(f"å·²è¼‰å…¥è¨­å®šæª”: {config_file}")
            return config
        except Exception as e:
            logging.warning(f"è¨­å®šæª”è¼‰å…¥å¤±æ•—ï¼Œä½¿ç”¨é è¨­è¨­å®š: {e}")
    else:
        logging.info("æœªæ‰¾åˆ°è¨­å®šæª”ï¼Œä½¿ç”¨é è¨­è¨­å®š")
    
    return DEFAULT_CONFIG

if __name__ == "__main__":
    main(), na=False)]
        
        # éæ¿¾çµ±è¨ˆè¡Œ
        if "name" in clean_df.columns:
            clean_df = clean_df[~clean_df["name"].str.contains("å…±.*ç­†|åˆè¨ˆ|çµ±è¨ˆ|è¨»:|èªªæ˜:", na=False, regex=True)]
        
        return clean_df.sort_values("stock_id").reset_index(drop=True)
    
    def _clean_investment_trust(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ¸…æ´—æŠ•ä¿¡è²·è³£è¶…è³‡æ–™"""
        cols = list(df.columns)
        
        rank_col = next((c for c in cols if "æ’è¡Œ" in c), None)
        code_col = next((c for c in cols if "ä»£è™Ÿ" in c), None)
        name_col = next((c for c in cols if "åç¨±" in c), None)
        
        if not all([rank_col, code_col, name_col]):
            logging.error("  ç¼ºå°‘å¿…è¦æ¬„ä½")
            return None
        
        column_mapping = {
            rank_col: "rank",
            code_col: "stock_id",
            name_col: "name"
        }
        
        it_fields = {
            "è²·é€²": "it_buy_shares",
            "è³£å‡º": "it_sell_shares",
            "è²·è³£è¶…": "it_diff_shares",
            "è²·é€²é‡‘é¡": "it_buy_amount",
            "è³£å‡ºé‡‘é¡": "it_sell_amount",
            "è²·è³£è¶…é‡‘é¡": "it_diff_amount"
        }
        
        for pattern, new_name in it_fields.items():
            matching_col = next((c for c in cols if pattern in c), None)
            if matching_col:
                column_mapping[matching_col] = new_name
        
        available_cols = [col for col in column_mapping.keys() if col in df.columns]
        clean_df = df[available_cols].rename(columns=column_mapping).copy()
        
        clean_df["stock_id"] = self.extract_stock_id(clean_df["stock_id"])
        clean_df = clean_df.dropna(subset=["stock_id"])
        clean_df = clean_df[clean_df["stock_id"].str.match(r'^\d{4}
    
    def clean_all_historical_files(self) -> Dict[str, int]:
        """æ¸…æ´—æ‰€æœ‰æ­·å²æª”æ¡ˆ"""
        self.ensure_dir(CLEAN_DIR)
        results = {"success": 0, "failed": 0, "failed_files": []}
        
        # å–å¾—æ‰€æœ‰æª”æ¡ˆï¼ŒæŒ‰æ—¥æœŸåˆ†çµ„
        files_by_date = self.get_all_raw_files_by_date()
        total_dates = len(files_by_date)
        
        logging.info(f"\n=== é–‹å§‹æ¸…æ´—æ­·å²è³‡æ–™ ===")
        logging.info(f"æ‰¾åˆ° {total_dates} å€‹æ—¥æœŸçš„è³‡æ–™")
        
        with self.performance_monitor.measure_time("æ­·å²è³‡æ–™æ¸…æ´—"):
            for date_idx, (date_str, file_list) in enumerate(sorted(files_by_date.items()), 1):
                logging.info(f"\nâ”€â”€ æ¸…æ´—æ—¥æœŸ {date_str} ({date_idx}/{total_dates}) â”€â”€")
                
                for file_path in file_list:
                    try:
                        if self.clean_single_file(file_path):
                            results["success"] += 1
                        else:
                            results["failed"] += 1
                            results["failed_files"].append(file_path.name)
                    except Exception as e:
                        logging.error(f"æ¸…ç†æª”æ¡ˆ {file_path.name} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                        results["failed"] += 1
                        results["failed_files"].append(file_path.name)
        
        logging.info(f"\n[ğŸ“Š] æ¸…æ´—çµ±è¨ˆ:")
        logging.info(f"    - æˆåŠŸ: {results['success']}")
        logging.info(f"    - å¤±æ•—: {results['failed']}")
        
        return results

def main():
    """ä¸»è¦åŸ·è¡Œå‡½æ•¸"""
    setup_logging()
    logging.info("=== ä¸Šæ«ƒæ­·å²è³‡æ–™æ‰¹é‡ä¸‹è¼‰ + æ¸…æ´—ç³»çµ± ===")
    
    config = load_config()
    
    # ç¢ºèªåŸ·è¡Œ
    print(f"\nç›®æ¨™æ—¥æœŸç¯„åœ: {START_DATE.strftime('%Y-%m-%d')} ~ {END_DATE.strftime('%Y-%m-%d')}")
    print(f"é ä¼°äº¤æ˜“æ—¥: ~{len([d for d in pd.date_range(START_DATE, END_DATE) if d.weekday() < 5])} å¤©")
    print(f"è³‡æ–™é …ç›®: {len(config['download_items'])} ç¨®")
    print(f"é ä¼°ç¸½æ™‚é–“: 4-6 å°æ™‚")
    print(f"è¼¸å‡ºç›®éŒ„: {RAW_DIR} (åŸå§‹), {CLEAN_DIR} (æ¸…æ´—)")
    
    response = input("\nâš ï¸  é€™æ˜¯é•·æ™‚é–“åŸ·è¡Œä»»å‹™ï¼Œæ˜¯å¦ç¢ºå®šé–‹å§‹? (y/N): ").strip().lower()
    if response != 'y':
        print("å–æ¶ˆåŸ·è¡Œ")
        return
    
    start_time = datetime.now()
    performance_monitor = PerformanceMonitor()
    
    try:
        # æ­¥é©Ÿ 1: æ‰¹é‡ä¸‹è¼‰æ­·å²è³‡æ–™
        logging.info("\n=== æ­¥é©Ÿ 1: æ‰¹é‡ä¸‹è¼‰æ­·å²è³‡æ–™ ===")
        downloader = OTCHistoricalDownloader(config)
        
        with performance_monitor.measure_time("ç¸½ä¸‹è¼‰æ™‚é–“"):
            download_results = downloader.download_all_historical()
        
        # æ­¥é©Ÿ 2: æ¸…æ´—æ‰€æœ‰ä¸‹è¼‰çš„è³‡æ–™
        logging.info("\n=== æ­¥é©Ÿ 2: æ¸…æ´—æ­·å²è³‡æ–™ ===")
        cleaner = OTCDataCleaner(config)
        
        with performance_monitor.measure_time("ç¸½æ¸…æ´—æ™‚é–“"):
            clean_results = cleaner.clean_all_historical_files()
        
        # å®Œæˆå ±å‘Š
        end_time = datetime.now()
        duration = end_time - start_time
        
        logging.info(f"\n[ğŸ‰] æ‰€æœ‰ç¨‹åºå®Œæˆ!")
        logging.info(f"[â±] ç¸½åŸ·è¡Œæ™‚é–“: {duration}")
        logging.info(f"[ğŸ“Š] ä¸‹è¼‰çµæœ: æˆåŠŸ {download_results['success']}, å¤±æ•— {download_results['failed']}, è·³é {download_results['skipped']}")
        logging.info(f"[ğŸ“Š] æ¸…æ´—çµæœ: æˆåŠŸ {clean_results['success']}, å¤±æ•— {clean_results['failed']}")
        logging.info(f"[ğŸ“] åŸå§‹è³‡æ–™: {RAW_DIR}")
        logging.info(f"[ğŸ“] æ¸…æ´—è³‡æ–™: {CLEAN_DIR}")
        
        # ä¿å­˜åŸ·è¡Œå ±å‘Š
        performance_report_path = LOG_DIR / f"historical_performance_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        performance_monitor.save_report(performance_report_path)
        
    except KeyboardInterrupt:
        logging.warning("\n[â¹] ä½¿ç”¨è€…ä¸­æ–·åŸ·è¡Œ")
    except Exception as e:
        logging.error(f"\n[âŒ] åŸ·è¡Œéç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
        logging.error(traceback.format_exc())
        raise

def load_config() -> Dict[str, Any]:
    """è¼‰å…¥è¨­å®šæª”"""
    config_file = BASE_DIR / "otc_config.json"
    
    if config_file.exists():
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
            logging.info(f"å·²è¼‰å…¥è¨­å®šæª”: {config_file}")
            return config
        except Exception as e:
            logging.warning(f"è¨­å®šæª”è¼‰å…¥å¤±æ•—ï¼Œä½¿ç”¨é è¨­è¨­å®š: {e}")
    else:
        logging.info("æœªæ‰¾åˆ°è¨­å®šæª”ï¼Œä½¿ç”¨é è¨­è¨­å®š")
    
    return DEFAULT_CONFIG

if __name__ == "__main__":
    main(), na=False)]
        
        numeric_cols = [col for col in clean_df.columns if col not in ["rank", "stock_id", "name"]]
        for col in numeric_cols:
            clean_df[col] = self.clean_numeric_column(clean_df[col])
        
        return clean_df.sort_values("stock_id").reset_index(drop=True)
    
    def clean_all_historical_files(self) -> Dict[str, int]:
        """æ¸…æ´—æ‰€æœ‰æ­·å²æª”æ¡ˆ"""
        self.ensure_dir(CLEAN_DIR)
        results = {"success": 0, "failed": 0, "failed_files": []}
        
        # å–å¾—æ‰€æœ‰æª”æ¡ˆï¼ŒæŒ‰æ—¥æœŸåˆ†çµ„
        files_by_date = self.get_all_raw_files_by_date()
        total_dates = len(files_by_date)
        
        logging.info(f"\n=== é–‹å§‹æ¸…æ´—æ­·å²è³‡æ–™ ===")
        logging.info(f"æ‰¾åˆ° {total_dates} å€‹æ—¥æœŸçš„è³‡æ–™")
        
        with self.performance_monitor.measure_time("æ­·å²è³‡æ–™æ¸…æ´—"):
            for date_idx, (date_str, file_list) in enumerate(sorted(files_by_date.items()), 1):
                logging.info(f"\nâ”€â”€ æ¸…æ´—æ—¥æœŸ {date_str} ({date_idx}/{total_dates}) â”€â”€")
                
                for file_path in file_list:
                    try:
                        if self.clean_single_file(file_path):
                            results["success"] += 1
                        else:
                            results["failed"] += 1
                            results["failed_files"].append(file_path.name)
                    except Exception as e:
                        logging.error(f"æ¸…ç†æª”æ¡ˆ {file_path.name} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                        results["failed"] += 1
                        results["failed_files"].append(file_path.name)
        
        logging.info(f"\n[ğŸ“Š] æ¸…æ´—çµ±è¨ˆ:")
        logging.info(f"    - æˆåŠŸ: {results['success']}")
        logging.info(f"    - å¤±æ•—: {results['failed']}")
        
        return results

def main():
    """ä¸»è¦åŸ·è¡Œå‡½æ•¸"""
    setup_logging()
    logging.info("=== ä¸Šæ«ƒæ­·å²è³‡æ–™æ‰¹é‡ä¸‹è¼‰ + æ¸…æ´—ç³»çµ± ===")
    
    config = load_config()
    
    # ç¢ºèªåŸ·è¡Œ
    print(f"\nç›®æ¨™æ—¥æœŸç¯„åœ: {START_DATE.strftime('%Y-%m-%d')} ~ {END_DATE.strftime('%Y-%m-%d')}")
    print(f"é ä¼°äº¤æ˜“æ—¥: ~{len([d for d in pd.date_range(START_DATE, END_DATE) if d.weekday() < 5])} å¤©")
    print(f"è³‡æ–™é …ç›®: {len(config['download_items'])} ç¨®")
    print(f"é ä¼°ç¸½æ™‚é–“: 4-6 å°æ™‚")
    print(f"è¼¸å‡ºç›®éŒ„: {RAW_DIR} (åŸå§‹), {CLEAN_DIR} (æ¸…æ´—)")
    
    response = input("\nâš ï¸  é€™æ˜¯é•·æ™‚é–“åŸ·è¡Œä»»å‹™ï¼Œæ˜¯å¦ç¢ºå®šé–‹å§‹? (y/N): ").strip().lower()
    if response != 'y':
        print("å–æ¶ˆåŸ·è¡Œ")
        return
    
    start_time = datetime.now()
    performance_monitor = PerformanceMonitor()
    
    try:
        # æ­¥é©Ÿ 1: æ‰¹é‡ä¸‹è¼‰æ­·å²è³‡æ–™
        logging.info("\n=== æ­¥é©Ÿ 1: æ‰¹é‡ä¸‹è¼‰æ­·å²è³‡æ–™ ===")
        downloader = OTCHistoricalDownloader(config)
        
        with performance_monitor.measure_time("ç¸½ä¸‹è¼‰æ™‚é–“"):
            download_results = downloader.download_all_historical()
        
        # æ­¥é©Ÿ 2: æ¸…æ´—æ‰€æœ‰ä¸‹è¼‰çš„è³‡æ–™
        logging.info("\n=== æ­¥é©Ÿ 2: æ¸…æ´—æ­·å²è³‡æ–™ ===")
        cleaner = OTCDataCleaner(config)
        
        with performance_monitor.measure_time("ç¸½æ¸…æ´—æ™‚é–“"):
            clean_results = cleaner.clean_all_historical_files()
        
        # å®Œæˆå ±å‘Š
        end_time = datetime.now()
        duration = end_time - start_time
        
        logging.info(f"\n[ğŸ‰] æ‰€æœ‰ç¨‹åºå®Œæˆ!")
        logging.info(f"[â±] ç¸½åŸ·è¡Œæ™‚é–“: {duration}")
        logging.info(f"[ğŸ“Š] ä¸‹è¼‰çµæœ: æˆåŠŸ {download_results['success']}, å¤±æ•— {download_results['failed']}, è·³é {download_results['skipped']}")
        logging.info(f"[ğŸ“Š] æ¸…æ´—çµæœ: æˆåŠŸ {clean_results['success']}, å¤±æ•— {clean_results['failed']}")
        logging.info(f"[ğŸ“] åŸå§‹è³‡æ–™: {RAW_DIR}")
        logging.info(f"[ğŸ“] æ¸…æ´—è³‡æ–™: {CLEAN_DIR}")
        
        # ä¿å­˜åŸ·è¡Œå ±å‘Š
        performance_report_path = LOG_DIR / f"historical_performance_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        performance_monitor.save_report(performance_report_path)
        
    except KeyboardInterrupt:
        logging.warning("\n[â¹] ä½¿ç”¨è€…ä¸­æ–·åŸ·è¡Œ")
    except Exception as e:
        logging.error(f"\n[âŒ] åŸ·è¡Œéç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
        logging.error(traceback.format_exc())
        raise

def load_config() -> Dict[str, Any]:
    """è¼‰å…¥è¨­å®šæª”"""
    config_file = BASE_DIR / "otc_config.json"
    
    if config_file.exists():
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
            logging.info(f"å·²è¼‰å…¥è¨­å®šæª”: {config_file}")
            return config
        except Exception as e:
            logging.warning(f"è¨­å®šæª”è¼‰å…¥å¤±æ•—ï¼Œä½¿ç”¨é è¨­è¨­å®š: {e}")
    else:
        logging.info("æœªæ‰¾åˆ°è¨­å®šæª”ï¼Œä½¿ç”¨é è¨­è¨­å®š")
    
    return DEFAULT_CONFIG

if __name__ == "__main__":
    main()
